{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jSL_R-hO3k1K","executionInfo":{"status":"ok","timestamp":1724408783019,"user_tz":-330,"elapsed":25480,"user":{"displayName":"Gaurav Meena","userId":"13738632202280078628"}},"outputId":"2df9416b-eb9d-41b6-9bdd-ba440f7c1b6d"},"id":"jSL_R-hO3k1K","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","EPS = np.float64(1e-12)\n","train_path = '/content/drive/MyDrive/IITD/assignment2/train1.csv'#\"train1.csv\"\n","test_path  = '/content/drive/MyDrive/IITD/assignment2/test1.csv'#\"test1.csv\"\n","pred_path  = '/content/drive/MyDrive/IITD/assignment2/test_pred1.csv'#\"test_pred1.csv\"\n","param_path = '/content/drive/MyDrive/IITD/assignment2/tests/test1/params.txt'#\".\\\\tests\\\\test1\\\\params.txt\"\n","\n","def get_train_data(train_file):\n","    df = pd.read_csv(train_file)\n","    #columns_to_update = ['Operating Certificate Number', 'Permanent Facility Id', 'Total Costs', 'Length of Stay', 'Birth Weight']\n","    #for column in columns_to_update:\n","    #    df[column] = pd.to_numeric(df[column], errors='coerce')\n","    #    df[column] = (df[column] > 0).astype(int)\n","\n","    X0 = df.iloc[:, :-1].values\n","\n","    # Add bias term (column of ones)\n","    X_train = np.hstack([np.ones((X0.shape[0], 1)), X0])\n","\n","    # Extract labels and adjust class labels to start from 0\n","    Y_train = df.iloc[:, -1].values\n","    Y_train = Y_train - 1  # Assuming labels start from 1, we subtract 1 to start from 0\n","\n","    # One-hot encoding of labels\n","    num_class = len(np.unique(Y_train))\n","    Y_train_oh = np.eye(num_class)[Y_train]\n","\n","    return X_train, Y_train, Y_train_oh\n","\n","def get_test_data(test_file, pred_file):\n","    X0 = pd.read_csv(test_file).values\n","    X_test = np.hstack([np.ones((X0.shape[0], 1)), X0])  # Adding bias\n","    Y_test = pd.read_csv(pred_file).iloc[:, -1].values\n","    Y_test = Y_test - 1  # Adjust class labels to start from 0\n","\n","    # One-hot encoding of labels\n","    num_class = len(np.unique(Y_test))\n","    Y_test_oh = np.eye(num_class)[Y_test]\n","\n","    return X_test, Y_test, Y_test_oh\n","\n","def get_params(param_file):\n","    with open(param_file, 'r') as file:\n","        lines = file.readlines()\n","\n","    # Read parameters from the file\n","    learning_strategy = np.int32(lines[0].strip())  # First line as an integer\n","    second_line_values = np.array(lines[1].strip().split(','), dtype=np.float64)\n","\n","    if len(second_line_values) == 1:\n","        n0 = second_line_values[0]\n","        k = None\n","    else:\n","        n0, k = second_line_values\n","\n","    epochs = np.int32(lines[2].strip())\n","    batch_size = np.int32(lines[3].strip())\n","\n","    return learning_strategy, n0, k , epochs, batch_size\n","\n","# Load training data\n","X_train, Y_train_indices, Y_train_oh = get_train_data(train_path)\n","\n","# Number of features and classes\n","n_features = X_train.shape[1]\n","num_class = len(np.unique(Y_train_indices))\n","\n","# Initialize weights\n","W = np.zeros((n_features, num_class), dtype=np.float64)\n","\n","# Load parameters\n","strategy, step, k, epochs, batch_size = get_params(param_path)\n","print(strategy, step, k, epochs, batch_size)\n","\n","# Calculate the frequency of each class using the original class labels"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VvbJUgpa3wIr","executionInfo":{"status":"ok","timestamp":1724408797322,"user_tz":-330,"elapsed":8739,"user":{"displayName":"Gaurav Meena","userId":"13738632202280078628"}},"outputId":"704007a4-91cd-4f28-8687-e634f89524f1"},"id":"VvbJUgpa3wIr","execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["1 1e-09 None 25 87595\n"]}]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z7XdlBWygcDw","executionInfo":{"status":"ok","timestamp":1724409259010,"user_tz":-330,"elapsed":395127,"user":{"displayName":"Gaurav Meena","userId":"13738632202280078628"}},"outputId":"786a89ab-5297-488f-db5e-b2ce5509748b"},"id":"Z7XdlBWygcDw","execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Class Frequencies: [16131  1032 25406 45026]\n","Loss: 4.770042276676647e-05\n"]}]},{"cell_type":"code","execution_count":7,"id":"75f48e9b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"75f48e9b","executionInfo":{"status":"ok","timestamp":1724409266188,"user_tz":-330,"elapsed":403,"user":{"displayName":"Gaurav Meena","userId":"13738632202280078628"}},"outputId":"d81911a8-2320-413a-bb54-802f1f542b3c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Class Frequencies: [16131  1032 25406 45026]\n","[19204.04391665  2721.69205303 28804.46072119 36864.80330914]\n"]}],"source":["#W = np.zeros((n_features, num_class), dtype=np.float64)\n","\n","\n","print(\"Class Frequencies:\", freq)\n","pred_probs = softmax(X_train, W)\n","vertical_sum = np.sum(pred_probs, axis=0)\n","print(vertical_sum)\n"]},{"cell_type":"code","execution_count":null,"id":"7cb1338c","metadata":{"id":"7cb1338c"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"897e4226","metadata":{"cellView":"form","collapsed":true,"id":"897e4226","outputId":"9f2e7769-fff0-42eb-da54-da4e420eebcd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Iteration 1: low = 0, high = 1e-09, rate1 = 3.3333333333333337e-10, rate2 = 6.666666666666667e-10\n","Iteration 1: losslow = 0.6931471805599445, losshigh = 8.392494847622308, loss1 = 8.370435971279305, loss2 = 8.392494847622308\n","Iteration 2: low = 0, high = 6.666666666666667e-10, rate1 = 2.2222222222222224e-10, rate2 = 4.444444444444445e-10\n","Iteration 2: losslow = 0.6931471805599445, losshigh = 8.392494847622308, loss1 = 8.355575473164082, loss2 = 8.379840795402895\n","Iteration 3: low = 0, high = 4.444444444444445e-10, rate1 = 1.4814814814814817e-10, rate2 = 2.9629629629629634e-10\n","Iteration 3: losslow = 0.6931471805599445, losshigh = 8.379840795402895, loss1 = 8.337552688256432, loss2 = 8.367301030105443\n","Iteration 4: low = 0, high = 2.9629629629629634e-10, rate1 = 9.876543209876544e-11, rate2 = 1.9753086419753088e-10\n","Iteration 4: losslow = 0.6931471805599445, losshigh = 8.367301030105443, loss1 = 8.32382057211481, loss2 = 8.350802356687877\n","Iteration 5: low = 0, high = 1.9753086419753088e-10, rate1 = 6.584362139917696e-11, rate2 = 1.3168724279835392e-10\n","Iteration 5: losslow = 0.6931471805599445, losshigh = 8.350802356687877, loss1 = 8.309777937094553, loss2 = 8.332991846181308\n","Iteration 6: low = 0, high = 1.3168724279835392e-10, rate1 = 4.389574759945131e-11, rate2 = 8.779149519890261e-11\n","Iteration 6: losslow = 0.6931471805599445, losshigh = 8.332991846181308, loss1 = 8.294202074938474, loss2 = 8.31997432688686\n","Iteration 7: low = 0, high = 8.779149519890261e-11, rate1 = 2.926383173296754e-11, rate2 = 5.852766346593508e-11\n","Iteration 7: losslow = 0.6931471805599445, losshigh = 8.31997432688686, loss1 = 8.242004740948145, loss2 = 8.305593820797938\n","Iteration 8: low = 0, high = 5.852766346593508e-11, rate1 = 1.9509221155311692e-11, rate2 = 3.9018442310623384e-11\n","Iteration 8: losslow = 0.6931471805599445, losshigh = 8.305593820797938, loss1 = 8.175079800428046, loss2 = 8.278500525412644\n","Iteration 9: low = 0, high = 3.9018442310623384e-11, rate1 = 1.3006147436874461e-11, rate2 = 2.6012294873748922e-11\n","Iteration 9: losslow = 0.6931471805599445, losshigh = 8.278500525412644, loss1 = 7.930695820665173, loss2 = 8.229070810442767\n","Iteration 10: low = 0, high = 2.6012294873748922e-11, rate1 = 8.670764957916307e-12, rate2 = 1.7341529915832614e-11\n","Iteration 10: losslow = 0.6931471805599445, losshigh = 8.229070810442767, loss1 = 7.609353783793586, loss2 = 8.103261922028732\n","Iteration 11: low = 0, high = 1.7341529915832614e-11, rate1 = 5.7805099719442045e-12, rate2 = 1.1561019943888409e-11\n","Iteration 11: losslow = 0.6931471805599445, losshigh = 8.103261922028732, loss1 = 6.020945372842861, loss2 = 7.865386136788327\n","Iteration 12: low = 0, high = 1.1561019943888409e-11, rate1 = 3.8536733146294694e-12, rate2 = 7.707346629258939e-12\n","Iteration 12: losslow = 0.6931471805599445, losshigh = 7.865386136788327, loss1 = 4.801374937340956, loss2 = 7.108107030112255\n","Iteration 13: low = 0, high = 7.707346629258939e-12, rate1 = 2.569115543086313e-12, rate2 = 5.138231086172626e-12\n","Iteration 13: losslow = 0.6931471805599445, losshigh = 7.108107030112255, loss1 = 3.26433279159497, loss2 = 5.650807431046152\n","Iteration 14: low = 0, high = 5.138231086172626e-12, rate1 = 1.7127436953908755e-12, rate2 = 3.425487390781751e-12\n","Iteration 14: losslow = 0.6931471805599445, losshigh = 5.650807431046152, loss1 = 2.212044193815273, loss2 = 4.2887859749270385\n","Iteration 15: low = 0, high = 3.425487390781751e-12, rate1 = 1.1418291302605836e-12, rate2 = 2.2836582605211673e-12\n","Iteration 15: losslow = 0.6931471805599445, losshigh = 4.2887859749270385, loss1 = 1.508376389868376, loss2 = 2.9213626445068663\n","Iteration 16: low = 0, high = 2.2836582605211673e-12, rate1 = 7.612194201737225e-13, rate2 = 1.522438840347445e-12\n","Iteration 16: losslow = 0.6931471805599445, losshigh = 2.9213626445068663, loss1 = 1.0626663502678064, loss2 = 1.975285552091149\n","Iteration 17: low = 0, high = 1.522438840347445e-12, rate1 = 5.074796134491483e-13, rate2 = 1.0149592268982966e-12\n","Iteration 17: losslow = 0.6931471805599445, losshigh = 1.975285552091149, loss1 = 0.79844627831121, loss2 = 1.356238346008884\n","Iteration 18: low = 0, high = 1.0149592268982966e-12, rate1 = 3.383197422994322e-13, rate2 = 6.766394845988644e-13\n","Iteration 18: losslow = 0.6931471805599445, losshigh = 1.356238346008884, loss1 = 0.6594932149757178, loss2 = 0.9699801811830013\n","Iteration 19: low = 0, high = 6.766394845988644e-13, rate1 = 2.2554649486628812e-13, rate2 = 4.5109298973257623e-13\n","Iteration 19: losslow = 0.6931471805599445, losshigh = 0.9699801811830013, loss1 = 0.601774801915207, loss2 = 0.7473252095068597\n","Iteration 20: low = 0, high = 4.5109298973257623e-13, rate1 = 1.5036432991085874e-13, rate2 = 3.0072865982171747e-13\n","Iteration 20: losslow = 0.6931471805599445, losshigh = 0.7473252095068597, loss1 = 0.5911870895696287, loss2 = 0.6360462091590806\n","1.5036432991085874e-13\n","Iteration 1, Loss: 0.5911870895696287\n","Iteration 51, Loss: 0.5686196216754295\n","Iteration 101, Loss: 0.5685996671480068\n","Iteration 151, Loss: 0.5685889633583242\n"]}],"source":["# @title\n","import numpy as np\n","\n","def calculate_loss(y_true, y_pred):\n","    m = y_true.shape[0]\n","    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n","    loss = -np.sum(y_true * np.log(y_pred)) / (2*m)\n","    return loss\n","\n","def softmax(x):\n","    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n","    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n","\n","\n","\n","\n","\n","def ternary_search(X, Y, W, base=1e-9, max_iter=20):\n","    low = 0\n","    high = base\n","\n","    gradient = calculate_gradient(X, Y, get_prediction(X, W))\n","\n","    # Expand the high boundary if necessary\n","    while calculate_loss(Y, get_prediction(X, W)) > calculate_loss(Y, get_prediction(X, W - high * gradient)):\n","        high *= 2\n","        if high > 1e6:  # Prevents the search from expanding indefinitely\n","            break\n","\n","    for i in range(max_iter):\n","        rate_1 = (2 * low + high) / 3\n","        rate_2 = (2 * high + low) / 3\n","\n","        losslow = calculate_loss(Y, get_prediction(X, W - low * gradient))\n","        losshigh = calculate_loss(Y, get_prediction(X, W - high * gradient))\n","        loss1 = calculate_loss(Y, get_prediction(X, W - rate_1 * gradient))\n","        loss2 = calculate_loss(Y, get_prediction(X, W - rate_2 * gradient))\n","\n","        print(f\"Iteration {i+1}: low = {low}, high = {high}, rate1 = {rate_1}, rate2 = {rate_2}\")\n","        print(f\"Iteration {i+1}: losslow = {losslow}, losshigh = {losshigh}, loss1 = {loss1}, loss2 = {loss2}\")\n","\n","        if loss1 < loss2:\n","            high = rate_2\n","        elif loss1 > loss2:\n","            low = rate_1\n","        else:\n","            low = rate_1\n","            high = rate_2\n","\n","    return (low + high) / 2\n","\n","\n","W = np.zeros((n_features, num_class))  # Random initialization\n","\n","# Find the best learning rate using ternary search\n","learning_rate = ternary_search(X_train, y_train_oh, W)\n","print(learning_rate)\n","epsilon = 1e-7\n","\n","for i in range(200):\n","    y_pred = get_prediction(X_train, W)\n","    loss_old = calculate_loss(y_train_oh, y_pred)\n","    gradient = calculate_gradient(X_train, y_train_oh, y_pred)\n","\n","    # Update weights\n","    Wnew = W - learning_rate * gradient\n","    y_pred_new = get_prediction(X_train, Wnew)\n","    loss_new = calculate_loss(y_train_oh, y_pred_new)\n","\n","    if loss_new < loss_old:\n","        W = Wnew\n","    else:\n","        print(\"Loss did not decrease. Halving learning rate.\")\n","        learning_rate /= 2  # Halve the learning rate if loss doesn't decrease\n","\n","    if i% 50 == 0:\n","        print(f\"Iteration {i+1}, Loss: {loss_new}\")\n","\n","    if np.abs(loss_new - loss_old) < epsilon:\n","        print(f\"Converged after {i+1} iterations\")\n","        break\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"},"colab":{"provenance":[],"gpuType":"V28"},"accelerator":"TPU"},"nbformat":4,"nbformat_minor":5}