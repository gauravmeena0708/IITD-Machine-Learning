{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c17ac5f2",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-10-21T11:53:16.436824Z",
     "iopub.status.busy": "2024-10-21T11:53:16.436439Z",
     "iopub.status.idle": "2024-10-21T11:53:24.435568Z",
     "shell.execute_reply": "2024-10-21T11:53:24.434341Z"
    },
    "papermill": {
     "duration": 8.005449,
     "end_time": "2024-10-21T11:53:24.437987",
     "exception": false,
     "start_time": "2024-10-21T11:53:16.432538",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp -r /kaggle/input/col774a3/* /kaggle/working/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31080da7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-21T11:53:24.444438Z",
     "iopub.status.busy": "2024-10-21T11:53:24.444079Z",
     "iopub.status.idle": "2024-10-21T11:53:24.455131Z",
     "shell.execute_reply": "2024-10-21T11:53:24.454295Z"
    },
    "papermill": {
     "duration": 0.016508,
     "end_time": "2024-10-21T11:53:24.456984",
     "exception": false,
     "start_time": "2024-10-21T11:53:24.440476",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_scheduler(optimizer, scheduler_type='step', **kwargs):\n",
    "    \"\"\"\n",
    "    Returns the specified learning rate scheduler.\n",
    "\n",
    "    Args:\n",
    "        optimizer: The optimizer for which to schedule the learning rate.\n",
    "        scheduler_type (str): Type of the scheduler ('step', 'cosine', 'plateau', 'exponential', 'cyclic').\n",
    "        **kwargs: Additional arguments depending on the scheduler type.\n",
    "\n",
    "    Returns:\n",
    "        A learning rate scheduler.\n",
    "    \"\"\"\n",
    "    if scheduler_type == 'step':\n",
    "        step_size = kwargs.get('step_size', 30)\n",
    "        gamma = kwargs.get('gamma', 0.1)\n",
    "        return torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "    elif scheduler_type == 'cosine':\n",
    "        T_max = kwargs.get('T_max', 100)\n",
    "        eta_min = kwargs.get('eta_min', 0.0)\n",
    "        return torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_max, eta_min=eta_min)\n",
    "\n",
    "    elif scheduler_type == 'plateau':\n",
    "        patience = kwargs.get('patience', 10)\n",
    "        factor = kwargs.get('factor', 0.1)\n",
    "        return torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=patience, factor=factor)\n",
    "\n",
    "    elif scheduler_type == 'exponential':\n",
    "        gamma = kwargs.get('gamma', 0.9)\n",
    "        return torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)\n",
    "\n",
    "    elif scheduler_type == 'cyclic':\n",
    "        base_lr = kwargs.get('base_lr', 0.001)\n",
    "        max_lr = kwargs.get('max_lr', 0.1)\n",
    "        step_size_up = kwargs.get('step_size_up', 2000)\n",
    "        step_size_down = kwargs.get('step_size_down', step_size_up)  # Defaults to equal steps up/down\n",
    "        mode = kwargs.get('mode', 'triangular')\n",
    "        return torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=base_lr, max_lr=max_lr,\n",
    "                                                 step_size_up=step_size_up, step_size_down=step_size_down, mode=mode)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported scheduler type: {scheduler_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f287653",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-21T11:53:24.462845Z",
     "iopub.status.busy": "2024-10-21T11:53:24.462420Z",
     "iopub.status.idle": "2024-10-21T12:53:31.658800Z",
     "shell.execute_reply": "2024-10-21T12:53:31.657328Z"
    },
    "papermill": {
     "duration": 3607.202075,
     "end_time": "2024-10-21T12:53:31.661138",
     "exception": false,
     "start_time": "2024-10-21T11:53:24.459063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 0, Loss: 5.073, Acc: 0.781%\n",
      "Epoch 0, Batch 100, Loss: 4.684, Acc: 1.160%\n",
      "Epoch 0, Batch 200, Loss: 4.626, Acc: 1.290%\n",
      "Epoch 0, Batch 300, Loss: 4.587, Acc: 1.412%\n",
      "Epoch 0 Training Loss: 4.583, Accuracy: 1.417%\n",
      "Epoch 0, Test Accuracy: 3.04%\n",
      "New best model saved with accuracy: 3.04%\n",
      "Epoch 1, Batch 0, Loss: 4.500, Acc: 2.344%\n",
      "Epoch 1, Batch 100, Loss: 4.435, Acc: 2.398%\n",
      "Epoch 1, Batch 200, Loss: 4.353, Acc: 2.771%\n",
      "Epoch 1, Batch 300, Loss: 4.290, Acc: 3.226%\n",
      "Epoch 1 Training Loss: 4.281, Accuracy: 3.268%\n",
      "Epoch 1, Test Accuracy: 5.80%\n",
      "New best model saved with accuracy: 5.80%\n",
      "Epoch 2, Batch 0, Loss: 4.249, Acc: 3.906%\n",
      "Epoch 2, Batch 100, Loss: 4.062, Acc: 5.121%\n",
      "Epoch 2, Batch 200, Loss: 4.019, Acc: 5.578%\n",
      "Epoch 2, Batch 300, Loss: 3.971, Acc: 6.170%\n",
      "Epoch 2 Training Loss: 3.966, Accuracy: 6.200%\n",
      "Epoch 2, Test Accuracy: 10.52%\n",
      "New best model saved with accuracy: 10.52%\n",
      "Epoch 3, Batch 0, Loss: 3.644, Acc: 12.500%\n",
      "Epoch 3, Batch 100, Loss: 3.802, Acc: 8.161%\n",
      "Epoch 3, Batch 200, Loss: 3.763, Acc: 8.738%\n",
      "Epoch 3, Batch 300, Loss: 3.730, Acc: 9.336%\n",
      "Epoch 3 Training Loss: 3.725, Accuracy: 9.395%\n",
      "Epoch 3, Test Accuracy: 13.61%\n",
      "New best model saved with accuracy: 13.61%\n",
      "Epoch 4, Batch 0, Loss: 3.666, Acc: 12.500%\n",
      "Epoch 4, Batch 100, Loss: 3.591, Acc: 11.394%\n",
      "Epoch 4, Batch 200, Loss: 3.562, Acc: 11.629%\n",
      "Epoch 4, Batch 300, Loss: 3.530, Acc: 11.945%\n",
      "Epoch 4 Training Loss: 3.525, Accuracy: 11.998%\n",
      "Epoch 4, Test Accuracy: 15.29%\n",
      "New best model saved with accuracy: 15.29%\n",
      "Epoch 5, Batch 0, Loss: 3.351, Acc: 12.500%\n",
      "Epoch 5, Batch 100, Loss: 3.412, Acc: 13.598%\n",
      "Epoch 5, Batch 200, Loss: 3.371, Acc: 13.899%\n",
      "Epoch 5, Batch 300, Loss: 3.336, Acc: 14.548%\n",
      "Epoch 5 Training Loss: 3.331, Accuracy: 14.640%\n",
      "Epoch 5, Test Accuracy: 17.83%\n",
      "New best model saved with accuracy: 17.83%\n",
      "Epoch 6, Batch 0, Loss: 3.406, Acc: 15.625%\n",
      "Epoch 6, Batch 100, Loss: 3.207, Acc: 16.917%\n",
      "Epoch 6, Batch 200, Loss: 3.175, Acc: 17.114%\n",
      "Epoch 6, Batch 300, Loss: 3.149, Acc: 17.522%\n",
      "Epoch 6 Training Loss: 3.143, Accuracy: 17.565%\n",
      "Epoch 6, Test Accuracy: 24.02%\n",
      "New best model saved with accuracy: 24.02%\n",
      "Epoch 7, Batch 0, Loss: 3.108, Acc: 15.625%\n",
      "Epoch 7, Batch 100, Loss: 3.015, Acc: 19.276%\n",
      "Epoch 7, Batch 200, Loss: 3.002, Acc: 19.593%\n",
      "Epoch 7, Batch 300, Loss: 2.979, Acc: 19.978%\n",
      "Epoch 7 Training Loss: 2.979, Accuracy: 19.955%\n",
      "Epoch 7, Test Accuracy: 26.21%\n",
      "New best model saved with accuracy: 26.21%\n",
      "Epoch 8, Batch 0, Loss: 3.109, Acc: 19.531%\n",
      "Epoch 8, Batch 100, Loss: 2.894, Acc: 21.326%\n",
      "Epoch 8, Batch 200, Loss: 2.849, Acc: 22.100%\n",
      "Epoch 8, Batch 300, Loss: 2.828, Acc: 22.314%\n",
      "Epoch 8 Training Loss: 2.826, Accuracy: 22.392%\n",
      "Epoch 8, Test Accuracy: 24.84%\n",
      "No improvement for 1 epoch(s)\n",
      "Epoch 9, Batch 0, Loss: 2.832, Acc: 24.219%\n",
      "Epoch 9, Batch 100, Loss: 2.706, Acc: 24.466%\n",
      "Epoch 9, Batch 200, Loss: 2.695, Acc: 24.541%\n",
      "Epoch 9, Batch 300, Loss: 2.683, Acc: 24.743%\n",
      "Epoch 9 Training Loss: 2.676, Accuracy: 24.863%\n",
      "Epoch 9, Test Accuracy: 30.80%\n",
      "New best model saved with accuracy: 30.80%\n",
      "Epoch 10, Batch 0, Loss: 2.688, Acc: 24.219%\n",
      "Epoch 10, Batch 100, Loss: 2.623, Acc: 25.874%\n",
      "Epoch 10, Batch 200, Loss: 2.598, Acc: 26.489%\n",
      "Epoch 10, Batch 300, Loss: 2.580, Acc: 26.809%\n",
      "Epoch 10 Training Loss: 2.579, Accuracy: 26.810%\n",
      "Epoch 10, Test Accuracy: 32.03%\n",
      "New best model saved with accuracy: 32.03%\n",
      "Epoch 11, Batch 0, Loss: 2.662, Acc: 27.344%\n",
      "Epoch 11, Batch 100, Loss: 2.497, Acc: 28.620%\n",
      "Epoch 11, Batch 200, Loss: 2.492, Acc: 28.619%\n",
      "Epoch 11, Batch 300, Loss: 2.490, Acc: 28.413%\n",
      "Epoch 11 Training Loss: 2.488, Accuracy: 28.442%\n",
      "Epoch 11, Test Accuracy: 31.07%\n",
      "No improvement for 1 epoch(s)\n",
      "Epoch 12, Batch 0, Loss: 2.448, Acc: 26.562%\n",
      "Epoch 12, Batch 100, Loss: 2.455, Acc: 28.821%\n",
      "Epoch 12, Batch 200, Loss: 2.431, Acc: 29.338%\n",
      "Epoch 12, Batch 300, Loss: 2.406, Acc: 29.825%\n",
      "Epoch 12 Training Loss: 2.406, Accuracy: 29.832%\n",
      "Epoch 12, Test Accuracy: 34.63%\n",
      "New best model saved with accuracy: 34.63%\n",
      "Epoch 13, Batch 0, Loss: 2.296, Acc: 32.812%\n",
      "Epoch 13, Batch 100, Loss: 2.332, Acc: 30.933%\n",
      "Epoch 13, Batch 200, Loss: 2.340, Acc: 30.694%\n",
      "Epoch 13, Batch 300, Loss: 2.343, Acc: 31.006%\n",
      "Epoch 13 Training Loss: 2.339, Accuracy: 31.090%\n",
      "Epoch 13, Test Accuracy: 37.78%\n",
      "New best model saved with accuracy: 37.78%\n",
      "Epoch 14, Batch 0, Loss: 2.087, Acc: 35.938%\n",
      "Epoch 14, Batch 100, Loss: 2.282, Acc: 32.031%\n",
      "Epoch 14, Batch 200, Loss: 2.282, Acc: 31.825%\n",
      "Epoch 14, Batch 300, Loss: 2.282, Acc: 32.088%\n",
      "Epoch 14 Training Loss: 2.282, Accuracy: 32.127%\n",
      "Epoch 14, Test Accuracy: 36.88%\n",
      "No improvement for 1 epoch(s)\n",
      "Epoch 15, Batch 0, Loss: 2.372, Acc: 34.375%\n",
      "Epoch 15, Batch 100, Loss: 2.237, Acc: 33.261%\n",
      "Epoch 15, Batch 200, Loss: 2.227, Acc: 33.306%\n",
      "Epoch 15, Batch 300, Loss: 2.224, Acc: 33.350%\n",
      "Epoch 15 Training Loss: 2.225, Accuracy: 33.370%\n",
      "Epoch 15, Test Accuracy: 38.16%\n",
      "New best model saved with accuracy: 38.16%\n",
      "Epoch 16, Batch 0, Loss: 2.040, Acc: 36.719%\n",
      "Epoch 16, Batch 100, Loss: 2.170, Acc: 34.360%\n",
      "Epoch 16, Batch 200, Loss: 2.187, Acc: 34.080%\n",
      "Epoch 16, Batch 300, Loss: 2.181, Acc: 34.128%\n",
      "Epoch 16 Training Loss: 2.179, Accuracy: 34.153%\n",
      "Epoch 16, Test Accuracy: 39.80%\n",
      "New best model saved with accuracy: 39.80%\n",
      "Epoch 17, Batch 0, Loss: 2.009, Acc: 43.750%\n",
      "Epoch 17, Batch 100, Loss: 2.140, Acc: 35.280%\n",
      "Epoch 17, Batch 200, Loss: 2.135, Acc: 35.211%\n",
      "Epoch 17, Batch 300, Loss: 2.135, Acc: 35.172%\n",
      "Epoch 17 Training Loss: 2.135, Accuracy: 35.170%\n",
      "Epoch 17, Test Accuracy: 32.27%\n",
      "No improvement for 1 epoch(s)\n",
      "Epoch 18, Batch 0, Loss: 2.049, Acc: 39.062%\n",
      "Epoch 18, Batch 100, Loss: 2.112, Acc: 35.481%\n",
      "Epoch 18, Batch 200, Loss: 2.107, Acc: 35.759%\n",
      "Epoch 18, Batch 300, Loss: 2.107, Acc: 35.810%\n",
      "Epoch 18 Training Loss: 2.109, Accuracy: 35.735%\n",
      "Epoch 18, Test Accuracy: 37.34%\n",
      "No improvement for 2 epoch(s)\n",
      "Epoch 19, Batch 0, Loss: 2.074, Acc: 36.719%\n",
      "Epoch 19, Batch 100, Loss: 2.040, Acc: 36.541%\n",
      "Epoch 19, Batch 200, Loss: 2.068, Acc: 36.027%\n",
      "Epoch 19, Batch 300, Loss: 2.069, Acc: 36.179%\n",
      "Epoch 19 Training Loss: 2.068, Accuracy: 36.220%\n",
      "Epoch 19, Test Accuracy: 40.70%\n",
      "New best model saved with accuracy: 40.70%\n",
      "Epoch 20, Batch 0, Loss: 1.773, Acc: 41.406%\n",
      "Epoch 20, Batch 100, Loss: 2.014, Acc: 37.732%\n",
      "Epoch 20, Batch 200, Loss: 2.031, Acc: 37.150%\n",
      "Epoch 20, Batch 300, Loss: 2.038, Acc: 36.846%\n",
      "Epoch 20 Training Loss: 2.036, Accuracy: 36.895%\n",
      "Epoch 20, Test Accuracy: 38.80%\n",
      "No improvement for 1 epoch(s)\n",
      "Epoch 21, Batch 0, Loss: 2.279, Acc: 35.156%\n",
      "Epoch 21, Batch 100, Loss: 1.981, Acc: 38.196%\n",
      "Epoch 21, Batch 200, Loss: 2.000, Acc: 38.235%\n",
      "Epoch 21, Batch 300, Loss: 1.999, Acc: 37.876%\n",
      "Epoch 21 Training Loss: 2.001, Accuracy: 37.840%\n",
      "Epoch 21, Test Accuracy: 42.54%\n",
      "New best model saved with accuracy: 42.54%\n",
      "Epoch 22, Batch 0, Loss: 1.762, Acc: 41.406%\n",
      "Epoch 22, Batch 100, Loss: 1.959, Acc: 38.745%\n",
      "Epoch 22, Batch 200, Loss: 1.951, Acc: 38.596%\n",
      "Epoch 22, Batch 300, Loss: 1.949, Acc: 38.826%\n",
      "Epoch 22 Training Loss: 1.950, Accuracy: 38.773%\n",
      "Epoch 22, Test Accuracy: 41.70%\n",
      "No improvement for 1 epoch(s)\n",
      "Epoch 23, Batch 0, Loss: 1.885, Acc: 35.156%\n",
      "Epoch 23, Batch 100, Loss: 1.895, Acc: 39.921%\n",
      "Epoch 23, Batch 200, Loss: 1.901, Acc: 39.782%\n",
      "Epoch 23, Batch 300, Loss: 1.912, Acc: 39.626%\n",
      "Epoch 23 Training Loss: 1.914, Accuracy: 39.605%\n",
      "Epoch 23, Test Accuracy: 45.90%\n",
      "New best model saved with accuracy: 45.90%\n",
      "Epoch 24, Batch 0, Loss: 1.866, Acc: 41.406%\n",
      "Epoch 24, Batch 100, Loss: 1.879, Acc: 40.238%\n",
      "Epoch 24, Batch 200, Loss: 1.878, Acc: 40.221%\n",
      "Epoch 24, Batch 300, Loss: 1.883, Acc: 40.319%\n",
      "Epoch 24 Training Loss: 1.883, Accuracy: 40.335%\n",
      "Epoch 24, Test Accuracy: 45.69%\n",
      "No improvement for 1 epoch(s)\n",
      "Epoch 25, Batch 0, Loss: 62.641, Acc: 52.344%\n",
      "Epoch 25, Batch 100, Loss: 2.433, Acc: 41.592%\n",
      "Epoch 25, Batch 200, Loss: 2.142, Acc: 40.979%\n",
      "Epoch 25, Batch 300, Loss: 2.049, Acc: 40.835%\n",
      "Epoch 25 Training Loss: 2.043, Accuracy: 40.792%\n",
      "Epoch 25, Test Accuracy: 42.60%\n",
      "No improvement for 2 epoch(s)\n",
      "Epoch 26, Batch 0, Loss: 1.622, Acc: 46.875%\n",
      "Epoch 26, Batch 100, Loss: 1.806, Acc: 41.932%\n",
      "Epoch 26, Batch 200, Loss: 1.802, Acc: 42.273%\n",
      "Epoch 26, Batch 300, Loss: 1.806, Acc: 42.234%\n",
      "Epoch 26 Training Loss: 1.803, Accuracy: 42.303%\n",
      "Epoch 26, Test Accuracy: 45.98%\n",
      "New best model saved with accuracy: 45.98%\n",
      "Epoch 27, Batch 0, Loss: 1.752, Acc: 42.188%\n",
      "Epoch 27, Batch 100, Loss: 1.785, Acc: 42.365%\n",
      "Epoch 27, Batch 200, Loss: 1.785, Acc: 42.425%\n",
      "Epoch 27, Batch 300, Loss: 1.783, Acc: 42.509%\n",
      "Epoch 27 Training Loss: 1.781, Accuracy: 42.587%\n",
      "Epoch 27, Test Accuracy: 45.31%\n",
      "No improvement for 1 epoch(s)\n",
      "Epoch 28, Batch 0, Loss: 1.539, Acc: 49.219%\n",
      "Epoch 28, Batch 100, Loss: 1.744, Acc: 43.820%\n",
      "Epoch 28, Batch 200, Loss: 1.743, Acc: 43.668%\n",
      "Epoch 28, Batch 300, Loss: 1.746, Acc: 43.548%\n",
      "Epoch 28 Training Loss: 1.744, Accuracy: 43.587%\n",
      "Epoch 28, Test Accuracy: 49.47%\n",
      "New best model saved with accuracy: 49.47%\n",
      "Epoch 29, Batch 0, Loss: 1.502, Acc: 48.438%\n",
      "Epoch 29, Batch 100, Loss: 1.700, Acc: 44.276%\n",
      "Epoch 29, Batch 200, Loss: 1.710, Acc: 44.248%\n",
      "Epoch 29, Batch 300, Loss: 1.708, Acc: 44.222%\n",
      "Epoch 29 Training Loss: 1.712, Accuracy: 44.110%\n",
      "Epoch 29, Test Accuracy: 46.95%\n",
      "No improvement for 1 epoch(s)\n",
      "Epoch 30, Batch 0, Loss: 1.736, Acc: 47.656%\n",
      "Epoch 30, Batch 100, Loss: 1.670, Acc: 45.065%\n",
      "Epoch 30, Batch 200, Loss: 1.683, Acc: 44.590%\n",
      "Epoch 30, Batch 300, Loss: 1.683, Acc: 44.858%\n",
      "Epoch 30 Training Loss: 1.685, Accuracy: 44.867%\n",
      "Epoch 30, Test Accuracy: 50.22%\n",
      "New best model saved with accuracy: 50.22%\n",
      "Epoch 31, Batch 0, Loss: 56.348, Acc: 57.031%\n",
      "Epoch 31, Batch 100, Loss: 2.184, Acc: 45.962%\n",
      "Epoch 31, Batch 200, Loss: 1.919, Acc: 45.736%\n",
      "Epoch 31, Batch 300, Loss: 1.828, Acc: 45.619%\n",
      "Epoch 31 Training Loss: 1.820, Accuracy: 45.650%\n",
      "Epoch 31, Test Accuracy: 50.66%\n",
      "New best model saved with accuracy: 50.66%\n",
      "Epoch 32, Batch 0, Loss: 1.654, Acc: 47.656%\n",
      "Epoch 32, Batch 100, Loss: 1.581, Acc: 47.022%\n",
      "Epoch 32, Batch 200, Loss: 1.584, Acc: 47.287%\n",
      "Epoch 32, Batch 300, Loss: 1.596, Acc: 46.974%\n",
      "Epoch 32 Training Loss: 1.596, Accuracy: 46.955%\n",
      "Epoch 32, Test Accuracy: 52.32%\n",
      "New best model saved with accuracy: 52.32%\n",
      "Epoch 33, Batch 0, Loss: 59.350, Acc: 54.688%\n",
      "Epoch 33, Batch 100, Loss: 2.100, Acc: 48.345%\n",
      "Epoch 33, Batch 200, Loss: 1.827, Acc: 48.130%\n",
      "Epoch 33, Batch 300, Loss: 1.747, Acc: 47.905%\n",
      "Epoch 33 Training Loss: 1.742, Accuracy: 47.837%\n",
      "Epoch 33, Test Accuracy: 53.72%\n",
      "New best model saved with accuracy: 53.72%\n",
      "Epoch 34, Batch 0, Loss: 1.479, Acc: 44.531%\n",
      "Epoch 34, Batch 100, Loss: 1.505, Acc: 48.399%\n",
      "Epoch 34, Batch 200, Loss: 1.502, Acc: 48.799%\n",
      "Epoch 34, Batch 300, Loss: 1.502, Acc: 48.884%\n",
      "Epoch 34 Training Loss: 1.506, Accuracy: 48.792%\n",
      "Epoch 34, Test Accuracy: 52.52%\n",
      "No improvement for 1 epoch(s)\n",
      "Epoch 35, Batch 0, Loss: 56.140, Acc: 57.031%\n",
      "Epoch 35, Batch 100, Loss: 1.957, Acc: 51.315%\n",
      "Epoch 35, Batch 200, Loss: 1.710, Acc: 50.766%\n",
      "Epoch 35, Batch 300, Loss: 1.631, Acc: 50.462%\n",
      "Epoch 35 Training Loss: 1.624, Accuracy: 50.465%\n",
      "Epoch 35, Test Accuracy: 54.56%\n",
      "New best model saved with accuracy: 54.56%\n",
      "Epoch 36, Batch 0, Loss: 1.458, Acc: 46.875%\n",
      "Epoch 36, Batch 100, Loss: 1.392, Acc: 51.477%\n",
      "Epoch 36, Batch 200, Loss: 1.399, Acc: 51.407%\n",
      "Epoch 36, Batch 300, Loss: 1.400, Acc: 51.479%\n",
      "Epoch 36 Training Loss: 1.402, Accuracy: 51.535%\n",
      "Epoch 36, Test Accuracy: 54.16%\n",
      "No improvement for 1 epoch(s)\n",
      "Epoch 37, Batch 0, Loss: 1.350, Acc: 48.438%\n",
      "Epoch 37, Batch 100, Loss: 1.330, Acc: 52.816%\n",
      "Epoch 37, Batch 200, Loss: 1.313, Acc: 53.393%\n",
      "Epoch 37, Batch 300, Loss: 1.321, Acc: 53.470%\n",
      "Epoch 37 Training Loss: 1.321, Accuracy: 53.480%\n",
      "Epoch 37, Test Accuracy: 58.84%\n",
      "New best model saved with accuracy: 58.84%\n",
      "Epoch 38, Batch 0, Loss: 64.366, Acc: 50.781%\n",
      "Epoch 38, Batch 100, Loss: 1.884, Acc: 55.330%\n",
      "Epoch 38, Batch 200, Loss: 1.555, Acc: 55.636%\n",
      "Epoch 38, Batch 300, Loss: 1.444, Acc: 55.822%\n",
      "Epoch 38 Training Loss: 1.435, Accuracy: 55.837%\n",
      "Epoch 38, Test Accuracy: 59.50%\n",
      "New best model saved with accuracy: 59.50%\n",
      "Epoch 39, Batch 0, Loss: 59.338, Acc: 54.688%\n",
      "Epoch 39, Batch 100, Loss: 1.711, Acc: 58.431%\n",
      "Epoch 39, Batch 200, Loss: 1.435, Acc: 58.174%\n",
      "Epoch 39, Batch 300, Loss: 1.348, Acc: 57.838%\n",
      "Epoch 39 Training Loss: 1.343, Accuracy: 57.737%\n",
      "Epoch 39, Test Accuracy: 61.09%\n",
      "New best model saved with accuracy: 61.09%\n",
      "Epoch 40, Batch 0, Loss: 56.228, Acc: 57.031%\n",
      "Epoch 40, Batch 100, Loss: 1.633, Acc: 59.375%\n",
      "Epoch 40, Batch 200, Loss: 1.367, Acc: 59.356%\n",
      "Epoch 40, Batch 300, Loss: 1.268, Acc: 59.583%\n",
      "Epoch 40 Training Loss: 1.260, Accuracy: 59.557%\n",
      "Epoch 40, Test Accuracy: 62.77%\n",
      "New best model saved with accuracy: 62.77%\n",
      "Epoch 41, Batch 0, Loss: 50.003, Acc: 61.719%\n",
      "Epoch 41, Batch 100, Loss: 1.594, Acc: 58.640%\n",
      "Epoch 41, Batch 200, Loss: 1.357, Acc: 58.850%\n",
      "Epoch 41, Batch 300, Loss: 1.279, Acc: 58.817%\n",
      "Epoch 41 Training Loss: 1.276, Accuracy: 58.758%\n",
      "Epoch 41, Test Accuracy: 61.69%\n",
      "No improvement for 1 epoch(s)\n",
      "Epoch 42, Batch 0, Loss: 56.206, Acc: 57.031%\n",
      "Epoch 42, Batch 100, Loss: 1.733, Acc: 56.683%\n",
      "Epoch 42, Batch 200, Loss: 1.466, Acc: 56.437%\n",
      "Epoch 42, Batch 300, Loss: 1.375, Acc: 56.650%\n",
      "Epoch 42 Training Loss: 1.370, Accuracy: 56.557%\n",
      "Epoch 42, Test Accuracy: 60.24%\n",
      "No improvement for 2 epoch(s)\n",
      "Epoch 43, Batch 0, Loss: 59.355, Acc: 54.688%\n",
      "Epoch 43, Batch 100, Loss: 1.820, Acc: 55.461%\n",
      "Epoch 43, Batch 200, Loss: 1.540, Acc: 55.399%\n",
      "Epoch 43, Batch 300, Loss: 1.464, Acc: 54.812%\n",
      "Epoch 43 Training Loss: 1.455, Accuracy: 54.852%\n",
      "Epoch 43, Test Accuracy: 58.42%\n",
      "No improvement for 3 epoch(s)\n",
      "Epoch 44, Batch 0, Loss: 59.190, Acc: 54.688%\n",
      "Epoch 44, Batch 100, Loss: 1.885, Acc: 53.349%\n",
      "Epoch 44, Batch 200, Loss: 1.618, Acc: 52.833%\n",
      "Epoch 44, Batch 300, Loss: 1.539, Acc: 52.455%\n",
      "Epoch 44 Training Loss: 1.534, Accuracy: 52.430%\n",
      "Epoch 44, Test Accuracy: 55.84%\n",
      "No improvement for 4 epoch(s)\n",
      "Epoch 45, Batch 0, Loss: 62.408, Acc: 52.344%\n",
      "Epoch 45, Batch 100, Loss: 1.985, Acc: 51.694%\n",
      "Epoch 45, Batch 200, Loss: 1.716, Acc: 50.902%\n",
      "Epoch 45, Batch 300, Loss: 1.631, Acc: 50.690%\n",
      "Epoch 45 Training Loss: 1.623, Accuracy: 50.700%\n",
      "Epoch 45, Test Accuracy: 53.47%\n",
      "No improvement for 5 epoch(s)\n",
      "Epoch 46, Batch 0, Loss: 58.460, Acc: 55.469%\n",
      "Epoch 46, Batch 100, Loss: 2.013, Acc: 49.884%\n",
      "Epoch 46, Batch 200, Loss: 1.760, Acc: 49.242%\n",
      "Epoch 46, Batch 300, Loss: 1.652, Acc: 49.779%\n",
      "Epoch 46 Training Loss: 1.646, Accuracy: 49.748%\n",
      "Epoch 46, Test Accuracy: 52.10%\n",
      "No improvement for 6 epoch(s)\n",
      "Epoch 47, Batch 0, Loss: 1.531, Acc: 46.094%\n",
      "Epoch 47, Batch 100, Loss: 1.480, Acc: 49.397%\n",
      "Epoch 47, Batch 200, Loss: 1.498, Acc: 49.067%\n",
      "Epoch 47, Batch 300, Loss: 1.510, Acc: 48.736%\n",
      "Epoch 47 Training Loss: 1.511, Accuracy: 48.735%\n",
      "Epoch 47, Test Accuracy: 46.51%\n",
      "No improvement for 7 epoch(s)\n",
      "Epoch 48, Batch 0, Loss: 59.277, Acc: 54.688%\n",
      "Epoch 48, Batch 100, Loss: 2.087, Acc: 48.847%\n",
      "Epoch 48, Batch 200, Loss: 1.818, Acc: 48.220%\n",
      "Epoch 48, Batch 300, Loss: 1.734, Acc: 47.963%\n",
      "Epoch 48 Training Loss: 1.728, Accuracy: 47.950%\n",
      "Epoch 48, Test Accuracy: 50.56%\n",
      "No improvement for 8 epoch(s)\n",
      "Epoch 49, Batch 0, Loss: 1.824, Acc: 42.969%\n",
      "Epoch 49, Batch 100, Loss: 1.542, Acc: 48.020%\n",
      "Epoch 49, Batch 200, Loss: 1.556, Acc: 47.512%\n",
      "Epoch 49, Batch 300, Loss: 1.566, Acc: 47.685%\n",
      "Epoch 49 Training Loss: 1.566, Accuracy: 47.712%\n",
      "Epoch 49, Test Accuracy: 50.01%\n",
      "No improvement for 9 epoch(s)\n",
      "Epoch 50, Batch 0, Loss: 1.851, Acc: 39.844%\n",
      "Epoch 50, Batch 100, Loss: 1.616, Acc: 46.403%\n",
      "Epoch 50, Batch 200, Loss: 1.605, Acc: 46.758%\n",
      "Epoch 50, Batch 300, Loss: 1.606, Acc: 46.496%\n",
      "Epoch 50 Training Loss: 1.603, Accuracy: 46.532%\n",
      "Epoch 50, Test Accuracy: 48.34%\n",
      "No improvement for 10 epoch(s)\n",
      "Epoch 51, Batch 0, Loss: 1.439, Acc: 45.312%\n",
      "Epoch 51, Batch 100, Loss: 1.557, Acc: 47.857%\n",
      "Epoch 51, Batch 200, Loss: 1.571, Acc: 47.392%\n",
      "Epoch 51, Batch 300, Loss: 1.575, Acc: 47.495%\n",
      "Epoch 51 Training Loss: 1.578, Accuracy: 47.432%\n",
      "Epoch 51, Test Accuracy: 51.85%\n",
      "No improvement for 11 epoch(s)\n",
      "Epoch 52, Batch 0, Loss: 1.398, Acc: 50.000%\n",
      "Epoch 52, Batch 100, Loss: 1.541, Acc: 48.329%\n",
      "Epoch 52, Batch 200, Loss: 1.553, Acc: 48.057%\n",
      "Epoch 52, Batch 300, Loss: 1.560, Acc: 47.737%\n",
      "Epoch 52 Training Loss: 1.560, Accuracy: 47.722%\n",
      "Epoch 52, Test Accuracy: 51.01%\n",
      "No improvement for 12 epoch(s)\n",
      "Epoch 53, Batch 0, Loss: 1.389, Acc: 49.219%\n",
      "Epoch 53, Batch 100, Loss: 1.519, Acc: 48.476%\n",
      "Epoch 53, Batch 200, Loss: 1.536, Acc: 48.399%\n",
      "Epoch 53, Batch 300, Loss: 1.542, Acc: 48.269%\n",
      "Epoch 53 Training Loss: 1.542, Accuracy: 48.303%\n",
      "Epoch 53, Test Accuracy: 51.35%\n",
      "No improvement for 13 epoch(s)\n",
      "Epoch 54, Batch 0, Loss: 1.612, Acc: 45.312%\n",
      "Epoch 54, Batch 100, Loss: 1.529, Acc: 48.383%\n",
      "Epoch 54, Batch 200, Loss: 1.550, Acc: 48.181%\n",
      "Epoch 54, Batch 300, Loss: 1.545, Acc: 48.282%\n",
      "Epoch 54 Training Loss: 1.548, Accuracy: 48.233%\n",
      "Epoch 54, Test Accuracy: 52.73%\n",
      "No improvement for 14 epoch(s)\n",
      "Epoch 55, Batch 0, Loss: 60.397, Acc: 53.906%\n",
      "Epoch 55, Batch 100, Loss: 2.101, Acc: 47.888%\n",
      "Epoch 55, Batch 200, Loss: 1.832, Acc: 47.893%\n",
      "Epoch 55, Batch 300, Loss: 1.725, Acc: 48.367%\n",
      "Epoch 55 Training Loss: 1.717, Accuracy: 48.352%\n",
      "Epoch 55, Test Accuracy: 50.95%\n",
      "No improvement for 15 epoch(s)\n",
      "Epoch 56, Batch 0, Loss: 1.402, Acc: 49.219%\n",
      "Epoch 56, Batch 100, Loss: 1.511, Acc: 48.631%\n",
      "Epoch 56, Batch 200, Loss: 1.514, Acc: 48.857%\n",
      "Epoch 56, Batch 300, Loss: 1.520, Acc: 48.861%\n",
      "Epoch 56 Training Loss: 1.522, Accuracy: 48.843%\n",
      "Epoch 56, Test Accuracy: 49.84%\n",
      "No improvement for 16 epoch(s)\n",
      "Epoch 57, Batch 0, Loss: 58.413, Acc: 55.469%\n",
      "Epoch 57, Batch 100, Loss: 2.034, Acc: 50.456%\n",
      "Epoch 57, Batch 200, Loss: 1.777, Acc: 49.604%\n",
      "Epoch 57, Batch 300, Loss: 1.693, Acc: 49.349%\n",
      "Epoch 57 Training Loss: 1.688, Accuracy: 49.318%\n",
      "Epoch 57, Test Accuracy: 51.23%\n",
      "No improvement for 17 epoch(s)\n",
      "Epoch 58, Batch 0, Loss: 62.349, Acc: 52.344%\n",
      "Epoch 58, Batch 100, Loss: 2.134, Acc: 48.414%\n",
      "Epoch 58, Batch 200, Loss: 1.814, Acc: 48.815%\n",
      "Epoch 58, Batch 300, Loss: 1.718, Acc: 48.778%\n",
      "Epoch 58 Training Loss: 1.712, Accuracy: 48.767%\n",
      "Epoch 58, Test Accuracy: 54.12%\n",
      "No improvement for 18 epoch(s)\n",
      "Epoch 59, Batch 0, Loss: 1.611, Acc: 47.656%\n",
      "Epoch 59, Batch 100, Loss: 1.496, Acc: 49.412%\n",
      "Epoch 59, Batch 200, Loss: 1.485, Acc: 49.572%\n",
      "Epoch 59, Batch 300, Loss: 1.497, Acc: 49.413%\n",
      "Epoch 59 Training Loss: 1.498, Accuracy: 49.410%\n",
      "Epoch 59, Test Accuracy: 51.69%\n",
      "No improvement for 19 epoch(s)\n",
      "Epoch 60, Batch 0, Loss: 62.433, Acc: 52.344%\n",
      "Epoch 60, Batch 100, Loss: 2.069, Acc: 49.899%\n",
      "Epoch 60, Batch 200, Loss: 1.787, Acc: 49.526%\n",
      "Epoch 60, Batch 300, Loss: 1.686, Acc: 49.567%\n",
      "Epoch 60 Training Loss: 1.680, Accuracy: 49.542%\n",
      "Epoch 60, Test Accuracy: 53.65%\n",
      "No improvement for 20 epoch(s)\n",
      "Epoch 61, Batch 0, Loss: 1.556, Acc: 47.656%\n",
      "Epoch 61, Batch 100, Loss: 1.472, Acc: 50.302%\n",
      "Epoch 61, Batch 200, Loss: 1.484, Acc: 50.047%\n",
      "Epoch 61, Batch 300, Loss: 1.484, Acc: 49.803%\n",
      "Epoch 61 Training Loss: 1.486, Accuracy: 49.693%\n",
      "Epoch 61, Test Accuracy: 51.87%\n",
      "No improvement for 21 epoch(s)\n",
      "Epoch 62, Batch 0, Loss: 1.627, Acc: 45.312%\n",
      "Epoch 62, Batch 100, Loss: 1.458, Acc: 50.394%\n",
      "Epoch 62, Batch 200, Loss: 1.471, Acc: 50.082%\n",
      "Epoch 62, Batch 300, Loss: 1.477, Acc: 49.875%\n",
      "Epoch 62 Training Loss: 1.478, Accuracy: 49.833%\n",
      "Epoch 62, Test Accuracy: 52.44%\n",
      "No improvement for 22 epoch(s)\n",
      "Epoch 63, Batch 0, Loss: 1.324, Acc: 50.000%\n",
      "Epoch 63, Batch 100, Loss: 1.446, Acc: 50.774%\n",
      "Epoch 63, Batch 200, Loss: 1.463, Acc: 49.957%\n",
      "Epoch 63, Batch 300, Loss: 1.469, Acc: 49.748%\n",
      "Epoch 63 Training Loss: 1.468, Accuracy: 49.815%\n",
      "Epoch 63, Test Accuracy: 48.68%\n",
      "No improvement for 23 epoch(s)\n",
      "Epoch 64, Batch 0, Loss: 56.270, Acc: 57.031%\n",
      "Epoch 64, Batch 100, Loss: 1.978, Acc: 51.037%\n",
      "Epoch 64, Batch 200, Loss: 1.725, Acc: 50.280%\n",
      "Epoch 64, Batch 300, Loss: 1.643, Acc: 49.888%\n",
      "Epoch 64 Training Loss: 1.638, Accuracy: 49.843%\n",
      "Epoch 64, Test Accuracy: 50.03%\n",
      "No improvement for 24 epoch(s)\n",
      "Epoch 65, Batch 0, Loss: 57.310, Acc: 56.250%\n",
      "Epoch 65, Batch 100, Loss: 2.019, Acc: 50.433%\n",
      "Epoch 65, Batch 200, Loss: 1.743, Acc: 50.284%\n",
      "Epoch 65, Batch 300, Loss: 1.658, Acc: 49.831%\n",
      "Epoch 65 Training Loss: 1.649, Accuracy: 49.895%\n",
      "Epoch 65, Test Accuracy: 53.98%\n",
      "No improvement for 25 epoch(s)\n",
      "Epoch 66, Batch 0, Loss: 62.543, Acc: 52.344%\n",
      "Epoch 66, Batch 100, Loss: 2.036, Acc: 51.013%\n",
      "Epoch 66, Batch 200, Loss: 1.748, Acc: 50.575%\n",
      "Epoch 66, Batch 300, Loss: 1.657, Acc: 50.314%\n",
      "Epoch 66 Training Loss: 1.649, Accuracy: 50.285%\n",
      "Epoch 66, Test Accuracy: 51.30%\n",
      "No improvement for 26 epoch(s)\n",
      "Epoch 67, Batch 0, Loss: 62.351, Acc: 52.344%\n",
      "Epoch 67, Batch 100, Loss: 2.021, Acc: 50.982%\n",
      "Epoch 67, Batch 200, Loss: 1.733, Acc: 50.750%\n",
      "Epoch 67, Batch 300, Loss: 1.649, Acc: 50.436%\n",
      "Epoch 67 Training Loss: 1.642, Accuracy: 50.458%\n",
      "Epoch 67, Test Accuracy: 54.83%\n",
      "No improvement for 27 epoch(s)\n",
      "Epoch 68, Batch 0, Loss: 60.311, Acc: 53.906%\n",
      "Epoch 68, Batch 100, Loss: 1.982, Acc: 52.297%\n",
      "Epoch 68, Batch 200, Loss: 1.726, Acc: 51.679%\n",
      "Epoch 68, Batch 300, Loss: 1.637, Acc: 51.178%\n",
      "Epoch 68 Training Loss: 1.630, Accuracy: 51.233%\n",
      "Epoch 68, Test Accuracy: 52.65%\n",
      "No improvement for 28 epoch(s)\n",
      "Epoch 69, Batch 0, Loss: 1.473, Acc: 50.000%\n",
      "Epoch 69, Batch 100, Loss: 1.407, Acc: 51.655%\n",
      "Epoch 69, Batch 200, Loss: 1.423, Acc: 51.236%\n",
      "Epoch 69, Batch 300, Loss: 1.439, Acc: 50.981%\n",
      "Epoch 69 Training Loss: 1.441, Accuracy: 50.905%\n",
      "Epoch 69, Test Accuracy: 54.65%\n",
      "No improvement for 29 epoch(s)\n",
      "Epoch 70, Batch 0, Loss: 54.237, Acc: 58.594%\n",
      "Epoch 70, Batch 100, Loss: 1.917, Acc: 51.632%\n",
      "Epoch 70, Batch 200, Loss: 1.683, Acc: 51.096%\n",
      "Epoch 70, Batch 300, Loss: 1.610, Acc: 50.768%\n",
      "Epoch 70 Training Loss: 1.605, Accuracy: 50.680%\n",
      "Epoch 70, Test Accuracy: 55.72%\n",
      "No improvement for 30 epoch(s)\n",
      "Early stopping at epoch 70 due to no improvement for 30 epochs.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Early stopping parameters\n",
    "early_stopping_patience = 30  # Number of epochs with no improvement to stop training\n",
    "best_test_accuracy = 0.0  # Best test accuracy seen so far\n",
    "epochs_no_improvement = 0  # Counter for how many epochs with no improvement\n",
    "\n",
    "# Data Augmentation for Training Set\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),             # Random cropping with padding\n",
    "    transforms.RandomHorizontalFlip(),                # Random horizontal flip\n",
    "    transforms.RandomRotation(15), \n",
    "    transforms.RandomGrayscale(p=0.1),\n",
    "    transforms.RandomVerticalFlip(p=0.1),\n",
    "    transforms.GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 2.0)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Random jitter in brightness, contrast, etc.\n",
    "    transforms.ToTensor(),                            # Convert images to tensors\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize the images\n",
    "])\n",
    "\n",
    "# Data Transformations for the Test Set (no augmentation)\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),                            # Convert images to tensors\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize the images\n",
    "])\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "class CIFAR100Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, file_path, transform=None):\n",
    "        with open(file_path, 'rb') as f:\n",
    "            self.data = pickle.load(f)\n",
    "        self.transform = transform  # Add transform argument\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.data[idx]\n",
    "        \n",
    "        # Convert the tensor to a PIL image before applying transforms\n",
    "        if isinstance(image, torch.Tensor):\n",
    "            image = image.permute(1, 2, 0).numpy()  # Convert torch tensor to numpy array (H, W, C)\n",
    "            image = Image.fromarray((image * 255).astype('uint8'))  # Convert numpy array to PIL Image\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)  # Apply the transform\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "\n",
    "\n",
    "# Load train and test datasets with the new augmentations for the training set\n",
    "train_dataset = CIFAR100Dataset('train.pkl', transform=transform_train)\n",
    "test_dataset = CIFAR100Dataset('test.pkl', transform=transform_test)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "# Set paths and other parameters\n",
    "PENALTY_WEIGHT = 1  # Weight for penalizing incorrect predictions after 50% accuracy\n",
    "SAVE_PATH = './saved_models/'  # Directory to save model checkpoints\n",
    "if not os.path.exists(SAVE_PATH):\n",
    "    os.makedirs(SAVE_PATH)\n",
    "\n",
    "# Temperature Scaling class\n",
    "class TemperatureScaling(nn.Module):\n",
    "    def __init__(self, init_temp=1.0):\n",
    "        super(TemperatureScaling, self).__init__()\n",
    "        self.temperature = nn.Parameter(torch.ones(1) * init_temp)\n",
    "\n",
    "    def forward(self, logits):\n",
    "        return logits / self.temperature\n",
    "\n",
    "# Focal Loss for handling imbalanced datasets\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        BCE_loss = F.cross_entropy(outputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)  # Get the probability\n",
    "        focal_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "# Custom Loss Function with Focal Loss and Penalty for Wrong Predictions after 50% Accuracy\n",
    "def custom_loss_function(outputs, targets, current_accuracy):\n",
    "    # Apply softmax to get probabilities\n",
    "    probabilities = F.softmax(outputs, dim=1)\n",
    "    \n",
    "    # Get the max probability (confidence) and corresponding predicted class\n",
    "    confidences, predicted_classes = torch.max(probabilities, dim=1)\n",
    "    \n",
    "    # Calculate the Focal Loss for class imbalance\n",
    "    focal_loss = FocalLoss()(outputs, targets)\n",
    "    \n",
    "    # Heavily penalize wrong argmax predictions if training accuracy > 50%\n",
    "    wrong_predictions = (predicted_classes != targets).float()\n",
    "    if current_accuracy > 0.5:\n",
    "        wrong_prediction_penalty = PENALTY_WEIGHT * wrong_predictions.sum()\n",
    "    else:\n",
    "        wrong_prediction_penalty = 0\n",
    "\n",
    "    # Calculate the total loss\n",
    "    total_loss = focal_loss + wrong_prediction_penalty\n",
    "    return total_loss\n",
    "\n",
    "# WideResNeXt Block\n",
    "class WideResNeXtBlock(nn.Module):\n",
    "    expansion = 2  # Expansion factor for WideResNeXt\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1, cardinality=32, widen_factor=2):\n",
    "        super(WideResNeXtBlock, self).__init__()\n",
    "        D = cardinality * widen_factor\n",
    "        self.conv1 = nn.Conv2d(in_planes, D, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(D)\n",
    "        self.conv2 = nn.Conv2d(D, D, kernel_size=3, stride=stride, padding=1, groups=cardinality, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(D)\n",
    "        self.conv3 = nn.Conv2d(D, planes * WideResNeXtBlock.expansion, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * WideResNeXtBlock.expansion)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes * WideResNeXtBlock.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes * WideResNeXtBlock.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * WideResNeXtBlock.expansion)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.relu(self.bn1(self.conv1(x)))\n",
    "        out = torch.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = torch.relu(out)\n",
    "        return out\n",
    "\n",
    "# WideResNeXt Model with Temperature Scaling\n",
    "class WideResNeXt(nn.Module):\n",
    "    def __init__(self, block, num_blocks, cardinality=32, widen_factor=2, num_classes=100):\n",
    "        super(WideResNeXt, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1, cardinality=cardinality, widen_factor=widen_factor)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2, cardinality=cardinality, widen_factor=widen_factor)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2, cardinality=cardinality, widen_factor=widen_factor)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2, cardinality=cardinality, widen_factor=widen_factor)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.5)  # Add Dropout layer with 0.5 probability\n",
    "        self.linear = nn.Linear(512 * WideResNeXtBlock.expansion, num_classes)\n",
    "        self.temperature_scaling = TemperatureScaling()  # Temperature scaling layer\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride, cardinality, widen_factor):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride, cardinality, widen_factor))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = torch.nn.functional.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.dropout(out)  # Apply Dropout before the final linear layer\n",
    "        out = self.linear(out)\n",
    "        out = self.temperature_scaling(out)  # Apply temperature scaling before softmax\n",
    "        return out\n",
    "\n",
    "def train_with_penalty(epoch):\n",
    "    model.train()  # Set the model to training mode\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()  # Zero the gradients for the optimizer\n",
    "        \n",
    "        outputs = model(inputs)  # Forward pass\n",
    "        \n",
    "        # Calculate the overall training accuracy before updating weights\n",
    "        probabilities = F.softmax(outputs, dim=1)\n",
    "        _, predicted_classes = torch.max(probabilities, dim=1)\n",
    "        \n",
    "        correct_predictions = predicted_classes.eq(targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "        current_accuracy = correct_predictions / total\n",
    "        \n",
    "        # Calculate the custom loss with penalties if training accuracy > 50%\n",
    "        loss = custom_loss_function(outputs, targets, current_accuracy)\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update weights\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        correct += correct_predictions\n",
    "\n",
    "        if batch_idx % 100 == 0:  # Print every 100 batches\n",
    "            print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {train_loss / (batch_idx + 1):.3f}, Acc: {100.*correct/total:.3f}%')\n",
    "\n",
    "    # At the end of the epoch, print the final training accuracy\n",
    "    print(f'Epoch {epoch} Training Loss: {train_loss / len(trainloader):.3f}, Accuracy: {100.*correct/total:.3f}%')\n",
    "\n",
    "# Function to calculate accuracy based on True_label data in test_info.csv\n",
    "def test_accuracy(epoch, test_info_path):\n",
    "    global best_test_accuracy, epochs_no_improvement\n",
    "    model.eval()\n",
    "    correct_all = 0\n",
    "    total_all = 0\n",
    "\n",
    "    # Load True_label from test_info.csv\n",
    "    test_info = pd.read_csv(test_info_path)\n",
    "    true_labels = test_info['True_label'].values\n",
    "\n",
    "    predictions = []  # Store predictions\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            probabilities = F.softmax(outputs, dim=1)\n",
    "            _, predicted_classes = torch.max(probabilities, dim=1)\n",
    "\n",
    "            # Store predictions\n",
    "            predictions.extend(predicted_classes.cpu().numpy())\n",
    "\n",
    "    # Calculate accuracy based on True_label\n",
    "    correct_all = (predictions == true_labels).sum()\n",
    "    total_all = len(true_labels)\n",
    "\n",
    "    # Print accuracy\n",
    "    test_accuracy = 100. * correct_all / total_all\n",
    "    print(f\"Epoch {epoch}, Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "    # Check for improvement and early stopping condition\n",
    "    if test_accuracy > best_test_accuracy:\n",
    "        best_test_accuracy = test_accuracy\n",
    "        epochs_no_improvement = 0  # Reset the counter when improvement is seen\n",
    "        # Optionally save the model\n",
    "        save_model_checkpoint()\n",
    "    else:\n",
    "        epochs_no_improvement += 1  # No improvement\n",
    "        print(f'No improvement for {epochs_no_improvement} epoch(s)')\n",
    "    \n",
    "    # Stop if no improvement for 10 epochs\n",
    "    if epochs_no_improvement >= early_stopping_patience:\n",
    "        print(f\"Early stopping at epoch {epoch} due to no improvement for {early_stopping_patience} epochs.\")\n",
    "        return True  # Signal to stop training\n",
    "    return False\n",
    "\n",
    "def save_model_checkpoint():\n",
    "    save_path = os.path.join(SAVE_PATH, 'best_model.pth')\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"New best model saved with accuracy: {best_test_accuracy:.2f}%\")\n",
    "\n",
    "# Model, loss, optimizer, and scheduler\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = WideResNeXt(WideResNeXtBlock, [3, 4, 6, 3], cardinality=32, widen_factor=2).to(device)\n",
    "\n",
    "# Example optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# Define CyclicLR scheduler (to be used for the first 70 epochs)\n",
    "cyclic_scheduler = torch.optim.lr_scheduler.CyclicLR(\n",
    "    optimizer,\n",
    "    base_lr=0.001,\n",
    "    max_lr=0.1,\n",
    "    step_size_up=20\n",
    ")\n",
    "\n",
    "# Define StepLR scheduler (to be used after epoch 70)\n",
    "step_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer,\n",
    "    step_size=30,  # Decay every 30 epochs\n",
    "    gamma=0.1  # Reduce learning rate by a factor of 0.1\n",
    ")\n",
    "\n",
    "# Training loop with scheduler switching\n",
    "for epoch in range(0, 150):\n",
    "    train_with_penalty(epoch)  # Perform training for this epoch\n",
    "    stop_training = test_accuracy(epoch, 'test_info.csv')  # Evaluate test accuracy\n",
    "\n",
    "    # Use CyclicLR for the first 70 epochs\n",
    "    if epoch < 50:\n",
    "        cyclic_scheduler.step()\n",
    "    else:\n",
    "        step_scheduler.step()\n",
    "\n",
    "    if stop_training:\n",
    "        break  # Stop the training loop if early stopping is triggered\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5906773,
     "sourceId": 9666920,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3620.320516,
   "end_time": "2024-10-21T12:53:34.022731",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-21T11:53:13.702215",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
