{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2KlfAIpXguJ",
        "outputId": "61d60591-a8be-4da5-8b3b-5b7599bdc10a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.12.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.16.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2024.8.30)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "# Early stopping parameters\n",
        "early_stopping_patience = 150  # Number of epochs with no improvement to stop training\n",
        "best_test_accuracy = 0.0  # Best test accuracy seen so far\n",
        "epochs_no_improvement = 0  # Counter for how many epochs with no improvement\n",
        "\n",
        "# Data Augmentation for Training Set\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),             # Random cropping with padding\n",
        "    transforms.RandomHorizontalFlip(),                # Random horizontal flip\n",
        "    transforms.RandomRotation(15),                    # Randomly rotate by 15 degrees\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Random jitter in brightness, contrast, etc.\n",
        "    transforms.ToTensor(),                            # Convert images to tensors\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize the images\n",
        "])\n",
        "\n",
        "# Data Transformations for the Test Set (no augmentation)\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),                            # Convert images to tensors\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize the images\n",
        "])\n",
        "\n",
        "# Load the CIFAR100 dataset\n",
        "train_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
        "test_dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "# Dataloaders\n",
        "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "# Set paths and other parameters\n",
        "PENALTY_WEIGHT = 1 # Weight for penalizing incorrect predictions after 50% accuracy\n",
        "SAVE_PATH = './saved_models/'  # Directory to save model checkpoints\n",
        "if not os.path.exists(SAVE_PATH):\n",
        "    os.makedirs(SAVE_PATH)\n",
        "\n",
        "# Temperature Scaling class\n",
        "class TemperatureScaling(nn.Module):\n",
        "    def __init__(self, init_temp=1.0):\n",
        "        super(TemperatureScaling, self).__init__()\n",
        "        self.temperature = nn.Parameter(torch.ones(1) * init_temp)\n",
        "\n",
        "    def forward(self, logits):\n",
        "        return logits / self.temperature\n",
        "\n",
        "# Focal Loss for handling imbalanced datasets\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=1, gamma=2):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        BCE_loss = F.cross_entropy(outputs, targets, reduction='none')\n",
        "        pt = torch.exp(-BCE_loss)  # Get the probability\n",
        "        focal_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
        "        return focal_loss.mean()\n",
        "\n",
        "# Custom Loss Function with Focal Loss and Penalty for Wrong Predictions after 50% Accuracy\n",
        "def custom_loss_function(outputs, targets, current_accuracy):\n",
        "    probabilities = F.softmax(outputs, dim=1)\n",
        "    _, predicted_classes = torch.max(probabilities, dim=1)\n",
        "\n",
        "    focal_loss = FocalLoss()(outputs, targets)\n",
        "\n",
        "    wrong_predictions = (predicted_classes != targets).float()\n",
        "    if current_accuracy > 0.5:\n",
        "        wrong_prediction_penalty = PENALTY_WEIGHT * wrong_predictions.sum()\n",
        "    else:\n",
        "        wrong_prediction_penalty = 0\n",
        "\n",
        "    total_loss = focal_loss + wrong_prediction_penalty\n",
        "    return total_loss\n",
        "\n",
        "# WideResNeXt Block\n",
        "class WideResNeXtBlock(nn.Module):\n",
        "    expansion = 2  # Expansion factor for WideResNeXt\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, cardinality=32, widen_factor=2):\n",
        "        super(WideResNeXtBlock, self).__init__()\n",
        "        D = cardinality * widen_factor\n",
        "        self.conv1 = nn.Conv2d(in_planes, D, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(D)\n",
        "        self.conv2 = nn.Conv2d(D, D, kernel_size=3, stride=stride, padding=1, groups=cardinality, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(D)\n",
        "        self.conv3 = nn.Conv2d(D, planes * WideResNeXtBlock.expansion, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * WideResNeXtBlock.expansion)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes * WideResNeXtBlock.expansion:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, planes * WideResNeXtBlock.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * WideResNeXtBlock.expansion)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.relu(self.bn1(self.conv1(x)))\n",
        "        out = torch.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = torch.relu(out)\n",
        "        return out\n",
        "\n",
        "# WideResNeXt Model with Temperature Scaling\n",
        "class WideResNeXt(nn.Module):\n",
        "    def __init__(self, block, num_blocks, cardinality=32, widen_factor=2, num_classes=100):\n",
        "        super(WideResNeXt, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1, cardinality=cardinality, widen_factor=widen_factor)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2, cardinality=cardinality, widen_factor=widen_factor)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2, cardinality=cardinality, widen_factor=widen_factor)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2, cardinality=cardinality, widen_factor=widen_factor)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=0.5)  # Add Dropout layer with 0.5 probability\n",
        "        self.linear = nn.Linear(512 * WideResNeXtBlock.expansion, num_classes)\n",
        "        self.temperature_scaling = TemperatureScaling()  # Temperature scaling layer\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride, cardinality, widen_factor):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride, cardinality, widen_factor))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = torch.nn.functional.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.dropout(out)  # Apply Dropout before the final linear layer\n",
        "        out = self.linear(out)\n",
        "        out = self.temperature_scaling(out)  # Apply temperature scaling before softmax\n",
        "        return out\n",
        "\n",
        "def train_with_penalty(epoch):\n",
        "    model.train()  # Set the model to training mode\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()  # Zero the gradients for the optimizer\n",
        "\n",
        "        outputs = model(inputs)  # Forward pass\n",
        "\n",
        "        probabilities = F.softmax(outputs, dim=1)\n",
        "        _, predicted_classes = torch.max(probabilities, dim=1)\n",
        "\n",
        "        correct_predictions = predicted_classes.eq(targets).sum().item()\n",
        "        total += targets.size(0)\n",
        "        current_accuracy = correct_predictions / total\n",
        "\n",
        "        loss = custom_loss_function(outputs, targets, current_accuracy)\n",
        "        loss.backward()  # Backpropagation\n",
        "        optimizer.step()  # Update weights\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        correct += correct_predictions\n",
        "\n",
        "        if batch_idx % 100 == 0:  # Print every 100 batches\n",
        "            print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {train_loss / (batch_idx + 1):.3f}, Acc: {100.*correct/total:.3f}%')\n",
        "\n",
        "    print(f'Epoch {epoch} Training Loss: {train_loss / len(trainloader):.3f}, Accuracy: {100.*correct/total:.3f}%')\n",
        "\n",
        "# Save model\n",
        "def save_model(model, optimizer, filename):\n",
        "    torch.save({'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict()},\n",
        "                filename)\n",
        "    print(f\"Model saved to {filename}\")\n",
        "\n",
        "# Instantiate model, optimizer, and scheduler\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = WideResNeXt(WideResNeXtBlock, [3, 4, 6, 3], cardinality=32, widen_factor=2).to(device)\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
        "scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(5):\n",
        "    train_with_penalty(epoch)\n",
        "    scheduler.step()\n",
        "\n",
        "# Save the trained model after training\n",
        "save_model(model, optimizer, \"model.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oDTKof2XeAi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "# Load CIFAR-100 dataset\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "test_dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=2)\n",
        "\n",
        "# Modified WideResNeXt Model for Grad-CAM\n",
        "class WideResNeXtForGradCAM(WideResNeXt):\n",
        "    def __init__(self, block, num_blocks, cardinality=32, widen_factor=2, num_classes=100):\n",
        "        super(WideResNeXtForGradCAM, self).__init__(block, num_blocks, cardinality, widen_factor, num_classes)\n",
        "        self.gradients = None\n",
        "        self.activations = None\n",
        "\n",
        "    def save_gradient(self, grad):\n",
        "        self.gradients = grad\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "\n",
        "        # Hook for gradients and activations at the last conv layer\n",
        "        out = self.layer4(out)\n",
        "        self.activations = out  # Save activations\n",
        "        out.register_hook(self.save_gradient)  # Save gradients with hook\n",
        "\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.dropout(out)\n",
        "        out = self.linear(out)\n",
        "        out = self.temperature_scaling(out)\n",
        "        return out\n",
        "\n",
        "# Function to generate Grad-CAM\n",
        "def generate_gradcam(model, input_image, target_class):\n",
        "    model.eval()\n",
        "\n",
        "    # Forward pass\n",
        "    output = model(input_image)\n",
        "\n",
        "    # Zero the gradients and perform backpropagation on the target class\n",
        "    model.zero_grad()\n",
        "    one_hot_output = torch.zeros(output.shape, device=output.device)\n",
        "    one_hot_output[0][target_class] = 1\n",
        "    output.backward(gradient=one_hot_output)\n",
        "\n",
        "    # Get the gradients and activations from the last convolutional layer\n",
        "    gradients = model.gradients.data.cpu().numpy()[0]  # [C, H, W]\n",
        "    activations = model.activations.data.cpu().numpy()[0]  # Last conv layer activations\n",
        "\n",
        "    # Compute Grad-CAM\n",
        "    weights = np.mean(gradients, axis=(1, 2))  # Global average pooling of gradients\n",
        "    cam = np.zeros(activations.shape[1:], dtype=np.float32)\n",
        "    for i, w in enumerate(weights):\n",
        "        cam += w * activations[i]\n",
        "\n",
        "    cam = np.maximum(cam, 0)  # ReLU on the CAM\n",
        "    cam = cv2.resize(cam, (32, 32))  # Resize to match input image size\n",
        "    cam = cam - np.min(cam)\n",
        "    cam = cam / np.max(cam)  # Normalize CAM to [0, 1]\n",
        "    return cam\n",
        "\n",
        "# Helper function to visualize the CAM overlay on the original image\n",
        "def show_cam_and_image(img, cam, ax_original, ax_cam):\n",
        "    # Display the original image\n",
        "    ax_original.imshow(np.uint8(255 * img))  # Original image\n",
        "    ax_original.axis('off')\n",
        "    ax_original.set_title(\"Original Image\")\n",
        "\n",
        "    # Display the Grad-CAM heatmap overlaid on the original image\n",
        "    heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n",
        "    heatmap = np.float32(heatmap) / 255\n",
        "    cam_img = heatmap + np.float32(img)\n",
        "    cam_img = cam_img / np.max(cam_img)  # Normalize to [0, 1]\n",
        "    ax_cam.imshow(np.uint8(255 * cam_img))\n",
        "    ax_cam.axis('off')\n",
        "    ax_cam.set_title(\"Grad-CAM\")\n",
        "\n",
        "# Instantiate the modified model\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = WideResNeXtForGradCAM(WideResNeXtBlock, [3, 4, 6, 3], cardinality=32, widen_factor=2).to(device)\n",
        "\n",
        "# Load the pretrained model's weights\n",
        "model.load_state_dict(torch.load(\"model.pth\")['model_state_dict'])\n",
        "\n",
        "# Visualize Grad-CAM on 10 random images from the test set\n",
        "random_indices = random.sample(range(len(test_dataset)), 10)\n",
        "fig, axs = plt.subplots(10, 2, figsize=(10, 30))  # 10 rows, 2 columns for each image pair\n",
        "\n",
        "for i, idx in enumerate(random_indices):\n",
        "    img, label = test_dataset[idx]\n",
        "\n",
        "    # Add batch dimension and send image to device\n",
        "    input_img = img.unsqueeze(0).to(device)\n",
        "\n",
        "    # Generate Grad-CAM for the true label\n",
        "    cam = generate_gradcam(model, input_img, target_class=label)\n",
        "\n",
        "    # Convert image to numpy for visualization\n",
        "    img_np = img.permute(1, 2, 0).cpu().numpy()\n",
        "    img_np = (img_np * 0.5 + 0.5)  # Denormalize the image for visualization\n",
        "\n",
        "    # Plot original image and Grad-CAM side by side\n",
        "    show_cam_and_image(img_np, cam, axs[i, 0], axs[i, 1])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0evt1L9kWKJW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IR4JhqmMn8si",
        "outputId": "1aa40e87-b22c-42f3-e553-38d4b53714c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169001437/169001437 [00:05<00:00, 29999812.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Epoch 0, Batch 0, Loss: 4.832, Acc: 0.000%\n",
            "Epoch 0, Batch 100, Loss: 5.461, Acc: 1.926%\n",
            "Epoch 0, Batch 200, Loss: 5.018, Acc: 3.051%\n",
            "Epoch 0, Batch 300, Loss: 4.811, Acc: 4.135%\n",
            "Epoch 0 Training Loss: 4.797, Accuracy: 4.232%\n",
            "Evaluating at Epoch 0...\n",
            "Test Accuracy: 7.79%\n",
            "High Confidence Predictions: 0\n",
            "High Confidence Accuracy: 0.00%\n",
            "Wrong High Confidence Predictions: 0\n",
            "Epoch 1, Batch 0, Loss: 3.900, Acc: 5.469%\n",
            "Epoch 1, Batch 100, Loss: 3.968, Acc: 8.253%\n",
            "Epoch 1, Batch 200, Loss: 3.875, Acc: 9.103%\n",
            "Epoch 1, Batch 300, Loss: 3.796, Acc: 9.941%\n",
            "Epoch 1 Training Loss: 3.782, Accuracy: 10.085%\n",
            "Epoch 2, Batch 0, Loss: 3.455, Acc: 14.844%\n",
            "Epoch 2, Batch 100, Loss: 3.443, Acc: 13.482%\n",
            "Epoch 2, Batch 200, Loss: 3.389, Acc: 14.603%\n",
            "Epoch 2, Batch 300, Loss: 3.364, Acc: 15.259%\n",
            "Epoch 2 Training Loss: 3.362, Accuracy: 15.325%\n",
            "Epoch 3, Batch 0, Loss: 3.333, Acc: 12.500%\n",
            "Epoch 3, Batch 100, Loss: 3.156, Acc: 18.294%\n",
            "Epoch 3, Batch 200, Loss: 3.102, Acc: 19.022%\n",
            "Epoch 3, Batch 300, Loss: 3.058, Acc: 19.871%\n",
            "Epoch 3 Training Loss: 3.055, Accuracy: 20.005%\n",
            "Epoch 4, Batch 0, Loss: 2.695, Acc: 21.875%\n",
            "Epoch 4, Batch 100, Loss: 2.924, Acc: 22.772%\n",
            "Epoch 4, Batch 200, Loss: 2.845, Acc: 23.453%\n",
            "Epoch 4, Batch 300, Loss: 2.863, Acc: 23.902%\n",
            "Epoch 4 Training Loss: 2.854, Accuracy: 24.003%\n",
            "Epoch 5, Batch 0, Loss: 2.335, Acc: 26.562%\n",
            "Epoch 5, Batch 100, Loss: 2.676, Acc: 27.599%\n",
            "Epoch 5, Batch 200, Loss: 2.695, Acc: 27.942%\n",
            "Epoch 5, Batch 300, Loss: 2.704, Acc: 28.195%\n",
            "Epoch 5 Training Loss: 2.707, Accuracy: 28.215%\n",
            "Epoch 6, Batch 0, Loss: 2.308, Acc: 27.344%\n",
            "Epoch 6, Batch 100, Loss: 2.637, Acc: 30.747%\n",
            "Epoch 6, Batch 200, Loss: 2.585, Acc: 31.219%\n",
            "Epoch 6, Batch 300, Loss: 2.556, Acc: 31.657%\n",
            "Epoch 6 Training Loss: 2.567, Accuracy: 31.672%\n",
            "Epoch 7, Batch 0, Loss: 2.127, Acc: 35.938%\n",
            "Epoch 7, Batch 100, Loss: 2.568, Acc: 33.625%\n",
            "Epoch 7, Batch 200, Loss: 2.521, Acc: 33.862%\n",
            "Epoch 7, Batch 300, Loss: 2.540, Acc: 34.240%\n",
            "Epoch 7 Training Loss: 2.545, Accuracy: 34.280%\n",
            "Epoch 8, Batch 0, Loss: 2.065, Acc: 35.156%\n",
            "Epoch 8, Batch 100, Loss: 2.502, Acc: 36.781%\n",
            "Epoch 8, Batch 200, Loss: 2.486, Acc: 36.944%\n",
            "Epoch 8, Batch 300, Loss: 2.474, Acc: 37.163%\n",
            "Epoch 8 Training Loss: 2.471, Accuracy: 37.245%\n",
            "Epoch 9, Batch 0, Loss: 2.204, Acc: 32.031%\n",
            "Epoch 9, Batch 100, Loss: 2.438, Acc: 39.171%\n",
            "Epoch 9, Batch 200, Loss: 2.405, Acc: 39.296%\n",
            "Epoch 9, Batch 300, Loss: 2.346, Acc: 39.512%\n",
            "Epoch 9 Training Loss: 2.353, Accuracy: 39.550%\n",
            "Epoch 10, Batch 0, Loss: 1.885, Acc: 35.938%\n",
            "Epoch 10, Batch 100, Loss: 2.252, Acc: 41.282%\n",
            "Epoch 10, Batch 200, Loss: 2.289, Acc: 41.744%\n",
            "Epoch 10, Batch 300, Loss: 2.345, Acc: 41.920%\n",
            "Epoch 10 Training Loss: 2.355, Accuracy: 41.890%\n",
            "Evaluating at Epoch 10...\n",
            "Test Accuracy: 44.44%\n",
            "High Confidence Predictions: 1975\n",
            "High Confidence Accuracy: 91.85%\n",
            "Wrong High Confidence Predictions: 161\n",
            "Epoch 11, Batch 0, Loss: 1.403, Acc: 46.875%\n",
            "Epoch 11, Batch 100, Loss: 2.333, Acc: 44.160%\n",
            "Epoch 11, Batch 200, Loss: 2.332, Acc: 43.812%\n",
            "Epoch 11, Batch 300, Loss: 2.385, Acc: 43.708%\n",
            "Epoch 11 Training Loss: 2.388, Accuracy: 43.742%\n",
            "Epoch 12, Batch 0, Loss: 3.521, Acc: 43.750%\n",
            "Epoch 12, Batch 100, Loss: 2.346, Acc: 45.320%\n",
            "Epoch 12, Batch 200, Loss: 2.360, Acc: 45.211%\n",
            "Epoch 12, Batch 300, Loss: 2.334, Acc: 45.362%\n",
            "Epoch 12 Training Loss: 2.347, Accuracy: 45.425%\n",
            "Epoch 13, Batch 0, Loss: 2.490, Acc: 50.000%\n",
            "Epoch 13, Batch 100, Loss: 2.314, Acc: 46.364%\n",
            "Epoch 13, Batch 200, Loss: 2.326, Acc: 46.700%\n",
            "Epoch 13, Batch 300, Loss: 2.313, Acc: 46.569%\n",
            "Epoch 13 Training Loss: 2.306, Accuracy: 46.665%\n",
            "Epoch 14, Batch 0, Loss: 2.771, Acc: 44.531%\n",
            "Epoch 14, Batch 100, Loss: 2.179, Acc: 47.989%\n",
            "Epoch 14, Batch 200, Loss: 2.232, Acc: 48.220%\n",
            "Epoch 14, Batch 300, Loss: 2.274, Acc: 48.181%\n",
            "Epoch 14 Training Loss: 2.264, Accuracy: 48.060%\n",
            "Epoch 15, Batch 0, Loss: 3.419, Acc: 51.562%\n",
            "Epoch 15, Batch 100, Loss: 2.319, Acc: 50.015%\n",
            "Epoch 15, Batch 200, Loss: 2.306, Acc: 49.674%\n",
            "Epoch 15, Batch 300, Loss: 2.265, Acc: 49.769%\n",
            "Epoch 15 Training Loss: 2.252, Accuracy: 49.873%\n",
            "Epoch 16, Batch 0, Loss: 1.390, Acc: 50.781%\n",
            "Epoch 16, Batch 100, Loss: 2.142, Acc: 51.354%\n",
            "Epoch 16, Batch 200, Loss: 2.191, Acc: 51.104%\n",
            "Epoch 16, Batch 300, Loss: 2.203, Acc: 50.615%\n",
            "Epoch 16 Training Loss: 2.212, Accuracy: 50.635%\n",
            "Epoch 17, Batch 0, Loss: 3.359, Acc: 48.438%\n",
            "Epoch 17, Batch 100, Loss: 2.221, Acc: 52.421%\n",
            "Epoch 17, Batch 200, Loss: 2.279, Acc: 51.873%\n",
            "Epoch 17, Batch 300, Loss: 2.289, Acc: 52.045%\n",
            "Epoch 17 Training Loss: 2.283, Accuracy: 52.065%\n",
            "Epoch 18, Batch 0, Loss: 1.048, Acc: 57.812%\n",
            "Epoch 18, Batch 100, Loss: 2.176, Acc: 53.519%\n",
            "Epoch 18, Batch 200, Loss: 2.207, Acc: 52.872%\n",
            "Epoch 18, Batch 300, Loss: 2.208, Acc: 52.764%\n",
            "Epoch 18 Training Loss: 2.208, Accuracy: 52.667%\n",
            "Epoch 19, Batch 0, Loss: 2.036, Acc: 57.812%\n",
            "Epoch 19, Batch 100, Loss: 2.191, Acc: 53.976%\n",
            "Epoch 19, Batch 200, Loss: 2.239, Acc: 54.307%\n",
            "Epoch 19, Batch 300, Loss: 2.226, Acc: 53.953%\n",
            "Epoch 19 Training Loss: 2.211, Accuracy: 53.983%\n",
            "Epoch 20, Batch 0, Loss: 2.143, Acc: 58.594%\n",
            "Epoch 20, Batch 100, Loss: 2.294, Acc: 54.811%\n",
            "Epoch 20, Batch 200, Loss: 2.231, Acc: 54.528%\n",
            "Epoch 20, Batch 300, Loss: 2.192, Acc: 54.449%\n",
            "Epoch 20 Training Loss: 2.178, Accuracy: 54.530%\n",
            "Evaluating at Epoch 20...\n",
            "Test Accuracy: 55.19%\n",
            "High Confidence Predictions: 4043\n",
            "High Confidence Accuracy: 94.11%\n",
            "Wrong High Confidence Predictions: 238\n",
            "Epoch 21, Batch 0, Loss: 3.129, Acc: 59.375%\n",
            "Epoch 21, Batch 100, Loss: 2.166, Acc: 56.142%\n",
            "Epoch 21, Batch 200, Loss: 2.223, Acc: 55.869%\n",
            "Epoch 21, Batch 300, Loss: 2.257, Acc: 55.881%\n",
            "Epoch 21 Training Loss: 2.232, Accuracy: 55.867%\n",
            "Epoch 22, Batch 0, Loss: 1.003, Acc: 55.469%\n",
            "Epoch 22, Batch 100, Loss: 2.123, Acc: 56.706%\n",
            "Epoch 22, Batch 200, Loss: 2.265, Acc: 56.448%\n",
            "Epoch 22, Batch 300, Loss: 2.328, Acc: 56.240%\n",
            "Epoch 22 Training Loss: 2.323, Accuracy: 56.330%\n",
            "Epoch 23, Batch 0, Loss: 1.998, Acc: 61.719%\n",
            "Epoch 23, Batch 100, Loss: 2.214, Acc: 58.710%\n",
            "Epoch 23, Batch 200, Loss: 2.186, Acc: 57.809%\n",
            "Epoch 23, Batch 300, Loss: 2.251, Acc: 57.522%\n",
            "Epoch 23 Training Loss: 2.267, Accuracy: 57.570%\n",
            "Epoch 24, Batch 0, Loss: 4.034, Acc: 57.031%\n",
            "Epoch 24, Batch 100, Loss: 2.109, Acc: 58.485%\n",
            "Epoch 24, Batch 200, Loss: 2.141, Acc: 58.427%\n",
            "Epoch 24, Batch 300, Loss: 2.101, Acc: 58.178%\n",
            "Epoch 24 Training Loss: 2.085, Accuracy: 58.188%\n",
            "Epoch 25, Batch 0, Loss: 5.093, Acc: 57.031%\n",
            "Epoch 25, Batch 100, Loss: 2.359, Acc: 59.112%\n",
            "Epoch 25, Batch 200, Loss: 2.283, Acc: 58.784%\n",
            "Epoch 25, Batch 300, Loss: 2.239, Acc: 58.640%\n",
            "Epoch 25 Training Loss: 2.231, Accuracy: 58.657%\n",
            "Epoch 26, Batch 0, Loss: 1.816, Acc: 64.844%\n",
            "Epoch 26, Batch 100, Loss: 2.190, Acc: 60.373%\n",
            "Epoch 26, Batch 200, Loss: 2.247, Acc: 59.558%\n",
            "Epoch 26, Batch 300, Loss: 2.195, Acc: 59.458%\n",
            "Epoch 26 Training Loss: 2.200, Accuracy: 59.417%\n",
            "Epoch 27, Batch 0, Loss: 0.752, Acc: 65.625%\n",
            "Epoch 27, Batch 100, Loss: 2.174, Acc: 60.821%\n",
            "Epoch 27, Batch 200, Loss: 2.119, Acc: 60.685%\n",
            "Epoch 27, Batch 300, Loss: 2.156, Acc: 60.496%\n",
            "Epoch 27 Training Loss: 2.166, Accuracy: 60.530%\n",
            "Epoch 28, Batch 0, Loss: 0.848, Acc: 67.969%\n",
            "Epoch 28, Batch 100, Loss: 1.990, Acc: 61.502%\n",
            "Epoch 28, Batch 200, Loss: 2.075, Acc: 61.151%\n",
            "Epoch 28, Batch 300, Loss: 2.090, Acc: 61.026%\n",
            "Epoch 28 Training Loss: 2.116, Accuracy: 60.905%\n",
            "Epoch 29, Batch 0, Loss: 2.038, Acc: 64.062%\n",
            "Epoch 29, Batch 100, Loss: 2.115, Acc: 62.020%\n",
            "Epoch 29, Batch 200, Loss: 2.197, Acc: 61.804%\n",
            "Epoch 29, Batch 300, Loss: 2.112, Acc: 61.981%\n",
            "Epoch 29 Training Loss: 2.129, Accuracy: 61.895%\n",
            "Epoch 30, Batch 0, Loss: 1.768, Acc: 64.844%\n",
            "Epoch 30, Batch 100, Loss: 1.879, Acc: 65.145%\n",
            "Epoch 30, Batch 200, Loss: 1.893, Acc: 65.648%\n",
            "Epoch 30, Batch 300, Loss: 1.826, Acc: 66.074%\n",
            "Epoch 30 Training Loss: 1.825, Accuracy: 66.073%\n",
            "Evaluating at Epoch 30...\n",
            "Test Accuracy: 62.40%\n",
            "High Confidence Predictions: 6042\n",
            "High Confidence Accuracy: 94.49%\n",
            "Wrong High Confidence Predictions: 333\n",
            "Epoch 31, Batch 0, Loss: 1.636, Acc: 64.062%\n",
            "Epoch 31, Batch 100, Loss: 1.693, Acc: 67.273%\n",
            "Epoch 31, Batch 200, Loss: 1.812, Acc: 67.704%\n",
            "Epoch 31, Batch 300, Loss: 1.794, Acc: 67.868%\n",
            "Epoch 31 Training Loss: 1.799, Accuracy: 67.892%\n",
            "Epoch 32, Batch 0, Loss: 2.764, Acc: 70.312%\n",
            "Epoch 32, Batch 100, Loss: 2.046, Acc: 68.038%\n",
            "Epoch 32, Batch 200, Loss: 2.018, Acc: 68.249%\n",
            "Epoch 32, Batch 300, Loss: 1.976, Acc: 68.322%\n",
            "Epoch 32 Training Loss: 1.959, Accuracy: 68.315%\n",
            "Epoch 33, Batch 0, Loss: 2.609, Acc: 71.094%\n",
            "Epoch 33, Batch 100, Loss: 1.819, Acc: 69.524%\n",
            "Epoch 33, Batch 200, Loss: 1.829, Acc: 68.886%\n",
            "Epoch 33, Batch 300, Loss: 1.875, Acc: 68.903%\n",
            "Epoch 33 Training Loss: 1.885, Accuracy: 68.920%\n",
            "Epoch 34, Batch 0, Loss: 4.728, Acc: 69.531%\n",
            "Epoch 34, Batch 100, Loss: 1.856, Acc: 69.508%\n",
            "Epoch 34, Batch 200, Loss: 1.906, Acc: 69.042%\n",
            "Epoch 34, Batch 300, Loss: 1.871, Acc: 69.119%\n",
            "Epoch 34 Training Loss: 1.862, Accuracy: 69.153%\n",
            "Epoch 35, Batch 0, Loss: 0.504, Acc: 74.219%\n",
            "Epoch 35, Batch 100, Loss: 1.955, Acc: 69.817%\n",
            "Epoch 35, Batch 200, Loss: 1.825, Acc: 69.430%\n",
            "Epoch 35, Batch 300, Loss: 1.849, Acc: 69.407%\n",
            "Epoch 35 Training Loss: 1.847, Accuracy: 69.340%\n",
            "Epoch 36, Batch 0, Loss: 1.594, Acc: 69.531%\n",
            "Epoch 36, Batch 100, Loss: 1.900, Acc: 69.400%\n",
            "Epoch 36, Batch 200, Loss: 1.876, Acc: 69.485%\n",
            "Epoch 36, Batch 300, Loss: 1.850, Acc: 69.417%\n",
            "Epoch 36 Training Loss: 1.861, Accuracy: 69.390%\n",
            "Epoch 37, Batch 0, Loss: 1.473, Acc: 75.781%\n",
            "Epoch 37, Batch 100, Loss: 1.810, Acc: 70.235%\n",
            "Epoch 37, Batch 200, Loss: 1.920, Acc: 69.698%\n",
            "Epoch 37, Batch 300, Loss: 1.846, Acc: 69.788%\n",
            "Epoch 37 Training Loss: 1.834, Accuracy: 69.817%\n",
            "Epoch 38, Batch 0, Loss: 4.712, Acc: 67.969%\n",
            "Epoch 38, Batch 100, Loss: 1.769, Acc: 69.787%\n",
            "Epoch 38, Batch 200, Loss: 1.813, Acc: 70.068%\n",
            "Epoch 38, Batch 300, Loss: 1.821, Acc: 69.775%\n",
            "Epoch 38 Training Loss: 1.825, Accuracy: 69.778%\n",
            "Epoch 39, Batch 0, Loss: 1.713, Acc: 71.094%\n",
            "Epoch 39, Batch 100, Loss: 1.697, Acc: 70.405%\n",
            "Epoch 39, Batch 200, Loss: 1.744, Acc: 71.098%\n",
            "Epoch 39, Batch 300, Loss: 1.803, Acc: 70.842%\n",
            "Epoch 39 Training Loss: 1.802, Accuracy: 70.770%\n",
            "Epoch 40, Batch 0, Loss: 0.797, Acc: 64.844%\n",
            "Epoch 40, Batch 100, Loss: 1.858, Acc: 70.815%\n",
            "Epoch 40, Batch 200, Loss: 1.766, Acc: 70.954%\n",
            "Epoch 40, Batch 300, Loss: 1.801, Acc: 70.800%\n",
            "Epoch 40 Training Loss: 1.818, Accuracy: 70.800%\n",
            "Evaluating at Epoch 40...\n",
            "Test Accuracy: 63.42%\n",
            "High Confidence Predictions: 6826\n",
            "High Confidence Accuracy: 93.80%\n",
            "Wrong High Confidence Predictions: 423\n",
            "Epoch 41, Batch 0, Loss: 0.550, Acc: 76.562%\n",
            "Epoch 41, Batch 100, Loss: 1.671, Acc: 71.334%\n",
            "Epoch 41, Batch 200, Loss: 1.763, Acc: 71.175%\n",
            "Epoch 41, Batch 300, Loss: 1.727, Acc: 71.265%\n",
            "Epoch 41 Training Loss: 1.730, Accuracy: 71.235%\n",
            "Epoch 42, Batch 0, Loss: 3.601, Acc: 66.406%\n",
            "Epoch 42, Batch 100, Loss: 1.898, Acc: 71.419%\n",
            "Epoch 42, Batch 200, Loss: 1.846, Acc: 71.315%\n",
            "Epoch 42, Batch 300, Loss: 1.826, Acc: 71.299%\n",
            "Epoch 42 Training Loss: 1.827, Accuracy: 71.218%\n",
            "Epoch 43, Batch 0, Loss: 0.510, Acc: 75.000%\n",
            "Epoch 43, Batch 100, Loss: 1.759, Acc: 71.496%\n",
            "Epoch 43, Batch 200, Loss: 1.785, Acc: 71.389%\n",
            "Epoch 43, Batch 300, Loss: 1.764, Acc: 71.512%\n",
            "Epoch 43 Training Loss: 1.763, Accuracy: 71.420%\n",
            "Epoch 44, Batch 0, Loss: 0.714, Acc: 64.844%\n",
            "Epoch 44, Batch 100, Loss: 1.844, Acc: 70.978%\n",
            "Epoch 44, Batch 200, Loss: 1.880, Acc: 71.315%\n",
            "Epoch 44, Batch 300, Loss: 1.801, Acc: 71.553%\n",
            "Epoch 44 Training Loss: 1.824, Accuracy: 71.510%\n",
            "Epoch 45, Batch 0, Loss: 1.777, Acc: 66.406%\n",
            "Epoch 45, Batch 100, Loss: 1.543, Acc: 70.970%\n",
            "Epoch 45, Batch 200, Loss: 1.687, Acc: 71.304%\n",
            "Epoch 45, Batch 300, Loss: 1.688, Acc: 71.382%\n",
            "Epoch 45 Training Loss: 1.678, Accuracy: 71.425%\n",
            "Epoch 46, Batch 0, Loss: 2.596, Acc: 70.312%\n",
            "Epoch 46, Batch 100, Loss: 1.718, Acc: 71.805%\n",
            "Epoch 46, Batch 200, Loss: 1.719, Acc: 71.685%\n",
            "Epoch 46, Batch 300, Loss: 1.746, Acc: 71.787%\n",
            "Epoch 46 Training Loss: 1.740, Accuracy: 71.800%\n",
            "Epoch 47, Batch 0, Loss: 1.558, Acc: 76.562%\n",
            "Epoch 47, Batch 100, Loss: 1.750, Acc: 71.798%\n",
            "Epoch 47, Batch 200, Loss: 1.705, Acc: 71.828%\n",
            "Epoch 47, Batch 300, Loss: 1.709, Acc: 71.872%\n",
            "Epoch 47 Training Loss: 1.692, Accuracy: 71.853%\n",
            "Epoch 48, Batch 0, Loss: 0.698, Acc: 67.969%\n",
            "Epoch 48, Batch 100, Loss: 1.777, Acc: 71.976%\n",
            "Epoch 48, Batch 200, Loss: 1.792, Acc: 72.194%\n",
            "Epoch 48, Batch 300, Loss: 1.790, Acc: 72.298%\n",
            "Epoch 48 Training Loss: 1.761, Accuracy: 72.287%\n",
            "Epoch 49, Batch 0, Loss: 1.639, Acc: 73.438%\n",
            "Epoch 49, Batch 100, Loss: 1.835, Acc: 72.347%\n",
            "Epoch 49, Batch 200, Loss: 1.791, Acc: 72.746%\n",
            "Epoch 49, Batch 300, Loss: 1.726, Acc: 72.571%\n",
            "Epoch 49 Training Loss: 1.729, Accuracy: 72.532%\n",
            "Epoch 50, Batch 0, Loss: 1.531, Acc: 75.781%\n",
            "Epoch 50, Batch 100, Loss: 1.477, Acc: 73.066%\n",
            "Epoch 50, Batch 200, Loss: 1.643, Acc: 72.831%\n",
            "Epoch 50, Batch 300, Loss: 1.698, Acc: 72.674%\n",
            "Epoch 50 Training Loss: 1.718, Accuracy: 72.615%\n",
            "Evaluating at Epoch 50...\n",
            "Test Accuracy: 63.62%\n",
            "High Confidence Predictions: 7139\n",
            "High Confidence Accuracy: 93.32%\n",
            "Wrong High Confidence Predictions: 477\n",
            "Epoch 51, Batch 0, Loss: 1.580, Acc: 77.344%\n",
            "Epoch 51, Batch 100, Loss: 1.579, Acc: 73.878%\n",
            "Epoch 51, Batch 200, Loss: 1.717, Acc: 73.142%\n",
            "Epoch 51, Batch 300, Loss: 1.735, Acc: 72.843%\n",
            "Epoch 51 Training Loss: 1.713, Accuracy: 72.832%\n",
            "Epoch 52, Batch 0, Loss: 0.450, Acc: 72.656%\n",
            "Epoch 52, Batch 100, Loss: 1.888, Acc: 72.664%\n",
            "Epoch 52, Batch 200, Loss: 1.835, Acc: 72.544%\n",
            "Epoch 52, Batch 300, Loss: 1.874, Acc: 72.480%\n",
            "Epoch 52 Training Loss: 1.865, Accuracy: 72.537%\n",
            "Epoch 53, Batch 0, Loss: 1.743, Acc: 70.312%\n",
            "Epoch 53, Batch 100, Loss: 1.630, Acc: 72.904%\n",
            "Epoch 53, Batch 200, Loss: 1.723, Acc: 72.835%\n",
            "Epoch 53, Batch 300, Loss: 1.708, Acc: 72.892%\n",
            "Epoch 53 Training Loss: 1.703, Accuracy: 72.890%\n",
            "Epoch 54, Batch 0, Loss: 2.672, Acc: 68.750%\n",
            "Epoch 54, Batch 100, Loss: 1.955, Acc: 73.283%\n",
            "Epoch 54, Batch 200, Loss: 1.922, Acc: 73.138%\n",
            "Epoch 54, Batch 300, Loss: 1.839, Acc: 73.020%\n",
            "Epoch 54 Training Loss: 1.832, Accuracy: 72.968%\n",
            "Epoch 55, Batch 0, Loss: 1.568, Acc: 74.219%\n",
            "Epoch 55, Batch 100, Loss: 1.743, Acc: 73.407%\n",
            "Epoch 55, Batch 200, Loss: 1.899, Acc: 72.998%\n",
            "Epoch 55, Batch 300, Loss: 1.797, Acc: 73.160%\n",
            "Epoch 55 Training Loss: 1.790, Accuracy: 73.120%\n",
            "Epoch 56, Batch 0, Loss: 4.551, Acc: 75.781%\n",
            "Epoch 56, Batch 100, Loss: 1.787, Acc: 74.242%\n",
            "Epoch 56, Batch 200, Loss: 1.735, Acc: 73.951%\n",
            "Epoch 56, Batch 300, Loss: 1.739, Acc: 73.598%\n",
            "Epoch 56 Training Loss: 1.735, Accuracy: 73.578%\n",
            "Epoch 57, Batch 0, Loss: 2.627, Acc: 69.531%\n",
            "Epoch 57, Batch 100, Loss: 1.819, Acc: 74.002%\n",
            "Epoch 57, Batch 200, Loss: 1.845, Acc: 73.570%\n",
            "Epoch 57, Batch 300, Loss: 1.797, Acc: 73.508%\n",
            "Epoch 57 Training Loss: 1.818, Accuracy: 73.405%\n",
            "Epoch 58, Batch 0, Loss: 1.508, Acc: 74.219%\n",
            "Epoch 58, Batch 100, Loss: 1.620, Acc: 73.817%\n",
            "Epoch 58, Batch 200, Loss: 1.562, Acc: 73.504%\n",
            "Epoch 58, Batch 300, Loss: 1.641, Acc: 73.572%\n",
            "Epoch 58 Training Loss: 1.666, Accuracy: 73.562%\n",
            "Epoch 59, Batch 0, Loss: 0.577, Acc: 73.438%\n",
            "Epoch 59, Batch 100, Loss: 1.723, Acc: 73.700%\n",
            "Epoch 59, Batch 200, Loss: 1.714, Acc: 73.846%\n",
            "Epoch 59, Batch 300, Loss: 1.789, Acc: 73.850%\n",
            "Epoch 59 Training Loss: 1.780, Accuracy: 73.850%\n",
            "Epoch 60, Batch 0, Loss: 1.674, Acc: 75.781%\n",
            "Epoch 60, Batch 100, Loss: 1.600, Acc: 74.575%\n",
            "Epoch 60, Batch 200, Loss: 1.563, Acc: 74.475%\n",
            "Epoch 60, Batch 300, Loss: 1.624, Acc: 74.403%\n",
            "Epoch 60 Training Loss: 1.646, Accuracy: 74.377%\n",
            "Evaluating at Epoch 60...\n",
            "Test Accuracy: 64.33%\n",
            "High Confidence Predictions: 7513\n",
            "High Confidence Accuracy: 92.88%\n",
            "Wrong High Confidence Predictions: 535\n",
            "Epoch 61, Batch 0, Loss: 0.551, Acc: 75.781%\n",
            "Epoch 61, Batch 100, Loss: 1.478, Acc: 74.443%\n",
            "Epoch 61, Batch 200, Loss: 1.613, Acc: 74.639%\n",
            "Epoch 61, Batch 300, Loss: 1.591, Acc: 74.559%\n",
            "Epoch 61 Training Loss: 1.594, Accuracy: 74.570%\n",
            "Epoch 62, Batch 0, Loss: 0.582, Acc: 68.750%\n",
            "Epoch 62, Batch 100, Loss: 1.459, Acc: 74.838%\n",
            "Epoch 62, Batch 200, Loss: 1.558, Acc: 74.790%\n",
            "Epoch 62, Batch 300, Loss: 1.582, Acc: 74.883%\n",
            "Epoch 62 Training Loss: 1.589, Accuracy: 74.880%\n",
            "Epoch 63, Batch 0, Loss: 2.581, Acc: 71.875%\n",
            "Epoch 63, Batch 100, Loss: 1.656, Acc: 74.613%\n",
            "Epoch 63, Batch 200, Loss: 1.633, Acc: 74.689%\n",
            "Epoch 63, Batch 300, Loss: 1.598, Acc: 74.681%\n",
            "Epoch 63 Training Loss: 1.598, Accuracy: 74.685%\n",
            "Epoch 64, Batch 0, Loss: 1.379, Acc: 79.688%\n",
            "Epoch 64, Batch 100, Loss: 1.658, Acc: 74.729%\n",
            "Epoch 64, Batch 200, Loss: 1.711, Acc: 74.588%\n",
            "Epoch 64, Batch 300, Loss: 1.678, Acc: 74.621%\n",
            "Epoch 64 Training Loss: 1.680, Accuracy: 74.630%\n",
            "Epoch 65, Batch 0, Loss: 0.406, Acc: 85.156%\n",
            "Epoch 65, Batch 100, Loss: 1.525, Acc: 75.217%\n",
            "Epoch 65, Batch 200, Loss: 1.635, Acc: 74.911%\n",
            "Epoch 65, Batch 300, Loss: 1.720, Acc: 74.702%\n",
            "Epoch 65 Training Loss: 1.726, Accuracy: 74.665%\n",
            "Epoch 66, Batch 0, Loss: 1.485, Acc: 75.781%\n",
            "Epoch 66, Batch 100, Loss: 1.520, Acc: 75.248%\n",
            "Epoch 66, Batch 200, Loss: 1.650, Acc: 74.918%\n",
            "Epoch 66, Batch 300, Loss: 1.741, Acc: 74.834%\n",
            "Epoch 66 Training Loss: 1.732, Accuracy: 74.850%\n",
            "Epoch 67, Batch 0, Loss: 0.362, Acc: 75.000%\n",
            "Epoch 67, Batch 100, Loss: 1.650, Acc: 74.404%\n",
            "Epoch 67, Batch 200, Loss: 1.622, Acc: 74.254%\n",
            "Epoch 67, Batch 300, Loss: 1.696, Acc: 74.346%\n",
            "Epoch 67 Training Loss: 1.711, Accuracy: 74.400%\n",
            "Epoch 68, Batch 0, Loss: 0.404, Acc: 78.125%\n",
            "Epoch 68, Batch 100, Loss: 1.783, Acc: 74.915%\n",
            "Epoch 68, Batch 200, Loss: 1.707, Acc: 75.155%\n",
            "Epoch 68, Batch 300, Loss: 1.699, Acc: 74.953%\n",
            "Epoch 68 Training Loss: 1.690, Accuracy: 74.960%\n",
            "Epoch 69, Batch 0, Loss: 2.448, Acc: 77.344%\n",
            "Epoch 69, Batch 100, Loss: 1.724, Acc: 74.381%\n",
            "Epoch 69, Batch 200, Loss: 1.748, Acc: 74.456%\n",
            "Epoch 69, Batch 300, Loss: 1.773, Acc: 74.504%\n",
            "Epoch 69 Training Loss: 1.767, Accuracy: 74.472%\n",
            "Epoch 70, Batch 0, Loss: 1.428, Acc: 76.562%\n",
            "Epoch 70, Batch 100, Loss: 1.722, Acc: 74.652%\n",
            "Epoch 70, Batch 200, Loss: 1.770, Acc: 74.405%\n",
            "Epoch 70, Batch 300, Loss: 1.711, Acc: 74.683%\n",
            "Epoch 70 Training Loss: 1.725, Accuracy: 74.775%\n",
            "Evaluating at Epoch 70...\n",
            "Test Accuracy: 64.52%\n",
            "High Confidence Predictions: 7531\n",
            "High Confidence Accuracy: 93.12%\n",
            "Wrong High Confidence Predictions: 518\n",
            "Epoch 71, Batch 0, Loss: 0.621, Acc: 75.781%\n",
            "Epoch 71, Batch 100, Loss: 1.648, Acc: 74.737%\n",
            "Epoch 71, Batch 200, Loss: 1.727, Acc: 74.693%\n",
            "Epoch 71, Batch 300, Loss: 1.739, Acc: 74.795%\n",
            "Epoch 71 Training Loss: 1.735, Accuracy: 74.750%\n",
            "Epoch 72, Batch 0, Loss: 2.611, Acc: 71.094%\n",
            "Epoch 72, Batch 100, Loss: 1.740, Acc: 75.193%\n",
            "Epoch 72, Batch 200, Loss: 1.752, Acc: 74.946%\n",
            "Epoch 72, Batch 300, Loss: 1.764, Acc: 74.727%\n",
            "Epoch 72 Training Loss: 1.751, Accuracy: 74.728%\n",
            "Epoch 73, Batch 0, Loss: 3.564, Acc: 72.656%\n",
            "Epoch 73, Batch 100, Loss: 1.563, Acc: 75.085%\n",
            "Epoch 73, Batch 200, Loss: 1.583, Acc: 74.681%\n",
            "Epoch 73, Batch 300, Loss: 1.562, Acc: 74.987%\n",
            "Epoch 73 Training Loss: 1.557, Accuracy: 75.017%\n",
            "Epoch 74, Batch 0, Loss: 2.427, Acc: 79.688%\n",
            "Epoch 74, Batch 100, Loss: 1.643, Acc: 74.706%\n",
            "Epoch 74, Batch 200, Loss: 1.676, Acc: 74.977%\n",
            "Epoch 74, Batch 300, Loss: 1.718, Acc: 75.112%\n",
            "Epoch 74 Training Loss: 1.714, Accuracy: 75.110%\n",
            "Epoch 75, Batch 0, Loss: 0.587, Acc: 69.531%\n",
            "Epoch 75, Batch 100, Loss: 1.667, Acc: 75.139%\n",
            "Epoch 75, Batch 200, Loss: 1.637, Acc: 75.202%\n",
            "Epoch 75, Batch 300, Loss: 1.658, Acc: 75.200%\n",
            "Epoch 75 Training Loss: 1.667, Accuracy: 75.153%\n",
            "Epoch 76, Batch 0, Loss: 3.401, Acc: 82.812%\n",
            "Epoch 76, Batch 100, Loss: 1.631, Acc: 74.737%\n",
            "Epoch 76, Batch 200, Loss: 1.652, Acc: 75.148%\n",
            "Epoch 76, Batch 300, Loss: 1.663, Acc: 75.018%\n",
            "Epoch 76 Training Loss: 1.668, Accuracy: 74.983%\n",
            "Epoch 77, Batch 0, Loss: 2.640, Acc: 75.781%\n",
            "Epoch 77, Batch 100, Loss: 1.803, Acc: 75.418%\n",
            "Epoch 77, Batch 200, Loss: 1.642, Acc: 75.311%\n",
            "Epoch 77, Batch 300, Loss: 1.644, Acc: 75.205%\n",
            "Epoch 77 Training Loss: 1.652, Accuracy: 75.220%\n",
            "Epoch 78, Batch 0, Loss: 1.415, Acc: 75.000%\n",
            "Epoch 78, Batch 100, Loss: 1.814, Acc: 75.077%\n",
            "Epoch 78, Batch 200, Loss: 1.682, Acc: 75.451%\n",
            "Epoch 78, Batch 300, Loss: 1.740, Acc: 75.311%\n",
            "Epoch 78 Training Loss: 1.728, Accuracy: 75.275%\n",
            "Epoch 79, Batch 0, Loss: 1.386, Acc: 79.688%\n",
            "Epoch 79, Batch 100, Loss: 1.516, Acc: 75.039%\n",
            "Epoch 79, Batch 200, Loss: 1.535, Acc: 75.016%\n",
            "Epoch 79, Batch 300, Loss: 1.622, Acc: 74.849%\n",
            "Epoch 79 Training Loss: 1.621, Accuracy: 74.870%\n",
            "Epoch 80, Batch 0, Loss: 1.509, Acc: 69.531%\n",
            "Epoch 80, Batch 100, Loss: 1.748, Acc: 75.750%\n",
            "Epoch 80, Batch 200, Loss: 1.743, Acc: 75.253%\n",
            "Epoch 80, Batch 300, Loss: 1.634, Acc: 75.052%\n",
            "Epoch 80 Training Loss: 1.625, Accuracy: 75.062%\n",
            "Evaluating at Epoch 80...\n",
            "Test Accuracy: 64.44%\n",
            "High Confidence Predictions: 7546\n",
            "High Confidence Accuracy: 93.10%\n",
            "Wrong High Confidence Predictions: 521\n",
            "Epoch 81, Batch 0, Loss: 0.462, Acc: 76.562%\n",
            "Epoch 81, Batch 100, Loss: 1.558, Acc: 75.224%\n",
            "Epoch 81, Batch 200, Loss: 1.648, Acc: 75.330%\n",
            "Epoch 81, Batch 300, Loss: 1.673, Acc: 75.187%\n",
            "Epoch 81 Training Loss: 1.667, Accuracy: 75.155%\n",
            "Epoch 82, Batch 0, Loss: 1.376, Acc: 78.906%\n",
            "Epoch 82, Batch 100, Loss: 1.883, Acc: 75.394%\n",
            "Epoch 82, Batch 200, Loss: 1.765, Acc: 75.264%\n",
            "Epoch 82, Batch 300, Loss: 1.759, Acc: 75.413%\n",
            "Epoch 82 Training Loss: 1.759, Accuracy: 75.345%\n",
            "Epoch 83, Batch 0, Loss: 0.359, Acc: 79.688%\n",
            "Epoch 83, Batch 100, Loss: 1.536, Acc: 75.015%\n",
            "Epoch 83, Batch 200, Loss: 1.608, Acc: 74.926%\n",
            "Epoch 83, Batch 300, Loss: 1.601, Acc: 75.247%\n",
            "Epoch 83 Training Loss: 1.590, Accuracy: 75.170%\n",
            "Epoch 84, Batch 0, Loss: 0.511, Acc: 74.219%\n",
            "Epoch 84, Batch 100, Loss: 1.767, Acc: 75.294%\n",
            "Epoch 84, Batch 200, Loss: 1.786, Acc: 75.587%\n",
            "Epoch 84, Batch 300, Loss: 1.762, Acc: 75.319%\n",
            "Epoch 84 Training Loss: 1.751, Accuracy: 75.340%\n",
            "Epoch 85, Batch 0, Loss: 2.445, Acc: 74.219%\n",
            "Epoch 85, Batch 100, Loss: 1.620, Acc: 75.936%\n",
            "Epoch 85, Batch 200, Loss: 1.636, Acc: 75.350%\n",
            "Epoch 85, Batch 300, Loss: 1.646, Acc: 75.075%\n",
            "Epoch 85 Training Loss: 1.647, Accuracy: 75.035%\n",
            "Epoch 86, Batch 0, Loss: 2.625, Acc: 71.094%\n",
            "Epoch 86, Batch 100, Loss: 1.799, Acc: 75.085%\n",
            "Epoch 86, Batch 200, Loss: 1.694, Acc: 75.264%\n",
            "Epoch 86, Batch 300, Loss: 1.625, Acc: 75.228%\n",
            "Epoch 86 Training Loss: 1.629, Accuracy: 75.213%\n",
            "Epoch 87, Batch 0, Loss: 3.442, Acc: 78.125%\n",
            "Epoch 87, Batch 100, Loss: 1.744, Acc: 75.093%\n",
            "Epoch 87, Batch 200, Loss: 1.688, Acc: 75.396%\n",
            "Epoch 87, Batch 300, Loss: 1.678, Acc: 75.327%\n",
            "Epoch 87 Training Loss: 1.658, Accuracy: 75.332%\n",
            "Epoch 88, Batch 0, Loss: 1.522, Acc: 70.312%\n",
            "Epoch 88, Batch 100, Loss: 1.511, Acc: 75.959%\n",
            "Epoch 88, Batch 200, Loss: 1.637, Acc: 75.369%\n",
            "Epoch 88, Batch 300, Loss: 1.587, Acc: 75.428%\n",
            "Epoch 88 Training Loss: 1.594, Accuracy: 75.395%\n",
            "Epoch 89, Batch 0, Loss: 2.368, Acc: 81.250%\n",
            "Epoch 89, Batch 100, Loss: 1.571, Acc: 75.611%\n",
            "Epoch 89, Batch 200, Loss: 1.538, Acc: 75.482%\n",
            "Epoch 89, Batch 300, Loss: 1.581, Acc: 75.239%\n",
            "Epoch 89 Training Loss: 1.587, Accuracy: 75.272%\n",
            "Epoch 90, Batch 0, Loss: 1.577, Acc: 69.531%\n",
            "Epoch 90, Batch 100, Loss: 1.680, Acc: 75.116%\n",
            "Epoch 90, Batch 200, Loss: 1.632, Acc: 75.404%\n",
            "Epoch 90, Batch 300, Loss: 1.663, Acc: 75.441%\n",
            "Epoch 90 Training Loss: 1.663, Accuracy: 75.460%\n",
            "Evaluating at Epoch 90...\n",
            "Test Accuracy: 64.67%\n",
            "High Confidence Predictions: 7665\n",
            "High Confidence Accuracy: 92.88%\n",
            "Wrong High Confidence Predictions: 546\n",
            "Epoch 91, Batch 0, Loss: 2.658, Acc: 71.094%\n",
            "Epoch 91, Batch 100, Loss: 1.726, Acc: 74.892%\n",
            "Epoch 91, Batch 200, Loss: 1.694, Acc: 75.486%\n",
            "Epoch 91, Batch 300, Loss: 1.673, Acc: 75.337%\n",
            "Epoch 91 Training Loss: 1.686, Accuracy: 75.360%\n",
            "Epoch 92, Batch 0, Loss: 2.497, Acc: 75.000%\n",
            "Epoch 92, Batch 100, Loss: 1.659, Acc: 75.843%\n",
            "Epoch 92, Batch 200, Loss: 1.649, Acc: 75.377%\n",
            "Epoch 92, Batch 300, Loss: 1.688, Acc: 75.392%\n",
            "Epoch 92 Training Loss: 1.667, Accuracy: 75.412%\n",
            "Epoch 93, Batch 0, Loss: 2.497, Acc: 75.781%\n",
            "Epoch 93, Batch 100, Loss: 1.603, Acc: 75.534%\n",
            "Epoch 93, Batch 200, Loss: 1.642, Acc: 75.346%\n",
            "Epoch 93, Batch 300, Loss: 1.676, Acc: 75.415%\n",
            "Epoch 93 Training Loss: 1.667, Accuracy: 75.382%\n",
            "Epoch 94, Batch 0, Loss: 1.643, Acc: 72.656%\n",
            "Epoch 94, Batch 100, Loss: 1.700, Acc: 75.534%\n",
            "Epoch 94, Batch 200, Loss: 1.688, Acc: 75.797%\n",
            "Epoch 94, Batch 300, Loss: 1.668, Acc: 75.607%\n",
            "Epoch 94 Training Loss: 1.670, Accuracy: 75.525%\n",
            "Epoch 95, Batch 0, Loss: 2.646, Acc: 67.188%\n",
            "Epoch 95, Batch 100, Loss: 1.601, Acc: 75.394%\n",
            "Epoch 95, Batch 200, Loss: 1.558, Acc: 75.361%\n",
            "Epoch 95, Batch 300, Loss: 1.544, Acc: 75.197%\n",
            "Epoch 95 Training Loss: 1.539, Accuracy: 75.243%\n",
            "Epoch 96, Batch 0, Loss: 0.390, Acc: 78.125%\n",
            "Epoch 96, Batch 100, Loss: 1.728, Acc: 76.037%\n",
            "Epoch 96, Batch 200, Loss: 1.701, Acc: 75.393%\n",
            "Epoch 96, Batch 300, Loss: 1.669, Acc: 75.478%\n",
            "Epoch 96 Training Loss: 1.659, Accuracy: 75.513%\n",
            "Epoch 97, Batch 0, Loss: 0.447, Acc: 78.906%\n",
            "Epoch 97, Batch 100, Loss: 1.547, Acc: 76.052%\n",
            "Epoch 97, Batch 200, Loss: 1.669, Acc: 75.700%\n",
            "Epoch 97, Batch 300, Loss: 1.690, Acc: 75.672%\n",
            "Epoch 97 Training Loss: 1.707, Accuracy: 75.620%\n",
            "Epoch 98, Batch 0, Loss: 2.552, Acc: 71.094%\n",
            "Epoch 98, Batch 100, Loss: 1.601, Acc: 74.807%\n",
            "Epoch 98, Batch 200, Loss: 1.667, Acc: 75.292%\n",
            "Epoch 98, Batch 300, Loss: 1.585, Acc: 75.304%\n",
            "Epoch 98 Training Loss: 1.593, Accuracy: 75.293%\n",
            "Epoch 99, Batch 0, Loss: 0.380, Acc: 74.219%\n",
            "Epoch 99, Batch 100, Loss: 1.724, Acc: 74.814%\n",
            "Epoch 99, Batch 200, Loss: 1.748, Acc: 75.214%\n",
            "Epoch 99, Batch 300, Loss: 1.749, Acc: 75.296%\n",
            "Epoch 99 Training Loss: 1.729, Accuracy: 75.310%\n",
            "Epoch 100, Batch 0, Loss: 1.612, Acc: 73.438%\n",
            "Epoch 100, Batch 100, Loss: 1.645, Acc: 75.874%\n",
            "Epoch 100, Batch 200, Loss: 1.724, Acc: 75.435%\n",
            "Epoch 100, Batch 300, Loss: 1.740, Acc: 75.475%\n",
            "Epoch 100 Training Loss: 1.714, Accuracy: 75.483%\n",
            "Evaluating at Epoch 100...\n",
            "Test Accuracy: 64.64%\n",
            "High Confidence Predictions: 7685\n",
            "High Confidence Accuracy: 92.92%\n",
            "Wrong High Confidence Predictions: 544\n",
            "Epoch 101, Batch 0, Loss: 1.548, Acc: 73.438%\n",
            "Epoch 101, Batch 100, Loss: 1.557, Acc: 75.541%\n",
            "Epoch 101, Batch 200, Loss: 1.610, Acc: 75.517%\n",
            "Epoch 101, Batch 300, Loss: 1.646, Acc: 75.631%\n",
            "Epoch 101 Training Loss: 1.637, Accuracy: 75.585%\n",
            "Epoch 102, Batch 0, Loss: 3.557, Acc: 73.438%\n",
            "Epoch 102, Batch 100, Loss: 1.796, Acc: 75.294%\n",
            "Epoch 102, Batch 200, Loss: 1.630, Acc: 75.377%\n",
            "Epoch 102, Batch 300, Loss: 1.673, Acc: 75.301%\n",
            "Epoch 102 Training Loss: 1.644, Accuracy: 75.295%\n",
            "Epoch 103, Batch 0, Loss: 0.409, Acc: 81.250%\n",
            "Epoch 103, Batch 100, Loss: 1.405, Acc: 75.371%\n",
            "Epoch 103, Batch 200, Loss: 1.537, Acc: 75.233%\n",
            "Epoch 103, Batch 300, Loss: 1.539, Acc: 75.389%\n",
            "Epoch 103 Training Loss: 1.541, Accuracy: 75.425%\n",
            "Epoch 104, Batch 0, Loss: 1.361, Acc: 78.125%\n",
            "Epoch 104, Batch 100, Loss: 1.745, Acc: 75.186%\n",
            "Epoch 104, Batch 200, Loss: 1.749, Acc: 75.377%\n",
            "Epoch 104, Batch 300, Loss: 1.787, Acc: 75.519%\n",
            "Epoch 104 Training Loss: 1.776, Accuracy: 75.485%\n",
            "Epoch 105, Batch 0, Loss: 0.474, Acc: 75.000%\n",
            "Epoch 105, Batch 100, Loss: 1.728, Acc: 75.085%\n",
            "Epoch 105, Batch 200, Loss: 1.656, Acc: 75.567%\n",
            "Epoch 105, Batch 300, Loss: 1.682, Acc: 75.522%\n",
            "Epoch 105 Training Loss: 1.674, Accuracy: 75.522%\n",
            "Epoch 106, Batch 0, Loss: 2.394, Acc: 80.469%\n",
            "Epoch 106, Batch 100, Loss: 1.633, Acc: 75.333%\n",
            "Epoch 106, Batch 200, Loss: 1.639, Acc: 75.082%\n",
            "Epoch 106, Batch 300, Loss: 1.612, Acc: 75.184%\n",
            "Epoch 106 Training Loss: 1.591, Accuracy: 75.192%\n",
            "Epoch 107, Batch 0, Loss: 0.373, Acc: 81.250%\n",
            "Epoch 107, Batch 100, Loss: 1.682, Acc: 75.217%\n",
            "Epoch 107, Batch 200, Loss: 1.708, Acc: 75.295%\n",
            "Epoch 107, Batch 300, Loss: 1.693, Acc: 75.239%\n",
            "Epoch 107 Training Loss: 1.715, Accuracy: 75.225%\n",
            "Epoch 108, Batch 0, Loss: 1.456, Acc: 78.125%\n",
            "Epoch 108, Batch 100, Loss: 1.537, Acc: 75.541%\n",
            "Epoch 108, Batch 200, Loss: 1.581, Acc: 75.334%\n",
            "Epoch 108, Batch 300, Loss: 1.553, Acc: 75.475%\n",
            "Epoch 108 Training Loss: 1.551, Accuracy: 75.457%\n",
            "Epoch 109, Batch 0, Loss: 0.536, Acc: 74.219%\n",
            "Epoch 109, Batch 100, Loss: 1.609, Acc: 75.186%\n",
            "Epoch 109, Batch 200, Loss: 1.775, Acc: 75.323%\n",
            "Epoch 109, Batch 300, Loss: 1.815, Acc: 75.135%\n",
            "Epoch 109 Training Loss: 1.796, Accuracy: 75.228%\n",
            "Epoch 110, Batch 0, Loss: 0.538, Acc: 69.531%\n",
            "Epoch 110, Batch 100, Loss: 1.711, Acc: 75.193%\n",
            "Epoch 110, Batch 200, Loss: 1.622, Acc: 75.381%\n",
            "Epoch 110, Batch 300, Loss: 1.625, Acc: 75.478%\n",
            "Epoch 110 Training Loss: 1.634, Accuracy: 75.455%\n",
            "Evaluating at Epoch 110...\n",
            "Test Accuracy: 64.30%\n",
            "High Confidence Predictions: 7795\n",
            "High Confidence Accuracy: 92.39%\n",
            "Wrong High Confidence Predictions: 593\n",
            "Epoch 111, Batch 0, Loss: 0.382, Acc: 78.125%\n",
            "Epoch 111, Batch 100, Loss: 1.633, Acc: 75.920%\n",
            "Epoch 111, Batch 200, Loss: 1.542, Acc: 75.801%\n",
            "Epoch 111, Batch 300, Loss: 1.569, Acc: 75.607%\n",
            "Epoch 111 Training Loss: 1.567, Accuracy: 75.528%\n",
            "Epoch 112, Batch 0, Loss: 2.633, Acc: 68.750%\n",
            "Epoch 112, Batch 100, Loss: 1.632, Acc: 75.704%\n",
            "Epoch 112, Batch 200, Loss: 1.692, Acc: 75.599%\n",
            "Epoch 112, Batch 300, Loss: 1.672, Acc: 75.540%\n",
            "Epoch 112 Training Loss: 1.674, Accuracy: 75.593%\n",
            "Epoch 113, Batch 0, Loss: 0.511, Acc: 82.031%\n",
            "Epoch 113, Batch 100, Loss: 1.602, Acc: 75.155%\n",
            "Epoch 113, Batch 200, Loss: 1.604, Acc: 75.381%\n",
            "Epoch 113, Batch 300, Loss: 1.619, Acc: 75.350%\n",
            "Epoch 113 Training Loss: 1.622, Accuracy: 75.305%\n",
            "Epoch 114, Batch 0, Loss: 1.525, Acc: 76.562%\n",
            "Epoch 114, Batch 100, Loss: 1.635, Acc: 75.804%\n",
            "Epoch 114, Batch 200, Loss: 1.590, Acc: 75.773%\n",
            "Epoch 114, Batch 300, Loss: 1.634, Acc: 75.576%\n",
            "Epoch 114 Training Loss: 1.657, Accuracy: 75.623%\n",
            "Epoch 115, Batch 0, Loss: 1.642, Acc: 70.312%\n",
            "Epoch 115, Batch 100, Loss: 1.432, Acc: 75.835%\n",
            "Epoch 115, Batch 200, Loss: 1.512, Acc: 75.719%\n",
            "Epoch 115, Batch 300, Loss: 1.613, Acc: 75.626%\n",
            "Epoch 115 Training Loss: 1.609, Accuracy: 75.647%\n",
            "Epoch 116, Batch 0, Loss: 0.426, Acc: 73.438%\n",
            "Epoch 116, Batch 100, Loss: 1.521, Acc: 75.797%\n",
            "Epoch 116, Batch 200, Loss: 1.519, Acc: 75.715%\n",
            "Epoch 116, Batch 300, Loss: 1.565, Acc: 75.600%\n",
            "Epoch 116 Training Loss: 1.579, Accuracy: 75.522%\n",
            "Epoch 117, Batch 0, Loss: 1.501, Acc: 72.656%\n",
            "Epoch 117, Batch 100, Loss: 1.654, Acc: 76.269%\n",
            "Epoch 117, Batch 200, Loss: 1.722, Acc: 75.599%\n",
            "Epoch 117, Batch 300, Loss: 1.720, Acc: 75.540%\n",
            "Epoch 117 Training Loss: 1.709, Accuracy: 75.510%\n",
            "Epoch 118, Batch 0, Loss: 0.490, Acc: 77.344%\n",
            "Epoch 118, Batch 100, Loss: 1.684, Acc: 75.464%\n",
            "Epoch 118, Batch 200, Loss: 1.688, Acc: 75.350%\n",
            "Epoch 118, Batch 300, Loss: 1.653, Acc: 75.374%\n",
            "Epoch 118 Training Loss: 1.643, Accuracy: 75.373%\n",
            "Epoch 119, Batch 0, Loss: 1.568, Acc: 75.781%\n",
            "Epoch 119, Batch 100, Loss: 1.759, Acc: 75.627%\n",
            "Epoch 119, Batch 200, Loss: 1.717, Acc: 75.529%\n",
            "Epoch 119, Batch 300, Loss: 1.647, Acc: 75.361%\n",
            "Epoch 119 Training Loss: 1.651, Accuracy: 75.380%\n",
            "Epoch 120, Batch 0, Loss: 1.636, Acc: 74.219%\n",
            "Epoch 120, Batch 100, Loss: 1.581, Acc: 75.124%\n",
            "Epoch 120, Batch 200, Loss: 1.617, Acc: 75.295%\n",
            "Epoch 120, Batch 300, Loss: 1.631, Acc: 75.306%\n",
            "Epoch 120 Training Loss: 1.617, Accuracy: 75.275%\n",
            "Evaluating at Epoch 120...\n",
            "Test Accuracy: 64.23%\n",
            "High Confidence Predictions: 7750\n",
            "High Confidence Accuracy: 92.55%\n",
            "Wrong High Confidence Predictions: 577\n",
            "Epoch 121, Batch 0, Loss: 0.430, Acc: 79.688%\n",
            "Epoch 121, Batch 100, Loss: 1.563, Acc: 75.193%\n",
            "Epoch 121, Batch 200, Loss: 1.608, Acc: 75.575%\n",
            "Epoch 121, Batch 300, Loss: 1.610, Acc: 75.449%\n",
            "Epoch 121 Training Loss: 1.615, Accuracy: 75.433%\n",
            "Epoch 122, Batch 0, Loss: 1.615, Acc: 71.875%\n",
            "Epoch 122, Batch 100, Loss: 1.492, Acc: 74.977%\n",
            "Epoch 122, Batch 200, Loss: 1.528, Acc: 75.109%\n",
            "Epoch 122, Batch 300, Loss: 1.529, Acc: 75.244%\n",
            "Epoch 122 Training Loss: 1.550, Accuracy: 75.293%\n",
            "Epoch 123, Batch 0, Loss: 0.494, Acc: 71.875%\n",
            "Epoch 123, Batch 100, Loss: 1.572, Acc: 75.913%\n",
            "Epoch 123, Batch 200, Loss: 1.619, Acc: 75.808%\n",
            "Epoch 123, Batch 300, Loss: 1.611, Acc: 75.558%\n",
            "Epoch 123 Training Loss: 1.620, Accuracy: 75.558%\n",
            "Epoch 124, Batch 0, Loss: 0.404, Acc: 78.906%\n",
            "Epoch 124, Batch 100, Loss: 1.718, Acc: 75.379%\n",
            "Epoch 124, Batch 200, Loss: 1.672, Acc: 75.529%\n",
            "Epoch 124, Batch 300, Loss: 1.640, Acc: 75.504%\n",
            "Epoch 124 Training Loss: 1.637, Accuracy: 75.510%\n",
            "Epoch 125, Batch 0, Loss: 1.569, Acc: 71.875%\n",
            "Epoch 125, Batch 100, Loss: 1.741, Acc: 75.379%\n",
            "Epoch 125, Batch 200, Loss: 1.652, Acc: 75.179%\n",
            "Epoch 125, Batch 300, Loss: 1.713, Acc: 75.257%\n",
            "Epoch 125 Training Loss: 1.720, Accuracy: 75.235%\n",
            "Epoch 126, Batch 0, Loss: 3.651, Acc: 70.312%\n",
            "Epoch 126, Batch 100, Loss: 1.768, Acc: 74.683%\n",
            "Epoch 126, Batch 200, Loss: 1.683, Acc: 75.105%\n",
            "Epoch 126, Batch 300, Loss: 1.634, Acc: 75.151%\n",
            "Epoch 126 Training Loss: 1.654, Accuracy: 75.075%\n",
            "Epoch 127, Batch 0, Loss: 2.516, Acc: 73.438%\n",
            "Epoch 127, Batch 100, Loss: 1.491, Acc: 75.990%\n",
            "Epoch 127, Batch 200, Loss: 1.549, Acc: 75.742%\n",
            "Epoch 127, Batch 300, Loss: 1.545, Acc: 75.685%\n",
            "Epoch 127 Training Loss: 1.530, Accuracy: 75.665%\n",
            "Epoch 128, Batch 0, Loss: 0.333, Acc: 80.469%\n",
            "Epoch 128, Batch 100, Loss: 1.783, Acc: 75.603%\n",
            "Epoch 128, Batch 200, Loss: 1.810, Acc: 75.202%\n",
            "Epoch 128, Batch 300, Loss: 1.736, Acc: 75.363%\n",
            "Epoch 128 Training Loss: 1.727, Accuracy: 75.257%\n",
            "Epoch 129, Batch 0, Loss: 2.625, Acc: 68.750%\n",
            "Epoch 129, Batch 100, Loss: 1.765, Acc: 75.472%\n",
            "Epoch 129, Batch 200, Loss: 1.682, Acc: 75.579%\n",
            "Epoch 129, Batch 300, Loss: 1.702, Acc: 75.407%\n",
            "Epoch 129 Training Loss: 1.714, Accuracy: 75.422%\n",
            "Epoch 130, Batch 0, Loss: 2.406, Acc: 75.781%\n",
            "Epoch 130, Batch 100, Loss: 1.522, Acc: 76.021%\n",
            "Epoch 130, Batch 200, Loss: 1.544, Acc: 75.735%\n",
            "Epoch 130, Batch 300, Loss: 1.496, Acc: 75.859%\n",
            "Epoch 130 Training Loss: 1.506, Accuracy: 75.815%\n",
            "Evaluating at Epoch 130...\n",
            "Test Accuracy: 64.42%\n",
            "High Confidence Predictions: 7700\n",
            "High Confidence Accuracy: 92.40%\n",
            "Wrong High Confidence Predictions: 585\n",
            "Epoch 131, Batch 0, Loss: 0.452, Acc: 78.125%\n",
            "Epoch 131, Batch 100, Loss: 1.577, Acc: 75.596%\n",
            "Epoch 131, Batch 200, Loss: 1.609, Acc: 75.723%\n",
            "Epoch 131, Batch 300, Loss: 1.611, Acc: 75.685%\n",
            "Epoch 131 Training Loss: 1.625, Accuracy: 75.683%\n",
            "Epoch 132, Batch 0, Loss: 0.463, Acc: 80.469%\n",
            "Epoch 132, Batch 100, Loss: 1.535, Acc: 75.348%\n",
            "Epoch 132, Batch 200, Loss: 1.430, Acc: 75.466%\n",
            "Epoch 132, Batch 300, Loss: 1.478, Acc: 75.589%\n",
            "Epoch 132 Training Loss: 1.496, Accuracy: 75.608%\n",
            "Epoch 133, Batch 0, Loss: 0.484, Acc: 74.219%\n",
            "Epoch 133, Batch 100, Loss: 1.689, Acc: 75.085%\n",
            "Epoch 133, Batch 200, Loss: 1.690, Acc: 75.303%\n",
            "Epoch 133, Batch 300, Loss: 1.689, Acc: 75.241%\n",
            "Epoch 133 Training Loss: 1.679, Accuracy: 75.248%\n",
            "Epoch 134, Batch 0, Loss: 0.354, Acc: 82.812%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import os\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Early stopping parameters\n",
        "early_stopping_patience = 150  # Number of epochs with no improvement to stop training\n",
        "best_test_accuracy = 0.0  # Best test accuracy seen so far\n",
        "epochs_no_improvement = 0  # Counter for how many epochs with no improvement\n",
        "\n",
        "# Data Augmentation for Training Set\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),             # Random cropping with padding\n",
        "    transforms.RandomHorizontalFlip(),                # Random horizontal flip\n",
        "    transforms.RandomRotation(15),                    # Randomly rotate by 15 degrees\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Random jitter in brightness, contrast, etc.\n",
        "    transforms.ToTensor(),                            # Convert images to tensors\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize the images\n",
        "])\n",
        "\n",
        "# Data Transformations for the Test Set (no augmentation)\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),                            # Convert images to tensors\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize the images\n",
        "])\n",
        "\n",
        "# Load the CIFAR100 dataset\n",
        "dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
        "test_dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "# Randomly select 10,000 examples to move from train to test\n",
        "train_indices, extra_test_indices = train_test_split(list(range(len(dataset))), test_size=0.2, random_state=42)\n",
        "\n",
        "# Subset the train dataset to have 40,000 examples\n",
        "train_subset = torch.utils.data.Subset(dataset, train_indices)\n",
        "extra_test_subset = torch.utils.data.Subset(dataset, extra_test_indices)\n",
        "\n",
        "# Combine the additional 10,000 examples with the original test dataset\n",
        "test_dataset = torch.utils.data.ConcatDataset([test_dataset, extra_test_subset])\n",
        "\n",
        "# Create dataloaders\n",
        "trainloader = torch.utils.data.DataLoader(train_subset, batch_size=128, shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "# Set paths and other parameters\n",
        "PENALTY_WEIGHT = 1  # Weight for penalizing incorrect predictions after 50% accuracy\n",
        "SAVE_PATH = './saved_models/'  # Directory to save model checkpoints\n",
        "if not os.path.exists(SAVE_PATH):\n",
        "    os.makedirs(SAVE_PATH)\n",
        "\n",
        "# Temperature Scaling class\n",
        "class TemperatureScaling(nn.Module):\n",
        "    def __init__(self, init_temp=1.0):\n",
        "        super(TemperatureScaling, self).__init__()\n",
        "        self.temperature = nn.Parameter(torch.ones(1) * init_temp)\n",
        "\n",
        "    def forward(self, logits):\n",
        "        return logits / self.temperature\n",
        "\n",
        "# Focal Loss for handling imbalanced datasets\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=1, gamma=2):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        BCE_loss = F.cross_entropy(outputs, targets, reduction='none')\n",
        "        pt = torch.exp(-BCE_loss)  # Get the probability\n",
        "        focal_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
        "        return focal_loss.mean()\n",
        "\n",
        "# Custom Loss Function with Focal Loss and Penalty for Wrong Predictions after 50% Accuracy\n",
        "def custom_loss_function(outputs, targets, current_accuracy, confidences):\n",
        "    # Apply softmax to get probabilities\n",
        "    probabilities = F.softmax(outputs, dim=1)\n",
        "\n",
        "    # Get the predicted class and the confidence\n",
        "    confidences, predicted_classes = torch.max(probabilities, dim=1)\n",
        "\n",
        "    # Calculate the Focal Loss for class imbalance\n",
        "    focal_loss = FocalLoss()(outputs, targets)\n",
        "\n",
        "    # Penalize wrong high-confidence predictions (confidence > 80%)\n",
        "    high_confidence_penalty = (confidences > 0.8) & (predicted_classes != targets)\n",
        "    wrong_high_conf_penalty = PENALTY_WEIGHT * high_confidence_penalty.sum()\n",
        "\n",
        "    # Calculate the total loss\n",
        "    total_loss = focal_loss + wrong_high_conf_penalty\n",
        "    return total_loss\n",
        "\n",
        "# WideResNeXt Block\n",
        "class WideResNeXtBlock(nn.Module):\n",
        "    expansion = 2  # Expansion factor for WideResNeXt\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, cardinality=32, widen_factor=2):\n",
        "        super(WideResNeXtBlock, self).__init__()\n",
        "        D = cardinality * widen_factor\n",
        "        self.conv1 = nn.Conv2d(in_planes, D, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(D)\n",
        "        self.conv2 = nn.Conv2d(D, D, kernel_size=3, stride=stride, padding=1, groups=cardinality, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(D)\n",
        "        self.conv3 = nn.Conv2d(D, planes * WideResNeXtBlock.expansion, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * WideResNeXtBlock.expansion)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes * WideResNeXtBlock.expansion:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, planes * WideResNeXtBlock.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * WideResNeXtBlock.expansion)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.relu(self.bn1(self.conv1(x)))\n",
        "        out = torch.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = torch.relu(out)\n",
        "        return out\n",
        "\n",
        "# WideResNeXt Model with Temperature Scaling\n",
        "class WideResNeXt(nn.Module):\n",
        "    def __init__(self, block, num_blocks, cardinality=32, widen_factor=2, num_classes=100):\n",
        "        super(WideResNeXt, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1, cardinality=cardinality, widen_factor=widen_factor)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2, cardinality=cardinality, widen_factor=widen_factor)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2, cardinality=cardinality, widen_factor=widen_factor)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2, cardinality=cardinality, widen_factor=widen_factor)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=0.5)  # Add Dropout layer with 0.5 probability\n",
        "        self.linear = nn.Linear(512 * WideResNeXtBlock.expansion, num_classes)\n",
        "        self.temperature_scaling = TemperatureScaling()  # Temperature scaling layer\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride, cardinality, widen_factor):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride, cardinality, widen_factor))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = torch.nn.functional.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.dropout(out)  # Apply Dropout before the final linear layer\n",
        "        out = self.linear(out)\n",
        "        out = self.temperature_scaling(out)  # Apply temperature scaling before softmax\n",
        "        return out\n",
        "\n",
        "# Training function\n",
        "def train_with_penalty(epoch):\n",
        "    model.train()  # Set the model to training mode\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()  # Zero the gradients for the optimizer\n",
        "\n",
        "        outputs = model(inputs)  # Forward pass\n",
        "\n",
        "        # Calculate the overall training accuracy before updating weights\n",
        "        probabilities = F.softmax(outputs, dim=1)\n",
        "        _, predicted_classes = torch.max(probabilities, dim=1)\n",
        "\n",
        "        correct_predictions = predicted_classes.eq(targets).sum().item()\n",
        "        total += targets.size(0)\n",
        "        current_accuracy = correct_predictions / total\n",
        "\n",
        "        # Calculate the custom loss with penalties if training accuracy > 50%\n",
        "        loss = custom_loss_function(outputs, targets, current_accuracy, probabilities)\n",
        "        loss.backward()  # Backpropagation\n",
        "        optimizer.step()  # Update weights\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        correct += correct_predictions\n",
        "\n",
        "        if batch_idx % 100 == 0:  # Print every 100 batches\n",
        "            print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {train_loss / (batch_idx + 1):.3f}, Acc: {100.*correct/total:.3f}%')\n",
        "\n",
        "    # At the end of the epoch, print the final training accuracy\n",
        "    print(f'Epoch {epoch} Training Loss: {train_loss / len(trainloader):.3f}, Accuracy: {100.*correct/total:.3f}%')\n",
        "\n",
        "# Function to evaluate the model on the test set and calculate high confidence metrics\n",
        "def evaluate_with_high_confidence():\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    high_confidence_correct = 0\n",
        "    high_confidence_total = 0\n",
        "    high_confidence_wrong = 0  # Track high confidence wrong predictions\n",
        "    confidence_threshold = 0.80  # Define the high confidence threshold\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation for inference\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Calculate probabilities using softmax\n",
        "            probabilities = F.softmax(outputs, dim=1)\n",
        "\n",
        "            # Get the predicted class and the confidence (max probability)\n",
        "            confidences, predicted_classes = torch.max(probabilities, dim=1)\n",
        "\n",
        "            # Track total correct predictions\n",
        "            correct += predicted_classes.eq(targets).sum().item()\n",
        "            total += targets.size(0)\n",
        "\n",
        "            # Track high confidence predictions\n",
        "            high_confidence_mask = confidences > confidence_threshold\n",
        "            high_confidence_predictions = predicted_classes[high_confidence_mask]\n",
        "            high_confidence_targets = targets[high_confidence_mask]\n",
        "            high_confidence_correct += high_confidence_predictions.eq(high_confidence_targets).sum().item()\n",
        "            high_confidence_total += high_confidence_mask.sum().item()\n",
        "\n",
        "            # Count the number of wrong high-confidence predictions\n",
        "            high_confidence_wrong += (high_confidence_predictions != high_confidence_targets).sum().item()\n",
        "\n",
        "    # Calculate overall test accuracy\n",
        "    test_accuracy = 100. * correct / total\n",
        "\n",
        "    # Calculate high confidence accuracy\n",
        "    if high_confidence_total > 0:\n",
        "        high_confidence_accuracy = 100. * high_confidence_correct / high_confidence_total\n",
        "    else:\n",
        "        high_confidence_accuracy = 0.0\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "    print(f\"High Confidence Predictions: {high_confidence_total}\")\n",
        "    print(f\"High Confidence Accuracy: {high_confidence_accuracy:.2f}%\")\n",
        "    print(f\"Wrong High Confidence Predictions: {high_confidence_wrong}\")\n",
        "\n",
        "# Model, loss, optimizer, and scheduler\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = WideResNeXt(WideResNeXtBlock, [3, 4, 6, 3], cardinality=32, widen_factor=2).to(device)\n",
        "\n",
        "# Example optimizer\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
        "scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "\n",
        "# Training loop with evaluation after each epoch\n",
        "for epoch in range(0, 250):\n",
        "    train_with_penalty(epoch)  # Perform training for this epoch\n",
        "    scheduler.step()\n",
        "\n",
        "    if epoch % 10 == 0:  # Evaluate every 10 epochs or at desired intervals\n",
        "        print(f\"Evaluating at Epoch {epoch}...\")\n",
        "        evaluate_with_high_confidence()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jome4Gx_sn4r"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShAzI3hBeoDn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5Fyvx-x3Tkdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RklHL0uAavnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uRNitLe-Rk-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C1Cc5IGARbkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YP8EP3nbhokl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uM8er3a9iYOR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}