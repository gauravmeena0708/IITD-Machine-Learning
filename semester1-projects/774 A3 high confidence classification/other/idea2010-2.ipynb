{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0130466",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T15:26:44.640734Z",
     "iopub.status.busy": "2024-10-20T15:26:44.640347Z",
     "iopub.status.idle": "2024-10-20T15:26:49.503373Z",
     "shell.execute_reply": "2024-10-20T15:26:49.502534Z"
    },
    "papermill": {
     "duration": 4.869576,
     "end_time": "2024-10-20T15:26:49.505657",
     "exception": false,
     "start_time": "2024-10-20T15:26:44.636081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim.lr_scheduler import CyclicLR\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import torch.nn.functional as F\n",
    "import argparse\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cddc7bab",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-10-20T15:26:49.512502Z",
     "iopub.status.busy": "2024-10-20T15:26:49.512086Z",
     "iopub.status.idle": "2024-10-20T15:26:57.525671Z",
     "shell.execute_reply": "2024-10-20T15:26:57.524633Z"
    },
    "papermill": {
     "duration": 8.019553,
     "end_time": "2024-10-20T15:26:57.528066",
     "exception": false,
     "start_time": "2024-10-20T15:26:49.508513",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp -r /kaggle/input/col774a3/* /kaggle/working/\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e0b7829",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T15:26:57.534803Z",
     "iopub.status.busy": "2024-10-20T15:26:57.534343Z",
     "iopub.status.idle": "2024-10-20T15:26:57.552477Z",
     "shell.execute_reply": "2024-10-20T15:26:57.551585Z"
    },
    "papermill": {
     "duration": 0.023944,
     "end_time": "2024-10-20T15:26:57.554434",
     "exception": false,
     "start_time": "2024-10-20T15:26:57.530490",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import torch.optim as optim\n",
    "# import torchvision\n",
    "# import torchvision.transforms as transforms\n",
    "# from torch.optim.lr_scheduler import StepLR\n",
    "# import os\n",
    "# import pickle\n",
    "# import pandas as pd\n",
    "\n",
    "# # Set a confidence threshold (will be dynamically generated for each class)\n",
    "# PENALTY_WEIGHT = 10  # Weight for penalizing incorrect predictions after 50% accuracy\n",
    "# SAVE_PATH = './saved_models/'  # Directory to save model checkpoints\n",
    "# if not os.path.exists(SAVE_PATH):\n",
    "#     os.makedirs(SAVE_PATH)\n",
    "\n",
    "# class TemperatureScaling(nn.Module):\n",
    "#     def __init__(self, init_temp=1.0):  # Set a lower initial temperature\n",
    "#         super(TemperatureScaling, self).__init__()\n",
    "#         self.temperature = nn.Parameter(torch.ones(1) * init_temp)\n",
    "\n",
    "#     def forward(self, logits):\n",
    "#         return logits / self.temperature\n",
    "\n",
    "\n",
    "# # Focal Loss for handling imbalanced datasets\n",
    "# class FocalLoss(nn.Module):\n",
    "#     def __init__(self, alpha=1, gamma=2):\n",
    "#         super(FocalLoss, self).__init__()\n",
    "#         self.alpha = alpha\n",
    "#         self.gamma = gamma\n",
    "\n",
    "#     def forward(self, outputs, targets):\n",
    "#         BCE_loss = F.cross_entropy(outputs, targets, reduction='none')\n",
    "#         pt = torch.exp(-BCE_loss)  # Get the probability\n",
    "#         focal_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "#         return focal_loss.mean()\n",
    "\n",
    "# # Custom Loss Function with Focal Loss and Penalty for Wrong Predictions after 50% Accuracy\n",
    "# def custom_loss_function(outputs, targets, current_accuracy):\n",
    "#     # Apply softmax to get probabilities\n",
    "#     probabilities = F.softmax(outputs, dim=1)\n",
    "    \n",
    "#     # Get the max probability (confidence) and corresponding predicted class\n",
    "#     confidences, predicted_classes = torch.max(probabilities, dim=1)\n",
    "    \n",
    "#     # Calculate the Focal Loss for class imbalance\n",
    "#     focal_loss = FocalLoss()(outputs, targets)\n",
    "#     wrong_predictions = (predicted_classes != targets).float()\n",
    "#     # Modified penalty\n",
    "#     if current_accuracy > 0.5:\n",
    "#         wrong_prediction_penalty = PENALTY_WEIGHT * (1 - current_accuracy) * wrong_predictions.sum()  # Scale penalty based on accuracy\n",
    "#     else:\n",
    "#         wrong_prediction_penalty = 0\n",
    "\n",
    "#     # Calculate the total loss\n",
    "#     total_loss = focal_loss + wrong_prediction_penalty\n",
    "#     return total_loss\n",
    "\n",
    "# # WideResNeXt Block\n",
    "# class WideResNeXtBlock(nn.Module):\n",
    "#     expansion = 2  # Expansion factor for WideResNeXt\n",
    "\n",
    "#     def __init__(self, in_planes, planes, stride=1, cardinality=32, widen_factor=2):\n",
    "#         super(WideResNeXtBlock, self).__init__()\n",
    "#         D = cardinality * widen_factor\n",
    "#         self.conv1 = nn.Conv2d(in_planes, D, kernel_size=1, bias=False)\n",
    "#         self.bn1 = nn.BatchNorm2d(D)\n",
    "#         self.conv2 = nn.Conv2d(D, D, kernel_size=3, stride=stride, padding=1, groups=cardinality, bias=False)\n",
    "#         self.bn2 = nn.BatchNorm2d(D)\n",
    "#         self.conv3 = nn.Conv2d(D, planes * WideResNeXtBlock.expansion, kernel_size=1, bias=False)\n",
    "#         self.bn3 = nn.BatchNorm2d(planes * WideResNeXtBlock.expansion)\n",
    "\n",
    "#         self.shortcut = nn.Sequential()\n",
    "#         if stride != 1 or in_planes != planes * WideResNeXtBlock.expansion:\n",
    "#             self.shortcut = nn.Sequential(\n",
    "#                 nn.Conv2d(in_planes, planes * WideResNeXtBlock.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "#                 nn.BatchNorm2d(planes * WideResNeXtBlock.expansion)\n",
    "#             )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         out = torch.relu(self.bn1(self.conv1(x)))\n",
    "#         out = torch.relu(self.bn2(self.conv2(out)))\n",
    "#         out = self.bn3(self.conv3(out))\n",
    "#         out += self.shortcut(x)\n",
    "#         out = torch.relu(out)\n",
    "#         return out\n",
    "\n",
    "# # WideResNeXt Model\n",
    "# class WideResNeXt(nn.Module):\n",
    "#     def __init__(self, block, num_blocks, cardinality=32, widen_factor=2, num_classes=100):\n",
    "#         super(WideResNeXt, self).__init__()\n",
    "#         self.in_planes = 64\n",
    "\n",
    "#         self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "#         self.bn1 = nn.BatchNorm2d(64)\n",
    "\n",
    "#         self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1, cardinality=cardinality, widen_factor=widen_factor)\n",
    "#         self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2, cardinality=cardinality, widen_factor=widen_factor)\n",
    "#         self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2, cardinality=cardinality, widen_factor=widen_factor)\n",
    "#         self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2, cardinality=cardinality, widen_factor=widen_factor)\n",
    "\n",
    "#         self.linear = nn.Linear(512 * WideResNeXtBlock.expansion, num_classes)\n",
    "#         self.temperature_scaling = TemperatureScaling()  # Add temperature scaling\n",
    "\n",
    "#     def _make_layer(self, block, planes, num_blocks, stride, cardinality, widen_factor):\n",
    "#         strides = [stride] + [1] * (num_blocks - 1)\n",
    "#         layers = []\n",
    "#         for stride in strides:\n",
    "#             layers.append(block(self.in_planes, planes, stride, cardinality, widen_factor))\n",
    "#             self.in_planes = planes * block.expansion\n",
    "#         return nn.Sequential(*layers)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         out = torch.relu(self.bn1(self.conv1(x)))\n",
    "#         out = self.layer1(out)\n",
    "#         out = self.layer2(out)\n",
    "#         out = self.layer3(out)\n",
    "#         out = self.layer4(out)\n",
    "#         out = torch.nn.functional.avg_pool2d(out, 4)\n",
    "#         out = out.view(out.size(0), -1)\n",
    "#         out = self.linear(out)\n",
    "#         out = self.temperature_scaling(out)  # Apply temperature scaling to logits\n",
    "#         return out\n",
    "\n",
    "# # WideResNeXt-101 Model Definition\n",
    "# def WideResNeXt101():\n",
    "#     return WideResNeXt(WideResNeXtBlock, [3, 4, 23, 3], cardinality=32, widen_factor=2)\n",
    "\n",
    "# # Custom Dataset class for loading CIFAR100 data from .pkl files\n",
    "# class CIFAR100Dataset(Dataset):\n",
    "#     def __init__(self, file_path):\n",
    "#         with open(file_path, 'rb') as f:\n",
    "#             self.data = pickle.load(f)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         image, label = self.data[idx]\n",
    "#         return image, label\n",
    "\n",
    "# # Training function with temperature scaling\n",
    "# def train_with_penalty(epoch, class_thresholds):\n",
    "#     model.train()\n",
    "#     train_loss = 0\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "\n",
    "#     for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "#         inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(inputs)\n",
    "        \n",
    "#         # Calculate the overall training accuracy before updating weights\n",
    "#         probabilities = F.softmax(outputs, dim=1)\n",
    "#         _, predicted_classes = torch.max(probabilities, dim=1)\n",
    "#         correct_predictions = predicted_classes.eq(targets).sum().item()\n",
    "#         total += targets.size(0)\n",
    "#         current_accuracy = correct_predictions / total\n",
    "        \n",
    "#         # Calculate custom loss with penalties if training accuracy is above 50%\n",
    "#         loss = custom_loss_function(outputs, targets, current_accuracy)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         train_loss += loss.item()\n",
    "#         correct += correct_predictions\n",
    "\n",
    "#         if batch_idx % 100 == 0:\n",
    "#             print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {train_loss / (batch_idx + 1):.3f}, Acc: {100.*correct/total:.3f}%')\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# # Test function with class-specific thresholds and evaluation\n",
    "# def test_with_class_thresholds(epoch, class_thresholds, test_info_path):\n",
    "#     global best_accuracy\n",
    "#     model.eval()\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "    \n",
    "#     predictions = []  # Store predictions for submission\n",
    "#     ids = []  # Store corresponding IDs for submission\n",
    "    \n",
    "#     # Variables for calculating accuracy without considering confidence threshold\n",
    "#     correct_all = 0\n",
    "#     total_all = 0\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "        \n",
    "#         for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "#             inputs, targets = inputs.to(device), targets.to(device)\n",
    "#             outputs = model(inputs)\n",
    "#             probabilities = F.softmax(outputs, dim=1)\n",
    "#             confidences, predicted_classes = torch.max(probabilities, dim=1)\n",
    "            \n",
    "#             # Count correct predictions for calculating test accuracy (ignore confidence threshold)\n",
    "#             correct_predictions_all = predicted_classes.eq(targets).sum().item()\n",
    "#             correct_all += correct_predictions_all  # Add the correct predictions to total\n",
    "#             total_all += targets.size(0)  # Add total samples\n",
    "            \n",
    "#             # Now apply class-specific thresholds for skipping\n",
    "#             for i in range(len(inputs)):\n",
    "#                 class_idx = predicted_classes[i].item()\n",
    "#                 confidence = confidences[i].item()  # Convert to a float for comparison\n",
    "\n",
    "#                 # Use max(0.5, class_threshold) for determining low-confidence\n",
    "#                 threshold = max(0.5, class_thresholds[class_idx])\n",
    "\n",
    "#                 if confidence >= threshold:  # Check if confidence meets the class-specific threshold\n",
    "#                     predictions.append(predicted_classes[i].item())  # Save the predicted class\n",
    "#                 else:\n",
    "#                     predictions.append(-1)  # Assign -1 for low-confidence predictions\n",
    "                \n",
    "#                 ids.append(batch_idx * testloader.batch_size + i)\n",
    "\n",
    "#     # Save predictions to CSV\n",
    "#     save_predictions_to_csv(ids, predictions)\n",
    "    \n",
    "#     # Evaluate using calculate_score\n",
    "#     evaluate_and_print_score(test_info_path, 'submission.csv')\n",
    "\n",
    "#     # Calculate and print total correct and total test cases without applying confidence threshold\n",
    "#     print(f\"Total correct (without confidence threshold): {correct_all} out of {total_all} test cases\")\n",
    "\n",
    "#     # Calculate accuracy (without considering confidence threshold)\n",
    "#     if total_all > 0:\n",
    "#         test_accuracy = 100. * correct_all / total_all\n",
    "#     else:\n",
    "#         test_accuracy = 0.0\n",
    "#     print(f\"Test Accuracy (without confidence threshold): {test_accuracy:.2f}%\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def save_predictions_to_csv(ids, predictions):\n",
    "#     df = pd.DataFrame({'ID': ids, 'Predicted_label': predictions})\n",
    "#     df.to_csv('submission.csv', index=False)\n",
    "#     print(\"Predictions saved to submission.csv\")\n",
    "\n",
    "\n",
    "# # Function to evaluate and print score for each epoch\n",
    "# def evaluate_and_print_score(test_info_path, prediction_file):\n",
    "#     batch_1, batch_2 = evaluate_in_two_sets(test_info_path, prediction_file)\n",
    "    \n",
    "#     # Print scores for both sets\n",
    "#     print(f\"\\nBatch 1: Correct: {batch_1['correct']}, Incorrect: {batch_1['incorrect']}, Skipped: {batch_1['skipped']}, Score: {batch_1['score']:.2f}\")\n",
    "#     print(f\"Batch 2: Correct: {batch_2['correct']}, Incorrect: {batch_2['incorrect']}, Skipped: {batch_2['skipped']}, Score: {batch_2['score']:.2f}\")\n",
    "\n",
    "# # Function to calculate the score for predictions\n",
    "# def calculate_score(true_labels: pd.Series, predicted_labels: pd.Series, accuracy_threshold: float = 0.7, gamma: float = 5.0):\n",
    "#     data = pd.DataFrame({'True_label': true_labels, 'Predicted_label': predicted_labels})\n",
    "#     total_skipped = (data['Predicted_label'] == -1).sum()\n",
    "#     filtered_df = data[data['Predicted_label'] != -1]\n",
    "    \n",
    "#     all_classes = list(range(100))  # Assuming CIFAR-100 dataset with 100 classes\n",
    "#     sum_of_correctly_classified_high_accuracy = 0\n",
    "#     sum_of_correctly_classified_low_accuracy = 0\n",
    "#     total_correct = 0\n",
    "#     total_incorrect = 0\n",
    "    \n",
    "#     accuracy_per_class = {}\n",
    "#     grouped = filtered_df.groupby('Predicted_label')\n",
    "    \n",
    "#     for name, group in grouped:\n",
    "#         accuracy = (group['True_label'] == group['Predicted_label']).sum() / len(group)\n",
    "#         accuracy_per_class[name] = accuracy\n",
    "#         total_correct += (group['True_label'] == group['Predicted_label']).sum()\n",
    "#         total_incorrect += (group['True_label'] != group['Predicted_label']).sum()\n",
    "    \n",
    "#     for cls in all_classes:\n",
    "#         total = len(filtered_df[filtered_df['Predicted_label'] == cls])\n",
    "#         class_accuracy = accuracy_per_class.get(cls, 0.0)\n",
    "        \n",
    "#         if class_accuracy >= accuracy_threshold:\n",
    "#             sum_of_correctly_classified_high_accuracy += total\n",
    "#         else:\n",
    "#             sum_of_correctly_classified_low_accuracy += total\n",
    "\n",
    "#     final_score = sum_of_correctly_classified_high_accuracy - gamma * sum_of_correctly_classified_low_accuracy\n",
    "#     return final_score, total_correct, total_incorrect, total_skipped\n",
    "\n",
    "# # Function to evaluate the test predictions in two sets\n",
    "# def evaluate_in_two_sets(test_info_path: str, prediction_file: str):\n",
    "#     test_info = pd.read_csv(test_info_path)\n",
    "#     predictions = pd.read_csv(prediction_file)\n",
    "#     merged_data = pd.merge(test_info, predictions, on='ID')\n",
    "    \n",
    "#     first_half = merged_data[:10000]\n",
    "#     second_half = merged_data[10000:20000]\n",
    "    \n",
    "#     score_first_half, correct_first_half, incorrect_first_half, skipped_first_half = calculate_score(first_half['True_label'], first_half['Predicted_label'])\n",
    "#     score_second_half, correct_second_half, incorrect_second_half, skipped_second_half = calculate_score(second_half['True_label'], second_half['Predicted_label'])\n",
    "    \n",
    "#     batch_1 = {\n",
    "#         'total': 10000,\n",
    "#         'correct': correct_first_half,\n",
    "#         'incorrect': incorrect_first_half,\n",
    "#         'skipped': skipped_first_half,\n",
    "#         'score': score_first_half\n",
    "#     }\n",
    "    \n",
    "#     batch_2 = {\n",
    "#         'total': 10000,\n",
    "#         'correct': correct_second_half,\n",
    "#         'incorrect': incorrect_second_half,\n",
    "#         'skipped': skipped_second_half,\n",
    "#         'score': score_second_half\n",
    "#     }\n",
    "    \n",
    "#     return batch_1, batch_2\n",
    "\n",
    "# # Generate class-specific thresholds\n",
    "# def generate_class_thresholds(trainloader, target_accuracy=80.0):  # Adjusted to 70% confidence\n",
    "#     class_confidences = {i: [] for i in range(100)}  # Store confidences per class\n",
    "\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         for inputs, targets in trainloader:\n",
    "#             inputs, targets = inputs.to(device), targets.to(device)\n",
    "#             outputs = model(inputs)\n",
    "#             probabilities = F.softmax(outputs, dim=1)\n",
    "#             confidences, predicted_classes = torch.max(probabilities, dim=1)\n",
    "            \n",
    "#             for i in range(len(targets)):\n",
    "#                 class_idx = targets[i].item()\n",
    "#                 class_confidences[class_idx].append(confidences[i].item())\n",
    "\n",
    "#     class_thresholds = {}\n",
    "#     for cls, confidences in class_confidences.items():\n",
    "#         sorted_confidences = sorted(confidences)\n",
    "#         threshold_idx = int(len(sorted_confidences) * (1 - target_accuracy / 100))\n",
    "#         class_thresholds[cls] = sorted_confidences[threshold_idx]\n",
    "    \n",
    "#     return class_thresholds\n",
    "\n",
    "\n",
    "# # Load datasets\n",
    "# train_dataset = CIFAR100Dataset('train.pkl')\n",
    "# test_dataset = CIFAR100Dataset('test.pkl')\n",
    "\n",
    "# trainloader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
    "# testloader = DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "# # Model, loss, optimizer, and scheduler\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# model = WideResNeXt101().to(device)\n",
    "\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "# scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "# best_accuracy = 0.0\n",
    "\n",
    "# # Generate class-specific thresholds based on 99.99% accuracy\n",
    "# #class_thresholds = generate_class_thresholds(trainloader, target_accuracy=99.99)\n",
    "# class_thresholds = generate_class_thresholds(trainloader, target_accuracy=80)\n",
    "\n",
    "# # Example Training Loop\n",
    "# for epoch in range(0, 100):\n",
    "#     train_with_penalty(epoch, class_thresholds)\n",
    "#     test_with_class_thresholds(epoch, class_thresholds, 'test_info.csv')\n",
    "#     scheduler.step()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5906773,
     "sourceId": 9666920,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 16.930088,
   "end_time": "2024-10-20T15:26:58.877509",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-20T15:26:41.947421",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
