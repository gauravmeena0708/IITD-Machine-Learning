{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "959a26fe",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-10-29T03:44:34.109871Z",
     "iopub.status.busy": "2024-10-29T03:44:34.109443Z",
     "iopub.status.idle": "2024-10-29T09:56:33.838137Z",
     "shell.execute_reply": "2024-10-29T09:56:33.837123Z"
    },
    "papermill": {
     "duration": 22319.735269,
     "end_time": "2024-10-29T09:56:33.840462",
     "exception": false,
     "start_time": "2024-10-29T03:44:34.105193",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169001437/169001437 [00:02<00:00, 74294852.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n",
      "Epoch [1/100], Training Loss: 4.2253\n",
      "Epoch [1/100], Test Loss: 4.0431, Test Accuracy: 5.86%\n",
      "Epoch [2/100], Training Loss: 3.7841\n",
      "Epoch [2/100], Test Loss: 3.7266, Test Accuracy: 12.11%\n",
      "Epoch [3/100], Training Loss: 3.3455\n",
      "Epoch [3/100], Test Loss: 3.1482, Test Accuracy: 20.44%\n",
      "Epoch [4/100], Training Loss: 2.9458\n",
      "Epoch [4/100], Test Loss: 3.2848, Test Accuracy: 24.36%\n",
      "Epoch [5/100], Training Loss: 2.6482\n",
      "Epoch [5/100], Test Loss: 2.7499, Test Accuracy: 28.96%\n",
      "Epoch [6/100], Training Loss: 2.4429\n",
      "Epoch [6/100], Test Loss: 2.5433, Test Accuracy: 34.65%\n",
      "Epoch [7/100], Training Loss: 2.2920\n",
      "Epoch [7/100], Test Loss: 2.2227, Test Accuracy: 40.38%\n",
      "Epoch [8/100], Training Loss: 2.0705\n",
      "Epoch [8/100], Test Loss: 2.0577, Test Accuracy: 43.53%\n",
      "Epoch [9/100], Training Loss: 1.9891\n",
      "Epoch [9/100], Test Loss: 2.0642, Test Accuracy: 43.86%\n",
      "Epoch [10/100], Training Loss: 1.8494\n",
      "Epoch [10/100], Test Loss: 2.0891, Test Accuracy: 42.56%\n",
      "Epoch [11/100], Training Loss: 1.7541\n",
      "Epoch [11/100], Test Loss: 1.7655, Test Accuracy: 51.07%\n",
      "Epoch [12/100], Training Loss: 1.6310\n",
      "Epoch [12/100], Test Loss: 1.7833, Test Accuracy: 51.08%\n",
      "Epoch [13/100], Training Loss: 1.5496\n",
      "Epoch [13/100], Test Loss: 1.5666, Test Accuracy: 56.30%\n",
      "Epoch [14/100], Training Loss: 1.4946\n",
      "Epoch [14/100], Test Loss: 1.7342, Test Accuracy: 53.37%\n",
      "Epoch [15/100], Training Loss: 1.4340\n",
      "Epoch [15/100], Test Loss: 1.5416, Test Accuracy: 56.39%\n",
      "Epoch [16/100], Training Loss: 1.3723\n",
      "Epoch [16/100], Test Loss: 1.4887, Test Accuracy: 58.40%\n",
      "Epoch [17/100], Training Loss: 1.3549\n",
      "Epoch [17/100], Test Loss: 1.4251, Test Accuracy: 60.09%\n",
      "Epoch [18/100], Training Loss: 1.3023\n",
      "Epoch [18/100], Test Loss: 1.4503, Test Accuracy: 60.29%\n",
      "Epoch [19/100], Training Loss: 1.2751\n",
      "Epoch [19/100], Test Loss: 1.5956, Test Accuracy: 56.96%\n",
      "Epoch [20/100], Training Loss: 1.2313\n",
      "Epoch [20/100], Test Loss: 1.4205, Test Accuracy: 60.13%\n",
      "Epoch [21/100], Training Loss: 1.1802\n",
      "Epoch [21/100], Test Loss: 1.3287, Test Accuracy: 63.11%\n",
      "Epoch [22/100], Training Loss: 1.1572\n",
      "Epoch [22/100], Test Loss: 1.4711, Test Accuracy: 60.18%\n",
      "Epoch [23/100], Training Loss: 1.1294\n",
      "Epoch [23/100], Test Loss: 1.4209, Test Accuracy: 61.86%\n",
      "Epoch [24/100], Training Loss: 1.1051\n",
      "Epoch [24/100], Test Loss: 1.3392, Test Accuracy: 63.04%\n",
      "Epoch [25/100], Training Loss: 1.0897\n",
      "Epoch [25/100], Test Loss: 1.3561, Test Accuracy: 63.02%\n",
      "Epoch [26/100], Training Loss: 1.0526\n",
      "Epoch [26/100], Test Loss: 1.3824, Test Accuracy: 62.28%\n",
      "Epoch [27/100], Training Loss: 1.0729\n",
      "Epoch [27/100], Test Loss: 1.3104, Test Accuracy: 63.86%\n",
      "Epoch [28/100], Training Loss: 1.0333\n",
      "Epoch [28/100], Test Loss: 1.4221, Test Accuracy: 62.50%\n",
      "Epoch [29/100], Training Loss: 1.0130\n",
      "Epoch [29/100], Test Loss: 1.3119, Test Accuracy: 63.65%\n",
      "Epoch [30/100], Training Loss: 0.9946\n",
      "Epoch [30/100], Test Loss: 1.2403, Test Accuracy: 66.06%\n",
      "Epoch [31/100], Training Loss: 0.7277\n",
      "Epoch [31/100], Test Loss: 0.9902, Test Accuracy: 71.75%\n",
      "Epoch [32/100], Training Loss: 0.6404\n",
      "Epoch [32/100], Test Loss: 0.9736, Test Accuracy: 72.91%\n",
      "Epoch [33/100], Training Loss: 0.6115\n",
      "Epoch [33/100], Test Loss: 0.9732, Test Accuracy: 72.69%\n",
      "Epoch [34/100], Training Loss: 0.5883\n",
      "Epoch [34/100], Test Loss: 0.9851, Test Accuracy: 72.40%\n",
      "Epoch [35/100], Training Loss: 0.5580\n",
      "Epoch [35/100], Test Loss: 1.0002, Test Accuracy: 72.51%\n",
      "Epoch [36/100], Training Loss: 0.5358\n",
      "Epoch [36/100], Test Loss: 0.9957, Test Accuracy: 72.69%\n",
      "Epoch [37/100], Training Loss: 0.5127\n",
      "Epoch [37/100], Test Loss: 1.0042, Test Accuracy: 72.74%\n",
      "Epoch [38/100], Training Loss: 0.4960\n",
      "Epoch [38/100], Test Loss: 1.0086, Test Accuracy: 72.63%\n",
      "Epoch [39/100], Training Loss: 0.4757\n",
      "Epoch [39/100], Test Loss: 1.0305, Test Accuracy: 72.68%\n",
      "Epoch [40/100], Training Loss: 0.4632\n",
      "Epoch [40/100], Test Loss: 1.0075, Test Accuracy: 73.01%\n",
      "Epoch [41/100], Training Loss: 0.4416\n",
      "Epoch [41/100], Test Loss: 1.0365, Test Accuracy: 72.76%\n",
      "Epoch [42/100], Training Loss: 0.4319\n",
      "Epoch [42/100], Test Loss: 1.0384, Test Accuracy: 72.91%\n",
      "Epoch [43/100], Training Loss: 0.4158\n",
      "Epoch [43/100], Test Loss: 1.0272, Test Accuracy: 72.57%\n",
      "Epoch [44/100], Training Loss: 0.4086\n",
      "Epoch [44/100], Test Loss: 1.0276, Test Accuracy: 72.99%\n",
      "Epoch [45/100], Training Loss: 0.3915\n",
      "Epoch [45/100], Test Loss: 1.0365, Test Accuracy: 72.78%\n",
      "Epoch [46/100], Training Loss: 0.3818\n",
      "Epoch [46/100], Test Loss: 1.0655, Test Accuracy: 72.45%\n",
      "Epoch [47/100], Training Loss: 0.3708\n",
      "Epoch [47/100], Test Loss: 1.0408, Test Accuracy: 73.20%\n",
      "Epoch [48/100], Training Loss: 0.3631\n",
      "Epoch [48/100], Test Loss: 1.0544, Test Accuracy: 72.69%\n",
      "Epoch [49/100], Training Loss: 0.3510\n",
      "Epoch [49/100], Test Loss: 1.0487, Test Accuracy: 73.04%\n",
      "Epoch [50/100], Training Loss: 0.3433\n",
      "Epoch [50/100], Test Loss: 1.0754, Test Accuracy: 72.97%\n",
      "Epoch [51/100], Training Loss: 0.3290\n",
      "Epoch [51/100], Test Loss: 1.0884, Test Accuracy: 72.56%\n",
      "Epoch [52/100], Training Loss: 0.3288\n",
      "Epoch [52/100], Test Loss: 1.0673, Test Accuracy: 72.99%\n",
      "Epoch [53/100], Training Loss: 0.3166\n",
      "Epoch [53/100], Test Loss: 1.0622, Test Accuracy: 73.32%\n",
      "Epoch [54/100], Training Loss: 0.3127\n",
      "Epoch [54/100], Test Loss: 1.0767, Test Accuracy: 73.12%\n",
      "Epoch [55/100], Training Loss: 0.3036\n",
      "Epoch [55/100], Test Loss: 1.0947, Test Accuracy: 73.25%\n",
      "Epoch [56/100], Training Loss: 0.2923\n",
      "Epoch [56/100], Test Loss: 1.0951, Test Accuracy: 72.86%\n",
      "Epoch [57/100], Training Loss: 0.2870\n",
      "Epoch [57/100], Test Loss: 1.0899, Test Accuracy: 72.89%\n",
      "Epoch [58/100], Training Loss: 0.2856\n",
      "Epoch [58/100], Test Loss: 1.1000, Test Accuracy: 72.93%\n",
      "Epoch [59/100], Training Loss: 0.2757\n",
      "Epoch [59/100], Test Loss: 1.0946, Test Accuracy: 73.03%\n",
      "Epoch [60/100], Training Loss: 0.2683\n",
      "Epoch [60/100], Test Loss: 1.1073, Test Accuracy: 73.11%\n",
      "Epoch [61/100], Training Loss: 0.2269\n",
      "Epoch [61/100], Test Loss: 1.0739, Test Accuracy: 73.70%\n",
      "Epoch [62/100], Training Loss: 0.2152\n",
      "Epoch [62/100], Test Loss: 1.0602, Test Accuracy: 73.73%\n",
      "Epoch [63/100], Training Loss: 0.2097\n",
      "Epoch [63/100], Test Loss: 1.0601, Test Accuracy: 74.00%\n",
      "Epoch [64/100], Training Loss: 0.2052\n",
      "Epoch [64/100], Test Loss: 1.0723, Test Accuracy: 73.80%\n",
      "Epoch [65/100], Training Loss: 0.2061\n",
      "Epoch [65/100], Test Loss: 1.0575, Test Accuracy: 74.16%\n",
      "Epoch [66/100], Training Loss: 0.1958\n",
      "Epoch [66/100], Test Loss: 1.0542, Test Accuracy: 73.74%\n",
      "Epoch [67/100], Training Loss: 0.1971\n",
      "Epoch [67/100], Test Loss: 1.0466, Test Accuracy: 73.94%\n",
      "Epoch [68/100], Training Loss: 0.1973\n",
      "Epoch [68/100], Test Loss: 1.0539, Test Accuracy: 73.98%\n",
      "Epoch [69/100], Training Loss: 0.1945\n",
      "Epoch [69/100], Test Loss: 1.0618, Test Accuracy: 73.88%\n",
      "Epoch [70/100], Training Loss: 0.1911\n",
      "Epoch [70/100], Test Loss: 1.0706, Test Accuracy: 73.84%\n",
      "Epoch [71/100], Training Loss: 0.1898\n",
      "Epoch [71/100], Test Loss: 1.0632, Test Accuracy: 73.80%\n",
      "Epoch [72/100], Training Loss: 0.1908\n",
      "Epoch [72/100], Test Loss: 1.0686, Test Accuracy: 74.17%\n",
      "Epoch [73/100], Training Loss: 0.1882\n",
      "Epoch [73/100], Test Loss: 1.0507, Test Accuracy: 73.80%\n",
      "Epoch [74/100], Training Loss: 0.1855\n",
      "Epoch [74/100], Test Loss: 1.0632, Test Accuracy: 74.15%\n",
      "Epoch [75/100], Training Loss: 0.1845\n",
      "Epoch [75/100], Test Loss: 1.0725, Test Accuracy: 73.60%\n",
      "Epoch [76/100], Training Loss: 0.1812\n",
      "Epoch [76/100], Test Loss: 1.0664, Test Accuracy: 74.40%\n",
      "Epoch [77/100], Training Loss: 0.1813\n",
      "Epoch [77/100], Test Loss: 1.0818, Test Accuracy: 73.95%\n",
      "Epoch [78/100], Training Loss: 0.1791\n",
      "Epoch [78/100], Test Loss: 1.0715, Test Accuracy: 74.18%\n",
      "Epoch [79/100], Training Loss: 0.1801\n",
      "Epoch [79/100], Test Loss: 1.0687, Test Accuracy: 73.98%\n",
      "Epoch [80/100], Training Loss: 0.1754\n",
      "Epoch [80/100], Test Loss: 1.0594, Test Accuracy: 74.10%\n",
      "Epoch [81/100], Training Loss: 0.1768\n",
      "Epoch [81/100], Test Loss: 1.0570, Test Accuracy: 74.40%\n",
      "Epoch [82/100], Training Loss: 0.1750\n",
      "Epoch [82/100], Test Loss: 1.0662, Test Accuracy: 74.17%\n",
      "Epoch [83/100], Training Loss: 0.1753\n",
      "Epoch [83/100], Test Loss: 1.0599, Test Accuracy: 74.19%\n",
      "Epoch [84/100], Training Loss: 0.1724\n",
      "Epoch [84/100], Test Loss: 1.0781, Test Accuracy: 73.79%\n",
      "Epoch [85/100], Training Loss: 0.1718\n",
      "Epoch [85/100], Test Loss: 1.0865, Test Accuracy: 73.61%\n",
      "Epoch [86/100], Training Loss: 0.1723\n",
      "Epoch [86/100], Test Loss: 1.0742, Test Accuracy: 73.92%\n",
      "Epoch [87/100], Training Loss: 0.1695\n",
      "Epoch [87/100], Test Loss: 1.0730, Test Accuracy: 74.14%\n",
      "Epoch [88/100], Training Loss: 0.1692\n",
      "Epoch [88/100], Test Loss: 1.0732, Test Accuracy: 74.02%\n",
      "Epoch [89/100], Training Loss: 0.1689\n",
      "Epoch [89/100], Test Loss: 1.0659, Test Accuracy: 74.36%\n",
      "Epoch [90/100], Training Loss: 0.1652\n",
      "Epoch [90/100], Test Loss: 1.0678, Test Accuracy: 74.29%\n",
      "Epoch [91/100], Training Loss: 0.1630\n",
      "Epoch [91/100], Test Loss: 1.0813, Test Accuracy: 74.13%\n",
      "Epoch [92/100], Training Loss: 0.1623\n",
      "Epoch [92/100], Test Loss: 1.0654, Test Accuracy: 74.18%\n",
      "Epoch [93/100], Training Loss: 0.1620\n",
      "Epoch [93/100], Test Loss: 1.0724, Test Accuracy: 74.16%\n",
      "Epoch [94/100], Training Loss: 0.1618\n",
      "Epoch [94/100], Test Loss: 1.0488, Test Accuracy: 74.18%\n",
      "Epoch [95/100], Training Loss: 0.1606\n",
      "Epoch [95/100], Test Loss: 1.0753, Test Accuracy: 73.48%\n",
      "Epoch [96/100], Training Loss: 0.1590\n",
      "Epoch [96/100], Test Loss: 1.0755, Test Accuracy: 73.95%\n",
      "Epoch [97/100], Training Loss: 0.1600\n",
      "Epoch [97/100], Test Loss: 1.0719, Test Accuracy: 73.90%\n",
      "Epoch [98/100], Training Loss: 0.1638\n",
      "Epoch [98/100], Test Loss: 1.0774, Test Accuracy: 74.31%\n",
      "Epoch [99/100], Training Loss: 0.1643\n",
      "Epoch [99/100], Test Loss: 1.0656, Test Accuracy: 74.12%\n",
      "Epoch [100/100], Training Loss: 0.1627\n",
      "Epoch [100/100], Test Loss: 1.0778, Test Accuracy: 73.96%\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# Define the WideResNet model (simplified)\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, stride, drop_rate=0.0):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
    "        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.drop_rate = drop_rate\n",
    "        self.equal_in_out = (in_planes == out_planes)\n",
    "        self.shortcut = (not self.equal_in_out) and nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, padding=0, bias=False) or None\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.equal_in_out:\n",
    "            x = F.relu(self.bn1(x), inplace=True)\n",
    "            out = self.conv1(x)\n",
    "        else:\n",
    "            out = F.relu(self.bn1(x), inplace=True)\n",
    "            out = self.conv1(out)\n",
    "        out = F.relu(self.bn2(out), inplace=True)\n",
    "        if self.drop_rate > 0:\n",
    "            out = F.dropout(out, p=self.drop_rate, training=self.training)\n",
    "        out = self.conv2(out)\n",
    "        return torch.add(x if self.equal_in_out else self.shortcut(x), out)\n",
    "\n",
    "class NetworkBlock(nn.Module):\n",
    "    def __init__(self, nb_layers, in_planes, out_planes, block, stride, drop_rate=0.0):\n",
    "        super(NetworkBlock, self).__init__()\n",
    "        self.layer = self._make_layer(block, in_planes, out_planes, nb_layers, stride, drop_rate)\n",
    "\n",
    "    def _make_layer(self, block, in_planes, out_planes, nb_layers, stride, drop_rate):\n",
    "        layers = []\n",
    "        for i in range(nb_layers):\n",
    "            layers.append(block(i == 0 and in_planes or out_planes, out_planes, i == 0 and stride or 1, drop_rate))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "class WideResNet(nn.Module):\n",
    "    def __init__(self, depth, num_classes, widen_factor=10, drop_rate=0.0):\n",
    "        super(WideResNet, self).__init__()\n",
    "        nChannels = [16, 16 * widen_factor, 32 * widen_factor, 64 * widen_factor]\n",
    "        assert ((depth - 4) % 6 == 0)\n",
    "        n = (depth - 4) // 6\n",
    "        block = BasicBlock\n",
    "        self.conv1 = nn.Conv2d(3, nChannels[0], kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.block1 = NetworkBlock(n, nChannels[0], nChannels[1], block, 1, drop_rate)\n",
    "        self.block2 = NetworkBlock(n, nChannels[1], nChannels[2], block, 2, drop_rate)\n",
    "        self.block3 = NetworkBlock(n, nChannels[2], nChannels[3], block, 2, drop_rate)\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels[3])\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc = nn.Linear(nChannels[3], num_classes)\n",
    "        self.nChannels = nChannels[3]\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.block1(out)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "        out = F.relu(self.bn1(out), inplace=True)\n",
    "        out = F.avg_pool2d(out, 8)\n",
    "        out = out.view(-1, self.nChannels)\n",
    "        return self.fc(out)\n",
    "\n",
    "# CIFAR-100 dataset\n",
    "def get_cifar100_loaders(batch_size=128):\n",
    "    transform = transforms.Compose([transforms.RandomCrop(32, padding=4),\n",
    "                                    transforms.RandomHorizontalFlip(),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.507, 0.486, 0.441), (0.267, 0.256, 0.276))])\n",
    "\n",
    "    train_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    test_dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "# Function to calculate test loss and accuracy\n",
    "def evaluate_model(model, test_loader, criterion):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "    avg_loss = test_loss / total\n",
    "    accuracy = 100. * correct / total\n",
    "\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Training with multiple parameter groups\n",
    "def train_wideresnet_28_10():\n",
    "\n",
    "    # Initialize model, loss, and optimizer\n",
    "    model = WideResNet(depth=28, num_classes=100, widen_factor=10, drop_rate=0.3)\n",
    "    model = model.cuda()\n",
    "\n",
    "    # Define optimizer with multiple parameter groups\n",
    "    optimizer = optim.Adam([\n",
    "        {'params': model.conv1.parameters(), 'lr': 0.001, 'weight_decay': 1e-4},  # Learning rate for initial conv layer\n",
    "        {'params': model.block1.parameters(), 'lr': 0.001, 'weight_decay': 1e-4}, # Learning rate for 1st block\n",
    "        {'params': model.block2.parameters(), 'lr': 0.001, 'weight_decay': 1e-4}, # Learning rate for 2nd block\n",
    "        {'params': model.block3.parameters(), 'lr': 0.001, 'weight_decay': 1e-4}, # Learning rate for 3rd block\n",
    "        {'params': model.fc.parameters(), 'lr': 0.01, 'weight_decay': 1e-3}  # Separate LR for fully connected\n",
    "    ])\n",
    "\n",
    "    # Scheduler to reduce LR every 30 epochs\n",
    "    scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 100\n",
    "    train_loader, test_loader = get_cifar100_loaders()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {running_loss / len(train_loader):.4f}')\n",
    "\n",
    "        # Evaluate the model on test data after each epoch\n",
    "        test_loss, test_accuracy = evaluate_model(model, test_loader, criterion)\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "        # Adjust learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_wideresnet_28_10()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 22324.010607,
   "end_time": "2024-10-29T09:56:35.080325",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-29T03:44:31.069718",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
