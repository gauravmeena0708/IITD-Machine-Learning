{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0nnjE9Nqu2D",
        "outputId": "b0e00c56-3a87-405e-e7ba-b9070144ba6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch [1/100], Training Loss: 4.3176\n",
            "Epoch [1/100], Test Loss: 4.0591, Test Accuracy: 6.80%\n",
            "Epoch [2/100], Training Loss: 3.8763\n",
            "Epoch [2/100], Test Loss: 3.8001, Test Accuracy: 9.60%\n",
            "Epoch [3/100], Training Loss: 3.5818\n",
            "Epoch [3/100], Test Loss: 3.4422, Test Accuracy: 16.06%\n",
            "Epoch [4/100], Training Loss: 3.2161\n",
            "Epoch [4/100], Test Loss: 3.0592, Test Accuracy: 22.22%\n",
            "Epoch [5/100], Training Loss: 2.8770\n",
            "Epoch [5/100], Test Loss: 2.7916, Test Accuracy: 27.21%\n",
            "Epoch [6/100], Training Loss: 2.6094\n",
            "Epoch [6/100], Test Loss: 2.6047, Test Accuracy: 32.65%\n",
            "Epoch [7/100], Training Loss: 2.3971\n",
            "Epoch [7/100], Test Loss: 2.6177, Test Accuracy: 33.53%\n",
            "Epoch [8/100], Training Loss: 2.2058\n",
            "Epoch [8/100], Test Loss: 2.5009, Test Accuracy: 36.54%\n",
            "Epoch [9/100], Training Loss: 2.1568\n",
            "Epoch [9/100], Test Loss: 2.0443, Test Accuracy: 44.43%\n",
            "Epoch [10/100], Training Loss: 1.9595\n",
            "Epoch [10/100], Test Loss: 2.0438, Test Accuracy: 46.01%\n",
            "Epoch [11/100], Training Loss: 1.8144\n",
            "Epoch [11/100], Test Loss: 1.8792, Test Accuracy: 48.42%\n",
            "Epoch [12/100], Training Loss: 1.7262\n",
            "Epoch [12/100], Test Loss: 1.8762, Test Accuracy: 49.42%\n",
            "Epoch [13/100], Training Loss: 1.6465\n",
            "Epoch [13/100], Test Loss: 1.7239, Test Accuracy: 52.50%\n",
            "Epoch [14/100], Training Loss: 1.5696\n",
            "Epoch [14/100], Test Loss: 1.7318, Test Accuracy: 52.64%\n",
            "Epoch [15/100], Training Loss: 1.5074\n",
            "Epoch [15/100], Test Loss: 1.6171, Test Accuracy: 55.30%\n",
            "Epoch [16/100], Training Loss: 1.4558\n",
            "Epoch [16/100], Test Loss: 1.6602, Test Accuracy: 54.25%\n",
            "Epoch [17/100], Training Loss: 1.4282\n",
            "Epoch [17/100], Test Loss: 1.4677, Test Accuracy: 58.80%\n",
            "Epoch [18/100], Training Loss: 1.3535\n",
            "Epoch [18/100], Test Loss: 1.4875, Test Accuracy: 58.93%\n",
            "Epoch [19/100], Training Loss: 1.3121\n",
            "Epoch [19/100], Test Loss: 1.4655, Test Accuracy: 59.33%\n",
            "Epoch [20/100], Training Loss: 1.2793\n",
            "Epoch [20/100], Test Loss: 1.4168, Test Accuracy: 60.05%\n",
            "Epoch [21/100], Training Loss: 1.2381\n",
            "Epoch [21/100], Test Loss: 1.3850, Test Accuracy: 61.11%\n",
            "Epoch [22/100], Training Loss: 1.2226\n",
            "Epoch [22/100], Test Loss: 1.4442, Test Accuracy: 60.94%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "# Define the WideResNet model (simplified)\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, stride, drop_rate=0.0):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
        "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
        "        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.drop_rate = drop_rate\n",
        "        self.equal_in_out = (in_planes == out_planes)\n",
        "        self.shortcut = (not self.equal_in_out) and nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, padding=0, bias=False) or None\n",
        "\n",
        "    def forward(self, x):\n",
        "        if not self.equal_in_out:\n",
        "            x = F.relu(self.bn1(x), inplace=True)\n",
        "            out = self.conv1(x)\n",
        "        else:\n",
        "            out = F.relu(self.bn1(x), inplace=True)\n",
        "            out = self.conv1(out)\n",
        "        out = F.relu(self.bn2(out), inplace=True)\n",
        "        if self.drop_rate > 0:\n",
        "            out = F.dropout(out, p=self.drop_rate, training=self.training)\n",
        "        out = self.conv2(out)\n",
        "        return torch.add(x if self.equal_in_out else self.shortcut(x), out)\n",
        "\n",
        "class NetworkBlock(nn.Module):\n",
        "    def __init__(self, nb_layers, in_planes, out_planes, block, stride, drop_rate=0.0):\n",
        "        super(NetworkBlock, self).__init__()\n",
        "        self.layer = self._make_layer(block, in_planes, out_planes, nb_layers, stride, drop_rate)\n",
        "\n",
        "    def _make_layer(self, block, in_planes, out_planes, nb_layers, stride, drop_rate):\n",
        "        layers = []\n",
        "        for i in range(nb_layers):\n",
        "            layers.append(block(i == 0 and in_planes or out_planes, out_planes, i == 0 and stride or 1, drop_rate))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layer(x)\n",
        "\n",
        "class WideResNet(nn.Module):\n",
        "    def __init__(self, depth, num_classes, widen_factor=10, drop_rate=0.0):\n",
        "        super(WideResNet, self).__init__()\n",
        "        nChannels = [16, 16 * widen_factor, 32 * widen_factor, 64 * widen_factor]\n",
        "        assert ((depth - 4) % 6 == 0)\n",
        "        n = (depth - 4) // 6\n",
        "        block = BasicBlock\n",
        "        self.conv1 = nn.Conv2d(3, nChannels[0], kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.block1 = NetworkBlock(n, nChannels[0], nChannels[1], block, 1, drop_rate)\n",
        "        self.block2 = NetworkBlock(n, nChannels[1], nChannels[2], block, 2, drop_rate)\n",
        "        self.block3 = NetworkBlock(n, nChannels[2], nChannels[3], block, 2, drop_rate)\n",
        "        self.bn1 = nn.BatchNorm2d(nChannels[3])\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.fc = nn.Linear(nChannels[3], num_classes)\n",
        "        self.nChannels = nChannels[3]\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.block1(out)\n",
        "        out = self.block2(out)\n",
        "        out = self.block3(out)\n",
        "        out = F.relu(self.bn1(out), inplace=True)\n",
        "        out = F.avg_pool2d(out, 8)\n",
        "        out = out.view(-1, self.nChannels)\n",
        "        return self.fc(out)\n",
        "\n",
        "# CIFAR-100 dataset\n",
        "def get_cifar100_loaders(batch_size=128):\n",
        "    transform = transforms.Compose([transforms.RandomCrop(32, padding=4),\n",
        "                                    transforms.RandomHorizontalFlip(),\n",
        "                                    transforms.ToTensor(),\n",
        "                                    transforms.Normalize((0.507, 0.486, 0.441), (0.267, 0.256, 0.276))])\n",
        "\n",
        "    train_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    test_dataset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, test_loader\n",
        "\n",
        "# Function to calculate test loss and accuracy\n",
        "def evaluate_model(model, test_loader, criterion):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    test_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            test_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    avg_loss = test_loss / total\n",
        "    accuracy = 100. * correct / total\n",
        "\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# Training with multiple parameter groups\n",
        "def train_wideresnet_28_10():\n",
        "\n",
        "    # Initialize model, loss, and optimizer\n",
        "    model = WideResNet(depth=28, num_classes=100, widen_factor=10, drop_rate=0.3)\n",
        "    model = model.cuda()\n",
        "\n",
        "    # Define optimizer with multiple parameter groups\n",
        "    optimizer = optim.Adam([\n",
        "        {'params': model.conv1.parameters(), 'lr': 0.001, 'weight_decay': 1e-4},  # Learning rate for initial conv layer\n",
        "        {'params': model.block1.parameters(), 'lr': 0.001, 'weight_decay': 1e-4}, # Learning rate for 1st block\n",
        "        {'params': model.block2.parameters(), 'lr': 0.001, 'weight_decay': 1e-4}, # Learning rate for 2nd block\n",
        "        {'params': model.block3.parameters(), 'lr': 0.001, 'weight_decay': 1e-4}, # Learning rate for 3rd block\n",
        "        {'params': model.fc.parameters(), 'lr': 0.01, 'weight_decay': 1e-3}  # Separate LR for fully connected\n",
        "    ])\n",
        "\n",
        "    # Scheduler to reduce LR every 30 epochs\n",
        "    scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "\n",
        "    # Training loop\n",
        "    num_epochs = 100\n",
        "    train_loader, test_loader = get_cifar100_loaders()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for i, (inputs, targets) in enumerate(train_loader):\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {running_loss / len(train_loader):.4f}')\n",
        "\n",
        "        # Evaluate the model on test data after each epoch\n",
        "        test_loss, test_accuracy = evaluate_model(model, test_loader, criterion)\n",
        "        print(f'Epoch [{epoch + 1}/{num_epochs}], Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
        "\n",
        "        # Adjust learning rate\n",
        "        scheduler.step()\n",
        "\n",
        "    print('Finished Training')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train_wideresnet_28_10()\n"
      ]
    }
  ]
}