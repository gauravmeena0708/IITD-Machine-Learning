{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ee36e5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T06:46:04.014815Z",
     "iopub.status.busy": "2024-10-20T06:46:04.013955Z",
     "iopub.status.idle": "2024-10-20T06:46:10.911237Z",
     "shell.execute_reply": "2024-10-20T06:46:10.909938Z"
    },
    "papermill": {
     "duration": 6.90438,
     "end_time": "2024-10-20T06:46:10.913946",
     "exception": false,
     "start_time": "2024-10-20T06:46:04.009566",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp -r /kaggle/input/col774a3/* /kaggle/working/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da671624",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-10-20T06:46:10.921288Z",
     "iopub.status.busy": "2024-10-20T06:46:10.920912Z",
     "iopub.status.idle": "2024-10-20T09:02:44.245878Z",
     "shell.execute_reply": "2024-10-20T09:02:44.244756Z"
    },
    "papermill": {
     "duration": 8193.331662,
     "end_time": "2024-10-20T09:02:44.248338",
     "exception": false,
     "start_time": "2024-10-20T06:46:10.916676",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169001437/169001437 [00:02<00:00, 58932004.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
      "Epoch 0: Loss: 4.509624347662377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for epoch 0 saved to CSV.\n",
      "Test set: Accuracy: 0/20000 (0.00%) | Skipped: 20000\n",
      "Epoch 1: Loss: 3.765388569868434\n",
      "Predictions for epoch 1 saved to CSV.\n",
      "Test set: Accuracy: 0/20000 (0.00%) | Skipped: 19960\n",
      "Epoch 2: Loss: 3.334648060981575\n",
      "Predictions for epoch 2 saved to CSV.\n",
      "Test set: Accuracy: 0/20000 (0.00%) | Skipped: 19901\n",
      "Epoch 3: Loss: 2.9378749690092434\n",
      "Predictions for epoch 3 saved to CSV.\n",
      "Test set: Accuracy: 0/20000 (0.00%) | Skipped: 19748\n",
      "Epoch 4: Loss: 2.5560292014685433\n",
      "Predictions for epoch 4 saved to CSV.\n",
      "Test set: Accuracy: 0/20000 (0.00%) | Skipped: 18959\n",
      "Epoch 5: Loss: 2.260148223708658\n",
      "Predictions for epoch 5 saved to CSV.\n",
      "Test set: Accuracy: 0/20000 (0.00%) | Skipped: 18498\n",
      "Epoch 6: Loss: 2.051713596829368\n",
      "Predictions for epoch 6 saved to CSV.\n",
      "Test set: Accuracy: 0/20000 (0.00%) | Skipped: 18259\n",
      "Epoch 7: Loss: 1.9029012674565815\n",
      "Predictions for epoch 7 saved to CSV.\n",
      "Test set: Accuracy: 0/20000 (0.00%) | Skipped: 17238\n",
      "Epoch 8: Loss: 1.7950184387928994\n",
      "Predictions for epoch 8 saved to CSV.\n",
      "Test set: Accuracy: 0/20000 (0.00%) | Skipped: 17499\n",
      "Epoch 9: Loss: 1.7168033308995045\n",
      "Predictions for epoch 9 saved to CSV.\n",
      "Test set: Accuracy: 0/20000 (0.00%) | Skipped: 16809\n",
      "Epoch 10: Loss: 1.6484313755084181\n",
      "Predictions for epoch 10 saved to CSV.\n",
      "Test set: Accuracy: 0/20000 (0.00%) | Skipped: 16635\n",
      "Epoch 11: Loss: 1.5999900910555553\n",
      "Predictions for epoch 11 saved to CSV.\n",
      "Test set: Accuracy: 0/20000 (0.00%) | Skipped: 16986\n",
      "Epoch 12: Loss: 1.5657735620923054\n",
      "Predictions for epoch 12 saved to CSV.\n",
      "Test set: Accuracy: 1/20000 (0.01%) | Skipped: 16350\n",
      "Epoch 13: Loss: 1.5213920798752925\n",
      "Predictions for epoch 13 saved to CSV.\n",
      "Test set: Accuracy: 0/20000 (0.00%) | Skipped: 16638\n",
      "Epoch 14: Loss: 1.49702655628819\n",
      "Predictions for epoch 14 saved to CSV.\n",
      "Test set: Accuracy: 0/20000 (0.00%) | Skipped: 16335\n",
      "Epoch 15: Loss: 1.4671223428853029\n",
      "Predictions for epoch 15 saved to CSV.\n",
      "Test set: Accuracy: 0/20000 (0.00%) | Skipped: 15863\n",
      "Epoch 16: Loss: 1.453586110068709\n",
      "Predictions for epoch 16 saved to CSV.\n",
      "Test set: Accuracy: 0/20000 (0.00%) | Skipped: 16814\n",
      "Epoch 17: Loss: 1.42967136924529\n",
      "Predictions for epoch 17 saved to CSV.\n",
      "Test set: Accuracy: 1/20000 (0.01%) | Skipped: 15849\n",
      "Epoch 18: Loss: 1.407716204138363\n",
      "Predictions for epoch 18 saved to CSV.\n",
      "Test set: Accuracy: 0/20000 (0.00%) | Skipped: 16046\n",
      "Epoch 19: Loss: 1.3882616755297728\n",
      "Predictions for epoch 19 saved to CSV.\n",
      "Test set: Accuracy: 0/20000 (0.00%) | Skipped: 15558\n",
      "Epoch 20: Loss: 1.378613858881509\n",
      "Predictions for epoch 20 saved to CSV.\n",
      "Test set: Accuracy: 1/20000 (0.01%) | Skipped: 15855\n",
      "Epoch 21: Loss: 1.3633157350218204\n",
      "Predictions for epoch 21 saved to CSV.\n",
      "Test set: Accuracy: 0/20000 (0.00%) | Skipped: 15571\n",
      "Epoch 22: Loss: 1.3551494578266388\n",
      "Predictions for epoch 22 saved to CSV.\n",
      "Test set: Accuracy: 0/20000 (0.00%) | Skipped: 15305\n",
      "Epoch 23: Loss: 1.3316083359901252\n",
      "Predictions for epoch 23 saved to CSV.\n",
      "Test set: Accuracy: 1/20000 (0.01%) | Skipped: 15410\n",
      "Epoch 24: Loss: 1.3320150000360005\n",
      "Predictions for epoch 24 saved to CSV.\n",
      "Test set: Accuracy: 0/20000 (0.00%) | Skipped: 15320\n",
      "Epoch 25: Loss: 1.3224094790570877\n",
      "Predictions for epoch 25 saved to CSV.\n",
      "Test set: Accuracy: 0/20000 (0.00%) | Skipped: 14934\n",
      "Epoch 26: Loss: 1.2960911955674896\n",
      "Predictions for epoch 26 saved to CSV.\n",
      "Test set: Accuracy: 0/20000 (0.00%) | Skipped: 15182\n",
      "Epoch 27: Loss: 1.3061825396764615\n",
      "Predictions for epoch 27 saved to CSV.\n",
      "Test set: Accuracy: 1/20000 (0.01%) | Skipped: 14791\n",
      "Epoch 28: Loss: 1.2890329080469467\n",
      "Predictions for epoch 28 saved to CSV.\n",
      "Test set: Accuracy: 0/20000 (0.00%) | Skipped: 14917\n",
      "Epoch 29: Loss: 1.2741120610090777\n",
      "Predictions for epoch 29 saved to CSV.\n",
      "Test set: Accuracy: 0/20000 (0.00%) | Skipped: 15114\n",
      "Epoch 30: Loss: 0.8639267382719328\n",
      "Predictions for epoch 30 saved to CSV.\n",
      "Test set: Accuracy: 1/20000 (0.01%) | Skipped: 11225\n",
      "Epoch 31: Loss: 0.7186007650611955\n",
      "Predictions for epoch 31 saved to CSV.\n",
      "Test set: Accuracy: 1/20000 (0.01%) | Skipped: 10464\n",
      "Epoch 32: Loss: 0.6615830649195424\n",
      "Predictions for epoch 32 saved to CSV.\n",
      "Test set: Accuracy: 1/20000 (0.01%) | Skipped: 10155\n",
      "Epoch 33: Loss: 0.6171328667789469\n",
      "Predictions for epoch 33 saved to CSV.\n",
      "Test set: Accuracy: 1/20000 (0.01%) | Skipped: 9761\n",
      "Epoch 34: Loss: 0.5807040732382508\n",
      "Predictions for epoch 34 saved to CSV.\n",
      "Test set: Accuracy: 1/20000 (0.01%) | Skipped: 9548\n",
      "Epoch 35: Loss: 0.5448961318911189\n",
      "Predictions for epoch 35 saved to CSV.\n",
      "Test set: Accuracy: 1/20000 (0.01%) | Skipped: 9173\n",
      "Epoch 36: Loss: 0.5188378557524718\n",
      "Predictions for epoch 36 saved to CSV.\n",
      "Test set: Accuracy: 1/20000 (0.01%) | Skipped: 8826\n",
      "Epoch 37: Loss: 0.490483522110278\n",
      "Predictions for epoch 37 saved to CSV.\n",
      "Test set: Accuracy: 1/20000 (0.01%) | Skipped: 8781\n",
      "Epoch 38: Loss: 0.46493610518667705\n",
      "Predictions for epoch 38 saved to CSV.\n",
      "Test set: Accuracy: 1/20000 (0.01%) | Skipped: 8546\n",
      "Epoch 39: Loss: 0.4488852936441026\n",
      "Predictions for epoch 39 saved to CSV.\n",
      "Test set: Accuracy: 1/20000 (0.01%) | Skipped: 8472\n",
      "Epoch 40: Loss: 0.4318066275561862\n",
      "Predictions for epoch 40 saved to CSV.\n",
      "Test set: Accuracy: 0/20000 (0.00%) | Skipped: 8455\n",
      "Epoch 41: Loss: 0.41798680342371813\n",
      "Predictions for epoch 41 saved to CSV.\n",
      "Test set: Accuracy: 0/20000 (0.00%) | Skipped: 8079\n",
      "Epoch 42: Loss: 0.4033541342486506\n",
      "Predictions for epoch 42 saved to CSV.\n",
      "Test set: Accuracy: 1/20000 (0.01%) | Skipped: 8108\n",
      "Epoch 43: Loss: 0.3899367131921641\n",
      "Predictions for epoch 43 saved to CSV.\n",
      "Test set: Accuracy: 1/20000 (0.01%) | Skipped: 8002\n",
      "Epoch 44: Loss: 0.3773975942826942\n",
      "Predictions for epoch 44 saved to CSV.\n",
      "Test set: Accuracy: 1/20000 (0.01%) | Skipped: 7873\n",
      "Epoch 45: Loss: 0.3672862352084016\n",
      "Predictions for epoch 45 saved to CSV.\n",
      "Test set: Accuracy: 1/20000 (0.01%) | Skipped: 7564\n",
      "Epoch 46: Loss: 0.35839723313556\n",
      "Predictions for epoch 46 saved to CSV.\n",
      "Test set: Accuracy: 1/20000 (0.01%) | Skipped: 7544\n",
      "Epoch 47: Loss: 0.3529088906848522\n",
      "Predictions for epoch 47 saved to CSV.\n",
      "Test set: Accuracy: 1/20000 (0.01%) | Skipped: 7630\n",
      "Epoch 48: Loss: 0.3466283811251526\n",
      "Predictions for epoch 48 saved to CSV.\n",
      "Test set: Accuracy: 1/20000 (0.01%) | Skipped: 7617\n",
      "Epoch 49: Loss: 0.34455968249027075\n",
      "Predictions for epoch 49 saved to CSV.\n",
      "Test set: Accuracy: 1/20000 (0.01%) | Skipped: 7608\n",
      "Epoch 50: Loss: 0.3423857049030416\n",
      "Predictions for epoch 50 saved to CSV.\n",
      "Test set: Accuracy: 1/20000 (0.01%) | Skipped: 7382\n",
      "Epoch 51: Loss: 0.33484307785168327\n",
      "Predictions for epoch 51 saved to CSV.\n",
      "Test set: Accuracy: 1/20000 (0.01%) | Skipped: 7604\n",
      "Epoch 52: Loss: 0.3265795297634876\n",
      "Predictions for epoch 52 saved to CSV.\n",
      "Test set: Accuracy: 1/20000 (0.01%) | Skipped: 7276\n",
      "Epoch 53: Loss: 0.3259423402188074\n",
      "Predictions for epoch 53 saved to CSV.\n",
      "Test set: Accuracy: 1/20000 (0.01%) | Skipped: 7488\n",
      "Epoch 54: Loss: 0.3250328239501285\n",
      "Predictions for epoch 54 saved to CSV.\n",
      "Test set: Accuracy: 1/20000 (0.01%) | Skipped: 7271\n",
      "Epoch 55: Loss: 0.31930161959222514\n",
      "Predictions for epoch 55 saved to CSV.\n",
      "Test set: Accuracy: 1/20000 (0.01%) | Skipped: 7199\n",
      "Epoch 56: Loss: 0.3112583036541634\n",
      "Predictions for epoch 56 saved to CSV.\n",
      "Test set: Accuracy: 1/20000 (0.01%) | Skipped: 7400\n",
      "Epoch 57: Loss: 0.3135000213104136\n",
      "Predictions for epoch 57 saved to CSV.\n",
      "Test set: Accuracy: 2/20000 (0.01%) | Skipped: 7092\n",
      "Epoch 58: Loss: 0.3139904813693308\n",
      "Predictions for epoch 58 saved to CSV.\n",
      "Test set: Accuracy: 1/20000 (0.01%) | Skipped: 7412\n",
      "Epoch 59: Loss: 0.3064896771136452\n",
      "Predictions for epoch 59 saved to CSV.\n",
      "Test set: Accuracy: 1/20000 (0.01%) | Skipped: 7150\n",
      "Epoch 60: Loss: 0.18755809896056305\n",
      "Predictions for epoch 60 saved to CSV.\n",
      "Test set: Accuracy: 1/20000 (0.01%) | Skipped: 5425\n",
      "Epoch 61: Loss: 0.1364384117295675\n",
      "Predictions for epoch 61 saved to CSV.\n",
      "Test set: Accuracy: 1/20000 (0.01%) | Skipped: 5137\n",
      "Epoch 62: Loss: 0.11279453157120958\n",
      "Predictions for epoch 62 saved to CSV.\n",
      "Test set: Accuracy: 1/20000 (0.01%) | Skipped: 4927\n",
      "Epoch 63: Loss: 0.10308878913597988\n",
      "Predictions for epoch 63 saved to CSV.\n",
      "Test set: Accuracy: 1/20000 (0.01%) | Skipped: 4775\n",
      "Epoch 64: Loss: 0.09343652219494895\n",
      "Predictions for epoch 64 saved to CSV.\n",
      "Test set: Accuracy: 1/20000 (0.01%) | Skipped: 4622\n",
      "Epoch 65: Loss: 0.0898634721346371\n",
      "Predictions for epoch 65 saved to CSV.\n",
      "Test set: Accuracy: 1/20000 (0.01%) | Skipped: 4507\n",
      "Epoch 66: Loss: 0.08288221529034702\n",
      "Predictions for epoch 66 saved to CSV.\n",
      "Test set: Accuracy: 1/20000 (0.01%) | Skipped: 4465\n",
      "Epoch 67: Loss: 0.07803977496178864\n",
      "Predictions for epoch 67 saved to CSV.\n",
      "Test set: Accuracy: 1/20000 (0.01%) | Skipped: 4362\n",
      "Epoch 68: Loss: 0.0742615456490413\n",
      "Predictions for epoch 68 saved to CSV.\n",
      "Test set: Accuracy: 1/20000 (0.01%) | Skipped: 4292\n",
      "Epoch 69: Loss: 0.07058044722127488\n",
      "Predictions for epoch 69 saved to CSV.\n",
      "Test set: Accuracy: 1/20000 (0.01%) | Skipped: 4188\n",
      "Epoch 70: Loss: 0.06840353785439983\n",
      "Predictions for epoch 70 saved to CSV.\n",
      "Test set: Accuracy: 1/20000 (0.01%) | Skipped: 4171\n",
      "Epoch 71: Loss: 0.06521426519031262\n",
      "Predictions for epoch 71 saved to CSV.\n",
      "Test set: Accuracy: 1/20000 (0.01%) | Skipped: 4160\n",
      "Epoch 72: Loss: 0.06246749053011313\n",
      "Predictions for epoch 72 saved to CSV.\n",
      "Test set: Accuracy: 2/20000 (0.01%) | Skipped: 4100\n",
      "Epoch 73: Loss: 0.0596658912299158\n",
      "Predictions for epoch 73 saved to CSV.\n",
      "Test set: Accuracy: 1/20000 (0.01%) | Skipped: 4038\n",
      "Epoch 74: Loss: 0.05864801212116275\n",
      "Predictions for epoch 74 saved to CSV.\n",
      "Test set: Accuracy: 2/20000 (0.01%) | Skipped: 3934\n",
      "Epoch 75: Loss: 0.05616870150922814\n",
      "Predictions for epoch 75 saved to CSV.\n",
      "Test set: Accuracy: 2/20000 (0.01%) | Skipped: 3959\n",
      "Epoch 76: Loss: 0.053617905706281555\n",
      "Predictions for epoch 76 saved to CSV.\n",
      "Test set: Accuracy: 2/20000 (0.01%) | Skipped: 3847\n",
      "Epoch 77: Loss: 0.05253192572794912\n",
      "Predictions for epoch 77 saved to CSV.\n",
      "Test set: Accuracy: 1/20000 (0.01%) | Skipped: 3863\n",
      "Epoch 78: Loss: 0.050966162710924585\n",
      "Predictions for epoch 78 saved to CSV.\n",
      "Test set: Accuracy: 2/20000 (0.01%) | Skipped: 3810\n",
      "Epoch 79: Loss: 0.05001104424428909\n",
      "Predictions for epoch 79 saved to CSV.\n",
      "Test set: Accuracy: 2/20000 (0.01%) | Skipped: 3789\n",
      "Epoch 80: Loss: 0.04848948931392959\n",
      "Predictions for epoch 80 saved to CSV.\n",
      "Test set: Accuracy: 2/20000 (0.01%) | Skipped: 3730\n",
      "Epoch 81: Loss: 0.046828313204257385\n",
      "Predictions for epoch 81 saved to CSV.\n",
      "Test set: Accuracy: 2/20000 (0.01%) | Skipped: 3712\n",
      "Epoch 82: Loss: 0.04580859855160384\n",
      "Predictions for epoch 82 saved to CSV.\n",
      "Test set: Accuracy: 2/20000 (0.01%) | Skipped: 3642\n",
      "Epoch 83: Loss: 0.0438854079264814\n",
      "Predictions for epoch 83 saved to CSV.\n",
      "Test set: Accuracy: 2/20000 (0.01%) | Skipped: 3645\n",
      "Epoch 84: Loss: 0.04234493500493524\n",
      "Predictions for epoch 84 saved to CSV.\n",
      "Test set: Accuracy: 2/20000 (0.01%) | Skipped: 3601\n",
      "Epoch 85: Loss: 0.04269142394594829\n",
      "Predictions for epoch 85 saved to CSV.\n",
      "Test set: Accuracy: 2/20000 (0.01%) | Skipped: 3662\n",
      "Epoch 86: Loss: 0.042033831029177626\n",
      "Predictions for epoch 86 saved to CSV.\n",
      "Test set: Accuracy: 2/20000 (0.01%) | Skipped: 3588\n",
      "Epoch 87: Loss: 0.04006210122914875\n",
      "Predictions for epoch 87 saved to CSV.\n",
      "Test set: Accuracy: 2/20000 (0.01%) | Skipped: 3521\n",
      "Epoch 88: Loss: 0.039030620363800575\n",
      "Predictions for epoch 88 saved to CSV.\n",
      "Test set: Accuracy: 2/20000 (0.01%) | Skipped: 3579\n",
      "Epoch 89: Loss: 0.038630914057383454\n",
      "Predictions for epoch 89 saved to CSV.\n",
      "Test set: Accuracy: 2/20000 (0.01%) | Skipped: 3541\n",
      "Epoch 90: Loss: 0.03573650859601205\n",
      "Predictions for epoch 90 saved to CSV.\n",
      "Test set: Accuracy: 2/20000 (0.01%) | Skipped: 3523\n",
      "Epoch 91: Loss: 0.03614762827249058\n",
      "Predictions for epoch 91 saved to CSV.\n",
      "Test set: Accuracy: 2/20000 (0.01%) | Skipped: 3542\n",
      "Epoch 92: Loss: 0.03581524336629588\n",
      "Predictions for epoch 92 saved to CSV.\n",
      "Test set: Accuracy: 2/20000 (0.01%) | Skipped: 3548\n",
      "Epoch 93: Loss: 0.03547884341415085\n",
      "Predictions for epoch 93 saved to CSV.\n",
      "Test set: Accuracy: 2/20000 (0.01%) | Skipped: 3532\n",
      "Epoch 94: Loss: 0.035853146842164\n",
      "Predictions for epoch 94 saved to CSV.\n",
      "Test set: Accuracy: 2/20000 (0.01%) | Skipped: 3518\n",
      "Epoch 95: Loss: 0.03520177516733746\n",
      "Predictions for epoch 95 saved to CSV.\n",
      "Test set: Accuracy: 2/20000 (0.01%) | Skipped: 3504\n",
      "Epoch 96: Loss: 0.03521888808387777\n",
      "Predictions for epoch 96 saved to CSV.\n",
      "Test set: Accuracy: 2/20000 (0.01%) | Skipped: 3489\n",
      "Epoch 97: Loss: 0.03462380477610756\n",
      "Predictions for epoch 97 saved to CSV.\n",
      "Test set: Accuracy: 2/20000 (0.01%) | Skipped: 3532\n",
      "Epoch 98: Loss: 0.0353665159457861\n",
      "Predictions for epoch 98 saved to CSV.\n",
      "Test set: Accuracy: 2/20000 (0.01%) | Skipped: 3473\n",
      "Epoch 99: Loss: 0.03394845727107028\n",
      "Predictions for epoch 99 saved to CSV.\n",
      "Test set: Accuracy: 2/20000 (0.01%) | Skipped: 3539\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Set default thresholds for each class\n",
    "class_thresholds = {i: 0.9 for i in range(100)}  # Default threshold for each class\n",
    "\n",
    "# Custom function to calculate the score\n",
    "def calculate_score(true_labels: pd.Series, predicted_labels: pd.Series, accuracy_threshold: float = 0.7, gamma: float = 5.0) -> float:\n",
    "    data = pd.DataFrame({'True_label': true_labels, 'Predicted_label': predicted_labels})\n",
    "    filtered_df = data[data['Predicted_label'] != -1]\n",
    "    all_classes = list(range(100))\n",
    "    \n",
    "    sum_of_correctly_classified_high_accuracy = 0\n",
    "    sum_of_correctly_classified_low_accuracy = 0\n",
    "    accuracy_per_class = {}\n",
    "    \n",
    "    grouped = filtered_df.groupby('Predicted_label')\n",
    "    for name, group in grouped:\n",
    "        accuracy = (group['True_label'] == group['Predicted_label']).sum() / len(group)\n",
    "        accuracy_per_class[name] = accuracy\n",
    "    \n",
    "    for cls in all_classes:\n",
    "        total = len(filtered_df[filtered_df['Predicted_label'] == cls])\n",
    "        class_accuracy = accuracy_per_class.get(cls, 0.0)\n",
    "        if class_accuracy >= accuracy_threshold:\n",
    "            sum_of_correctly_classified_high_accuracy += total\n",
    "        else:\n",
    "            sum_of_correctly_classified_low_accuracy += total\n",
    "\n",
    "    final_score = sum_of_correctly_classified_high_accuracy - gamma * sum_of_correctly_classified_low_accuracy\n",
    "    return round(final_score / 2)\n",
    "\n",
    "# Custom function to apply thresholds and return predictions\n",
    "def apply_class_thresholds(probabilities, class_thresholds):\n",
    "    predicted_labels = []\n",
    "    for i in range(probabilities.size(0)):\n",
    "        max_prob, predicted_class = torch.max(probabilities[i], 0)\n",
    "        if max_prob >= class_thresholds[predicted_class.item()]:\n",
    "            predicted_labels.append(predicted_class.item())\n",
    "        else:\n",
    "            predicted_labels.append(-1)\n",
    "    return torch.tensor(predicted_labels)\n",
    "\n",
    "# CIFAR100Dataset for loading data from pkl files\n",
    "class CIFAR100Dataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        with open(file_path, 'rb') as f:\n",
    "            self.data = pickle.load(f)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.data[idx]\n",
    "        return image, label\n",
    "\n",
    "# Test function that loads from test.pkl and applies threshold-based predictions\n",
    "def test_with_thresholds_from_pkl(epoch, model, test_pkl_file, device, class_thresholds):\n",
    "    test_dataset = CIFAR100Dataset(test_pkl_file)  # Load test.pkl\n",
    "    test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    skipped_predictions = 0\n",
    "    \n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    predictions_for_csv = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            probabilities = F.softmax(outputs, dim=1)\n",
    "\n",
    "            # Apply class-specific thresholds\n",
    "            predictions = apply_class_thresholds(probabilities, class_thresholds).to(device)\n",
    "\n",
    "            total += targets.size(0)\n",
    "            correct_predictions = predictions.eq(targets).sum().item()\n",
    "            skipped_predictions += (predictions == -1).sum().item()\n",
    "\n",
    "            correct += correct_predictions\n",
    "            \n",
    "            true_labels.extend(targets.cpu().numpy())\n",
    "            predicted_labels.extend(predictions.cpu().numpy())\n",
    "\n",
    "            # Save predictions for CSV\n",
    "            predictions_for_csv.extend(predictions.cpu().numpy())\n",
    "\n",
    "    # Save predictions and true labels to CSV\n",
    "    prediction_df = pd.DataFrame({\n",
    "        'ID': list(range(len(predictions_for_csv))),\n",
    "        'Predicted_label': predictions_for_csv,\n",
    "        'True_label': true_labels\n",
    "    })\n",
    "    prediction_df.to_csv(f'predictions_epoch_{epoch}.csv', index=False)\n",
    "    print(f'Predictions for epoch {epoch} saved to CSV.')\n",
    "\n",
    "    # Calculate the test accuracy\n",
    "    test_accuracy = 100. * correct / total\n",
    "    print(f'Test set: Accuracy: {correct}/{total} ({test_accuracy:.2f}%) | Skipped: {skipped_predictions}')\n",
    "\n",
    "    return correct, total\n",
    "\n",
    "# WideResNeXt Block (same as before)\n",
    "class WideResNeXtBlock(nn.Module):\n",
    "    expansion = 2\n",
    "    def __init__(self, in_planes, planes, stride=1, cardinality=32, widen_factor=2):\n",
    "        super(WideResNeXtBlock, self).__init__()\n",
    "        D = cardinality * widen_factor\n",
    "        self.conv1 = nn.Conv2d(in_planes, D, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(D)\n",
    "        self.conv2 = nn.Conv2d(D, D, kernel_size=3, stride=stride, padding=1, groups=cardinality, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(D)\n",
    "        self.conv3 = nn.Conv2d(D, planes * WideResNeXtBlock.expansion, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * WideResNeXtBlock.expansion)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes * WideResNeXtBlock.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes * WideResNeXtBlock.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * WideResNeXtBlock.expansion)\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        out = torch.relu(self.bn1(self.conv1(x)))\n",
    "        out = torch.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = torch.relu(out)\n",
    "        return out\n",
    "\n",
    "# WideResNeXt Model (same as before)\n",
    "class WideResNeXt(nn.Module):\n",
    "    def __init__(self, block, num_blocks, cardinality=32, widen_factor=2, num_classes=100):\n",
    "        super(WideResNeXt, self).__init__()\n",
    "        self.in_planes = 64\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1, cardinality=cardinality, widen_factor=widen_factor)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2, cardinality=cardinality, widen_factor=widen_factor)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2, cardinality=cardinality, widen_factor=widen_factor)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2, cardinality=cardinality, widen_factor=widen_factor)\n",
    "        self.linear = nn.Linear(512 * WideResNeXtBlock.expansion, num_classes)\n",
    "    \n",
    "    def _make_layer(self, block, planes, num_blocks, stride, cardinality, widen_factor):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride, cardinality, widen_factor))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = torch.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = torch.nn.functional.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "# WideResNeXt-101 Model Definition\n",
    "def WideResNeXt101():\n",
    "    return WideResNeXt(WideResNeXtBlock, [3, 4, 23, 3], cardinality=32, widen_factor=2)\n",
    "\n",
    "# Load CIFAR-100 dataset\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "test_pkl_file = 'test.pkl'  # Path to your test.pkl file\n",
    "\n",
    "# Model, loss, optimizer, and scheduler\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = WideResNeXt101().to(device)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(0, 100):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = F.cross_entropy(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch}: Loss: {running_loss / (batch_idx + 1)}')\n",
    "    \n",
    "    # Test model with thresholds after each epoch using test.pkl\n",
    "    correct, total = test_with_thresholds_from_pkl(epoch, model, test_pkl_file, device, class_thresholds)\n",
    "    \n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09286f08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-20T09:02:44.294273Z",
     "iopub.status.busy": "2024-10-20T09:02:44.293452Z",
     "iopub.status.idle": "2024-10-20T09:02:44.304032Z",
     "shell.execute_reply": "2024-10-20T09:02:44.303157Z"
    },
    "papermill": {
     "duration": 0.036491,
     "end_time": "2024-10-20T09:02:44.305850",
     "exception": false,
     "start_time": "2024-10-20T09:02:44.269359",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import torch.optim as optim\n",
    "# import torchvision\n",
    "# import torchvision.transforms as transforms\n",
    "# import pandas as pd\n",
    "# from torch.optim.lr_scheduler import StepLR\n",
    "# import os\n",
    "\n",
    "# # Set default thresholds for each class\n",
    "# class_thresholds = {i: 0.9 for i in range(100)}  # Default threshold for each class\n",
    "\n",
    "# # Custom function to calculate the score\n",
    "# def calculate_score(true_labels: pd.Series, predicted_labels: pd.Series, accuracy_threshold: float = 0.7, gamma: float = 5.0) -> float:\n",
    "#     data = pd.DataFrame({'True_label': true_labels, 'Predicted_label': predicted_labels})\n",
    "#     filtered_df = data[data['Predicted_label'] != -1]\n",
    "#     all_classes = list(range(100))\n",
    "    \n",
    "#     sum_of_correctly_classified_high_accuracy = 0\n",
    "#     sum_of_correctly_classified_low_accuracy = 0\n",
    "#     accuracy_per_class = {}\n",
    "    \n",
    "#     grouped = filtered_df.groupby('Predicted_label')\n",
    "#     for name, group in grouped:\n",
    "#         accuracy = (group['True_label'] == group['Predicted_label']).sum() / len(group)\n",
    "#         accuracy_per_class[name] = accuracy\n",
    "    \n",
    "#     for cls in all_classes:\n",
    "#         total = len(filtered_df[filtered_df['Predicted_label'] == cls])\n",
    "#         class_accuracy = accuracy_per_class.get(cls, 0.0)\n",
    "#         if class_accuracy >= accuracy_threshold:\n",
    "#             sum_of_correctly_classified_high_accuracy += total\n",
    "#         else:\n",
    "#             sum_of_correctly_classified_low_accuracy += total\n",
    "\n",
    "#     final_score = sum_of_correctly_classified_high_accuracy - gamma * sum_of_correctly_classified_low_accuracy\n",
    "#     return round(final_score / 2)\n",
    "\n",
    "# # Custom function to apply thresholds and return predictions\n",
    "# def apply_class_thresholds(probabilities):\n",
    "#     predicted_labels = []\n",
    "#     for i in range(probabilities.size(0)):\n",
    "#         max_prob, predicted_class = torch.max(probabilities[i], 0)\n",
    "#         if max_prob >= class_thresholds[predicted_class.item()]:\n",
    "#             predicted_labels.append(predicted_class.item())\n",
    "#         else:\n",
    "#             predicted_labels.append(-1)\n",
    "#     return torch.tensor(predicted_labels)\n",
    "\n",
    "# # Test with class-specific thresholds and generate CSV\n",
    "# def test_with_thresholds(epoch, true_labels, predicted_labels):\n",
    "#     global best_accuracy\n",
    "#     model.eval()\n",
    "#     test_loss = 0\n",
    "#     correct = 0\n",
    "#     wrong = 0\n",
    "#     total = 0\n",
    "#     skipped_predictions = 0\n",
    "    \n",
    "#     predictions_for_csv = []\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "#             inputs, targets = inputs.to(device), targets.to(device)\n",
    "#             outputs = model(inputs)\n",
    "#             probabilities = F.softmax(outputs, dim=1)\n",
    "            \n",
    "#             # Apply class-specific thresholds and move predictions to the same device as targets\n",
    "#             predictions = apply_class_thresholds(probabilities).to(device)\n",
    "            \n",
    "#             total += targets.size(0)\n",
    "#             correct_predictions = predictions.eq(targets).sum().item()\n",
    "#             wrong_predictions = (predictions != -1).sum().item() - correct_predictions\n",
    "#             skipped_predictions += (predictions == -1).sum().item()\n",
    "\n",
    "#             correct += correct_predictions\n",
    "#             wrong += wrong_predictions\n",
    "            \n",
    "#             true_labels.extend(targets.cpu().numpy())\n",
    "#             predicted_labels.extend(predictions.cpu().numpy())\n",
    "\n",
    "#             # Save predictions for CSV\n",
    "#             predictions_for_csv.extend(predictions.cpu().numpy())\n",
    "\n",
    "#     # Save predictions to CSV\n",
    "#     prediction_df = pd.DataFrame({'ID': list(range(len(predictions_for_csv))), 'Predicted_label': predictions_for_csv})\n",
    "#     prediction_df.to_csv(f'predictions_epoch_{epoch}.csv', index=False)\n",
    "#     print(f'Predictions for epoch {epoch} saved to CSV.')\n",
    "\n",
    "#     # Calculate the test accuracy\n",
    "#     test_accuracy = 100. * correct / total\n",
    "#     print(f'Test set: Accuracy: {correct}/{total} ({test_accuracy:.2f}%) | Skipped: {skipped_predictions}')\n",
    "    \n",
    "#     return correct, wrong, total\n",
    "\n",
    "# # Load CIFAR-100 dataset\n",
    "# transform_train = transforms.Compose([\n",
    "#     transforms.RandomCrop(32, padding=4),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.ToTensor(),\n",
    "# ])\n",
    "\n",
    "# transform_test = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "# ])\n",
    "\n",
    "# trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
    "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "# testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
    "# testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "# # Model, loss, optimizer, and scheduler\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# model = WideResNeXt101().to(device)\n",
    "\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "# scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "# # Example Training Loop\n",
    "# true_labels = []\n",
    "# predicted_labels = []\n",
    "\n",
    "# for epoch in range(0, 100):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "#         inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "#         # Zero the parameter gradients\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         # Forward + Backward + Optimize\n",
    "#         outputs = model(inputs)\n",
    "#         loss = F.cross_entropy(outputs, targets)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "    \n",
    "#     print(f'Epoch {epoch}: Loss: {running_loss / (batch_idx + 1)}')\n",
    "    \n",
    "#     # Test model with thresholds after each epoch and generate predictions CSV\n",
    "#     correct, wrong, total = test_with_thresholds(epoch, true_labels, predicted_labels)\n",
    "    \n",
    "#     # Calculate and print the score\n",
    "#     final_score = calculate_score(pd.Series(true_labels), pd.Series(predicted_labels))\n",
    "#     print(f\"Epoch {epoch}: Correct predictions: {correct}, Wrong predictions: {wrong}, Total: {total}, Score: {final_score}\")\n",
    "    \n",
    "#     scheduler.step()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5906773,
     "sourceId": 9666920,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8204.789382,
   "end_time": "2024-10-20T09:02:45.848489",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-20T06:46:01.059107",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
