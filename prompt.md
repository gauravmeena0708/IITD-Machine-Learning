    I want you to help me generate a modular deep learning project structure for a new supervised learning task (e.g., image classification, time series prediction, etc.). The structure should be easily reusable and configurable.
    
    I need the following:
    âœ… 1. Folder & File Structure:
    
        project_name/
        â”œâ”€â”€ config.py                  # Central settings
        â”œâ”€â”€ main.py                    # Main training script
        â”œâ”€â”€ optuna_search.py           # For hyperparameter tuning
        â”œâ”€â”€ test_pipeline.py           # For quick end-to-end check
        â”œâ”€â”€ models/
        â”‚   â”œâ”€â”€ __init__.py
        â”‚   â”œâ”€â”€ model1.py
        â”‚   â”œâ”€â”€ model2.py
        â”œâ”€â”€ data/
        â”‚   â””â”€â”€ (placeholder for input files)
        â”œâ”€â”€ logs/
        â””â”€â”€ results/
    
        âœ… 2. Each file should include:
        ğŸ“ config.py
    
            Dataset name/source
    
            Model name
    
            Dropout, L2, learning rate
    
            Augmentation flags
    
            Batch size, epochs, early stopping
    
            WANDB project name
    
        ğŸ“ data_loader.py
    
            Load data based on source name (e.g., "CIFAR10", "BCI_IV_2A")
    
            Support local CSV or built-in datasets
    
        ğŸ“ preprocessor.py
    
            Normalize, standardize
    
            One-hot encode labels
    
            Optionally augment (Gaussian noise or flips)
    
            Log class distributions
    
        ğŸ“ models/
    
            Each model as build_model(...) with input_shape, num_classes, **kwargs
    
            Central registry with get_model_by_name() and list_available_models()
    
        ğŸ“ trainer.py
    
            Train model with class weights
    
            Early stopping support
    
            Return history
    
        ğŸ“ evaluator.py
    
            Evaluate on validation or test
    
            Return accuracy, confusion matrix, and classification report
    
        ğŸ“ main.py
    
            Use config.py
    
            Load data â†’ preprocess â†’ train â†’ evaluate â†’ save model
    
        ğŸ“ optuna_search.py
    
            Search dropout, l2, learning rate, batch size
    
            Log all to wandb
    
            Save best .keras and JSON config
    
        ğŸ“ test_pipeline.py
    
            Load a few samples
    
            Build model
    
            Train for 3 epochs
    
            Run eval
    
        âœ… Additional Requirements:
    
            Use wandb and optuna if USE_WANDB = True flag in config
    
            Save best model as .keras
    
            Write all logs to logs/ and results/<experiment_tag>/
    
            Print class-wise distribution in each set
    
            Use input_shape + num_classes everywhere as standard model interface
    
            Easy to extend with new models or datasets
    
        ğŸ§ª First Example Task:
    
        Start by generating the full project structure for a multiclass image classification task using CIFAR-10, using 2 CNN models (cnn_simple, cnn_batchnorm). Use TensorFlow/Keras.
    
        Once you're done, Iâ€™ll ask you to adapt it for time series, EEG, or NLP tasks.
    
    ğŸ” You Can Reuse This Prompt For:
    
        EEG signal classification
    
        NLP sentiment detection
    
        Tabular fraud detection
    
        Regression tasks
    
    Just change the "First Example Task" section to fit your new domain.
